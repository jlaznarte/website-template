
@techreport{hammer_remote_2007,
	title = {Remote sensing and atmospheric physics for an efficient use of renewable energies. {Status} {Report} 2004-2007},
	institution = {Virtual Institute of Energy Meteorology},
	author = {Hammer, A. and Heinemann, D. and Hoyer-Klick, C. and Lorenz, E. and Mayer, B. and Schroedter-Homscheidt, M.},
	year = {2007},
}

@article{hammer_short-term_1999,
	title = {Short-term forecasing of solar radiation: {A} statistical approach using satellite data},
	volume = {67},
	journal = {Solar Energy},
	author = {Hammer, A. and Heinemann, D. and Lorenz, E. and Lückehe, B.},
	year = {1999},
	pages = {139--150},
}

@article{kudo_forecasting_2009,
	title = {Forecasting electric power generation in a photovoltaic power system for an energy network},
	volume = {167},
	issn = {04247760},
	url = {http://sauwok.fecyt.es/apps/full_record.do?product=UA&search_mode=GeneralSearch&qid=1&SID=Q2hE3Ic2253CcnaNbBi&page=1&doc=9&colname=WOS},
	doi = {10.1002/eej.20755},
	number = {4},
	urldate = {2009-11-21},
	journal = {Electrical Engineering in Japan},
	author = {Kudo, Mitsuru and Takeuchi, Akira and Nozaki, Yousuke and Endo, Hisahito and Sumita, Jiro},
	month = jun,
	year = {2009},
	pages = {16--23},
	file = {ISI Web of Knowledge [v.4.6] - All Databases Full Record:files/663/full_record.html:text/html},
}

@article{grossberg_nonlinear_1988,
	title = {Nonlinear neural networks: {Principles}, mechanisms, and architectures.},
	volume = {1},
	shorttitle = {Nonlinear neural networks},
	number = {1},
	journal = {Neural Networks},
	author = {Grossberg, S.},
	year = {1988},
	pages = {17--61},
}

@inproceedings{krauter_pv_nodate,
	title = {{PV} {YIELD} {PREDICTION} {FOR} {THIN} {FILM} {TECHNOLOGIES} {AND} {THE} {EFFECT} {OF} {INPUT} {PARAMETERS} {INACCURACIES}},
	author = {Krauter, S. and Preiss, A. and Ferretti, N. and Grunow, P.},
}

@article{elizondo_development_1994,
	title = {Development of a natural network model to predict daily solar radiation},
	volume = {71},
	number = {1-2},
	journal = {Agricultural and forest meteorology},
	author = {Elizondo, D. and Hoogenboom, G. and McClendon, R. W.},
	year = {1994},
	keywords = {neural networks},
	pages = {115--132},
}

@article{voyant_predictability_2009,
	title = {Predictability of {PV} power grid performance on insular sites without weather stations: use of artificial neural networks},
	shorttitle = {Predictability of {PV} power grid performance on insular sites without weather stations},
	journal = {Arxiv preprint arXiv:0905.3569},
	author = {Voyant, C. and Muselli, M. and Paoli, C. and Nivet, M. L and Poggi, P.},
	year = {2009},
	keywords = {neural networks},
}

@article{paoli_solar_2009,
	title = {Solar radiation forecasting using ad-hoc time series preprocessing and neural networks},
	journal = {Imprint},
	author = {Paoli, C. and Voyant, C. and Muselli, M. and Nivet, M. L},
	year = {2009},
	keywords = {neural networks},
}

@article{krauter_pv_nodate-1,
	title = {{PV} {YIELD} {PREDICTION} {FOR} {THIN} {FILM} {TECHNOLOGIES} {AND} {THE} {EFFECT} {OF} {INPUT} {PARAMETERS} {INACCURACIES}},
	author = {Krauter, S. and Preiss, A. and Ferretti, N. and Grunow, P.},
}

@article{perez_forecasting_2007,
	title = {Forecasting solar radiation–{Preliminary} evaluation of an approach based upon the national forecast database},
	volume = {81},
	number = {6},
	journal = {Solar Energy},
	author = {Perez, R. and Moore, K. and Wilcox, S. and Renné, D. and Zelenka, A.},
	year = {2007},
	pages = {809--812},
}

@article{reikard_predicting_2009,
	title = {Predicting solar radiation at high resolutions: {A} comparison of time series forecasts},
	volume = {83},
	shorttitle = {Predicting solar radiation at high resolutions},
	number = {3},
	journal = {Solar Energy},
	author = {Reikard, G.},
	year = {2009},
	pages = {342--349},
}

@book{roy_sky_nodate,
	title = {Sky {Modelling} from {Digital} {Imagery}},
	url = {http://www.cadplan.com.au/Reports/dsmreport.pdf},
	publisher = {Report on Australian Research Council Project A},
	author = {Roy, G. G and Hayman, S. and Julian, W.},
	keywords = {cloud detection},
	file = {Roy1998.pdf:files/677/Roy1998.pdf:application/pdf},
}

@article{seiz_cloud_2002,
	title = {Cloud mapping from the ground: use of photogrammetric methods},
	volume = {68},
	shorttitle = {Cloud mapping from the ground},
	number = {9},
	journal = {Photogrammetric Engineering and Remote Sensing},
	author = {Seiz, G. and Baltsavias, E. P and Gruen, A.},
	year = {2002},
	pages = {941--951},
}

@article{calbo_feature_2008,
	title = {Feature {Extraction} from {Whole}-{Sky} {Ground}-{Based} {Images} for {Cloud}-{Type} {Recognition}},
	volume = {25},
	issn = {0739-0572},
	url = {http://eprints.usq.edu.au/4170/},
	doi = {10.1175/2007JTECHA959.1},
	number = {1},
	urldate = {2009-11-08},
	journal = {Journal of Atmospheric and Oceanic Technology},
	author = {Calbó, Josep and Sabburg, Jeff},
	year = {2008},
	keywords = {cloud detection},
	pages = {3},
	file = {Calbo2008feature.pdf:files/674/Calbo2008feature.pdf:application/pdf;Feature extraction from whole-sky ground-based images for cloud-type recognition - USQ ePrints:files/667/4170.html:text/html},
}

@incollection{anzalone_combining_2009,
	title = {Combining {Fuzzy} {C}-{Mean} and {Normalized} {Convolution} for {Cloud} {Detection} in {IR} {Images}},
	url = {http://dx.doi.org/10.1007/978-3-642-02282-1_18},
	abstract = {An important task for the cloud monitoring in several frameworks is providing maps of the cloud coverage. In this paper we
present a method to detect cloudy pixels for images taken from ground by an infra-red camera. The method is a three-steps
algorithm mainly based on a Fuzzy C-Mean clustering, that works on a feature space derived from the original image and the
output of the reconstructed image obtained via normalized convolution. Experiments, run on several infra-red images acquired
under different conditions, show that the cloud maps returned are satisfactory.},
	urldate = {2009-11-08},
	booktitle = {Fuzzy {Logic} and {Applications}},
	author = {Anzalone, Anna and Isgrò, Francesco and Tegolo, Domenico},
	year = {2009},
	keywords = {cloud detection},
	pages = {140--147},
	file = {Anzalone2009combining.pdf:files/675/Anzalone2009combining.pdf:application/pdf;SpringerLink Snapshot:files/666/e24k532076uvgn58.html:text/html},
}

@article{powers_evaluation:_2011,
	title = {Evaluation: from {Precision}, {Recall} and {F}-measure to {ROC}, {Informedness}, {Markedness} and {Correlation}},
	issn = {2229-3981},
	shorttitle = {Evaluation},
	url = {http://dspace.flinders.edu.au/xmlui/handle/2328/27165},
	abstract = {Commonly used evaluation measures including Recall, Precision, F-Measure and Rand Accuracy are
biased and should not be used without clear understanding of the biases, and corresponding identification of chance
or base case levels of the statistic. Using these measures a system that performs worse in the objective sense of
Informedness, can appear to perform better under any of these commonly used measures. We discuss several
concepts and measures that reflect the probability that prediction is informed versus chance. Informedness and
introduce Markedness as a dual measure for the probability that prediction is marked versus chance. Finally we
demonstrate elegant connections between the concepts of Informedness, Markedness, Correlation and Significance
as well as their intuitive relationships with Recall and Precision, and outline the extension from the dichotomous case
to the general multi-class case.},
	language = {en},
	urldate = {2017-04-18},
	author = {Powers, David Martin},
	month = dec,
	year = {2011},
	file = {Full Text PDF:/home/jlaznarte/Zotero/storage/SJDAZVQU/Powers - 2011 - Evaluation from Precision, Recall and F-measure t.pdf:application/pdf;Snapshot:/home/jlaznarte/Zotero/storage/CUU899GX/27165.html:text/html},
}

@article{yu_raq-random_2016,
	title = {{RAQ}-{A} {Random} {Forest} {Approach} for {Predicting} {Air} {Quality} in {Urban} {Sensing} {Systems}},
	volume = {16},
	issn = {1424-8220},
	doi = {10.3390/s16010086},
	abstract = {Air quality information such as the concentration of PM2.5 is of great significance for human health and city management. It affects the way of traveling, urban planning, government policies and so on. However, in major cities there is typically only a limited number of air quality monitoring stations. In the meantime, air quality varies in the urban areas and there can be large differences, even between closely neighboring regions. In this paper, a random forest approach for predicting air quality (RAQ) is proposed for urban sensing systems. The data generated by urban sensing includes meteorology data, road information, real-time traffic status and point of interest (POI) distribution. The random forest algorithm is exploited for data training and prediction. The performance of RAQ is evaluated with real city data. Compared with three other algorithms, this approach achieves better prediction precision. Exciting results are observed from the experiments that the air quality can be inferred with amazingly high accuracy from the data which are obtained from urban sensing.},
	language = {eng},
	number = {1},
	journal = {Sensors (Basel, Switzerland)},
	author = {Yu, Ruiyun and Yang, Yu and Yang, Leyou and Han, Guangjie and Move, Oguti Ann},
	month = jan,
	year = {2016},
	pmid = {26761008},
	pmcid = {PMC4732119},
	keywords = {air quality prediction, point of interest, random forest, traffic},
}

@article{raftery_use_2014,
	title = {Use and {Communication} of {Probabilistic} {Forecasts}},
	url = {http://arxiv.org/abs/1408.4812},
	abstract = {Probabilistic forecasts are becoming more and more available. How should they be used and communicated? What are the obstacles to their use in practice? I review experience with five problems where probabilistic forecasting played an important role. This leads me to identify five types of potential users: Low Stakes Users, who don't need probabilistic forecasts; General Assessors, who need an overall idea of the uncertainty in the forecast; Change Assessors, who need to know if a change is out of line with expectatations; Risk Avoiders, who wish to limit the risk of an adverse outcome; and Decision Theorists, who quantify their loss function and perform the decision-theoretic calculations. This suggests that it is important to interact with users and to consider their goals. The cognitive research tells us that calibration is important for trust in probability forecasts, and that it is important to match the verbal expression with the task. The cognitive load should be minimized, reducing the probabilistic forecast to a single percentile if appropriate. Probabilities of adverse events and percentiles of the predictive distribution of quantities of interest seem often to be the best way to summarize probabilistic forecasts. Formal decision theory has an important role, but in a limited range of applications.},
	urldate = {2017-04-18},
	journal = {arXiv:1408.4812 [stat]},
	author = {Raftery, Adrian E.},
	month = aug,
	year = {2014},
	note = {arXiv: 1408.4812},
	keywords = {probabilistic forecasts, Statistics - Applications},
	file = {arXiv\:1408.4812 PDF:files/498/Raftery - 2014 - Use and Communication of Probabilistic Forecasts.pdf:application/pdf;arXiv\:1408.4812 PDF:/home/jlaznarte/Zotero/storage/299S4WZ8/Raftery - 2014 - Use and Communication of Probabilistic Forecasts.pdf:application/pdf;arXiv.org Snapshot:files/499/1408.html:text/html;arXiv.org Snapshot:/home/jlaznarte/Zotero/storage/WJAUG687/1408.html:text/html},
}

@article{lou_thompson_review_2001,
	title = {A review of statistical methods for the meteorological adjustment of tropospheric ozone},
	volume = {35},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231000002612},
	doi = {10.1016/S1352-2310(00)00261-2},
	abstract = {A variety of statistical methods for meteorological adjustment of ozone have been proposed in the literature over the last decade for purposes of forecasting, estimating ozone time trends, or investigating underlying mechanisms from an empirical perspective. The methods can be broadly classified into regression, extreme value, and space–time methods. We present a critical review of these methods, beginning with a summary of what meteorological and ozone monitoring data have been considered and how they have been used for statistical analysis. We give particular attention to the question of trend estimation, and compare selected methods in an application to ozone time series from the Chicago area. We conclude that a number of approaches make useful contributions to the field, but that no one method is most appropriate for all purposes and all meteorological scenarios. Methodological issues such as the need for regional-scale analysis, the nonlinear dependence of ozone on meteorology, and extreme value analysis for trends are addressed. A comprehensive and reliable methodology for space–time extreme value analysis is attractive but lacking.},
	number = {3},
	urldate = {2017-04-18},
	journal = {Atmospheric Environment},
	author = {Lou Thompson, Mary and Reynolds, Joel and Cox, Lawrence H and Guttorp, Peter and Sampson, Paul D},
	year = {2001},
	keywords = {Environmetrics, Extreme values, Regression, Spatial statistics, time series},
	pages = {617--630},
	file = {ScienceDirect Full Text PDF:files/543/Lou Thompson et al. - 2001 - A review of statistical methods for the meteorolog.pdf:application/pdf;ScienceDirect Snapshot:/home/jlaznarte/Zotero/storage/8HK6SZU9/S1352231000002612.html:text/html},
}

@misc{noauthor_application_nodate,
	title = {The application of models under the {European} {Union}'s {Air} {Quality} {Directive}: {A} technical reference guide {\textless}br/{\textgreater}{EEA} {Technical} report {No} 10/2011},
	url = {http://acm.eionet.europa.eu/reports/EEA_TP_10_2011_fairmode_tech_ref_guide},
	urldate = {2015-09-04},
	file = {ETCACM_TP_2011_15_FAIRMODE_guide_modelling_NO2.pdf:files/587/ETCACM_TP_2011_15_FAIRMODE_guide_modelling_NO2.pdf:application/pdf;The application of models under the European Union's Air Quality Directive\: A technical reference guide <br/>EEA Technical report No 10/2011:files/586/EEA_TP_10_2011_fairmode_tech_ref_guide.html:text/html},
}

@article{agirre-basurko_regression_2006,
	series = {Urban {Air} {Quality} {ModellingUrban} {Air} {Quality} {Modelling}},
	title = {Regression and multilayer perceptron-based models to forecast hourly {O3} and {NO2} levels in the {Bilbao} area},
	volume = {21},
	issn = {1364-8152},
	url = {http://www.sciencedirect.com/science/article/pii/S1364815204003056},
	doi = {10.1016/j.envsoft.2004.07.008},
	abstract = {In this paper, we present the results obtained using three prognostic models to forecast ozone (O3) and nitrogen dioxide (NO2) levels in real-time up to 8 h ahead at four stations in Bilbao (Spain). Two multilayer perceptron (MLP) based models and one multiple linear regression based model were developed. The models utilised traffic variables, meteorological variables and O3 and NO2 hourly levels as input data, which were measured from 1993 to 1994. The performances of these three models were compared with persistence of levels and the observed values. The statistics of the Model Validation Kit determined the goodness of the fit of the developed models. The results indicated improved performance for the multilayer perceptron-based models over the multiple linear regression model. Furthermore, comparisons of the results of the multilayer perceptron-based models proved that the insertion of four additional seasonal input variables in the MLP provided the ability of obtaining more accurate predictions. The comparison of the results indicated that this model performance was more efficient in the forecasts of O3 and NO2 hourly levels k hours ahead (k = 1, 4, 5, 6, 7, 8), but not in the forecasted values 2 and 3 h ahead. Future research in this area could allow us to improve results for the above forecasts. The multilayer perceptron modelling was developed using the MATLAB software package.},
	number = {4},
	urldate = {2015-09-03},
	journal = {Environmental Modelling \& Software},
	author = {Agirre-Basurko, E. and Ibarra-Berastegi, G. and Madariaga, I.},
	month = apr,
	year = {2006},
	keywords = {Air quality modelling, Multilayer Perceptron, Multiple linear regression, Neural networks},
	pages = {430--446},
	file = {ScienceDirect Snapshot:files/588/S1364815204003056.html:text/html},
}

@article{donnelly_real_2015,
	title = {Real time air quality forecasting using integrated parametric and non-parametric regression techniques},
	volume = {103},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231014009595},
	doi = {10.1016/j.atmosenv.2014.12.011},
	abstract = {This paper presents a model for producing real time air quality forecasts with both high accuracy and high computational efficiency. Temporal variations in nitrogen dioxide (NO2) levels and historical correlations between meteorology and NO2 levels are used to estimate air quality 48 h in advance. Non-parametric kernel regression is used to produce linearized factors describing variations in concentrations with wind speed and direction and, furthermore, to produce seasonal and diurnal factors. The basis for the model is a multiple linear regression which uses these factors together with meteorological parameters and persistence as predictors. The model was calibrated at three urban sites and one rural site and the final fitted model achieved R values of between 0.62 and 0.79 for hourly forecasts and between 0.67 and 0.84 for daily maximum forecasts. Model validation using four model evaluation parameters, an index of agreement (IA), the correlation coefficient (R), the fraction of values within a factor of 2 (FAC2) and the fractional bias (FB), yielded good results. The IA for 24 hr forecasts of hourly NO2 was between 0.77 and 0.90 at urban sites and 0.74 at the rural site, while for daily maximum forecasts it was between 0.89 and 0.94 for urban sites and 0.78 for the rural site. R values of up to 0.79 and 0.81 and FAC2 values of 0.84 and 0.96 were observed for hourly and daily maximum predictions, respectively. The model requires only simple input data and very low computational resources. It found to be an accurate and efficient means of producing real time air quality forecasts.},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Donnelly, Aoife and Misstear, Bruce and Broderick, Brian},
	month = feb,
	year = {2015},
	keywords = {Air quality forecasting, Nitrogen Dioxide, Nonparametric kernel regression, Statistical modelling},
	pages = {53--65},
	file = {ScienceDirect Snapshot:files/589/S1352231014009595.html:text/html},
}

@article{zhang_real-time_2012,
	title = {Real-time air quality forecasting, part {II}: {State} of the science, current research needs, and future prospects},
	volume = {60},
	issn = {1352-2310},
	shorttitle = {Real-time air quality forecasting, part {II}},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231012001562},
	doi = {10.1016/j.atmosenv.2012.02.041},
	abstract = {The review of major 3-D global and regional real-time air quality forecasting (RT-AQF) models in Part I identifies several areas of improvement in meteorological forecasts, chemical inputs, and model treatments of atmospheric physical, dynamic, and chemical processes. Part II highlights several recent scientific advances in some of these areas that can be incorporated into RT-AQF models to address model deficiencies and improve forecast accuracies. Current major numerical, statistical, and computational techniques to improve forecasting skills are assessed. These include bias adjustment techniques to correct biases in forecast products, chemical data assimilation techniques for improving chemical initial and boundary conditions as well as emissions, and ensemble forecasting approaches to quantify the uncertainties of the forecasts. Several case applications of current 3-D RT-AQF models with the state-of-the-science model treatments, a detailed urban process module, and an advanced combined ensemble/data assimilation technique are presented to illustrate current model skills and capabilities. Major technical challenges and research priorities are provided. A new generation of comprehensive RT-AQF model systems, to emerge in the coming decades, will be based on state-of-the-science 3-D RT-AQF models, supplemented with efficient data assimilation techniques and sophisticated statistical models, and supported with modern numerical/computational technologies and a suite of real-time observational data from all platforms.},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Zhang, Yang and Bocquet, Marc and Mallet, Vivien and Seigneur, Christian and Baklanov, Alexander},
	month = dec,
	year = {2012},
	keywords = {Air quality forecasting, Chemical data assimilation, Ensemble forecasting, Scientific improvement},
	pages = {656--676},
	file = {ScienceDirect Snapshot:files/547/S1352231012001562.html:text/html},
}

@article{zhang_real-time_2012-1,
	title = {Real-time air quality forecasting, part {I}: {History}, techniques, and current status},
	volume = {60},
	issn = {1352-2310},
	shorttitle = {Real-time air quality forecasting, part {I}},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231012005900},
	doi = {10.1016/j.atmosenv.2012.06.031},
	abstract = {Real-time air quality forecasting (RT-AQF), a new discipline of the atmospheric sciences, represents one of the most far-reaching development and practical applications of science and engineering, poses unprecedented scientific, technical, and computational challenges, and generates significant opportunities for science dissemination and community participations. This two-part review provides a comprehensive assessment of the history, current status, major research and outreach challenges, and future directions of RT-AQF, with a focus on the application and improvement of three-dimensional (3-D) deterministic RT-AQF models. In Part I, major milestones in the history of RT-AQF are reviewed. The fundamentals of RT-AQF are introduced. Various RT-AQF techniques with varying degrees of sophistication and skills are described comparatively. Among all techniques, 3-D RT-AQF models with online-coupled meteorology–chemistry and their transitions from mesoscale to unified model systems across scales represent a significant advancement and would greatly enhance understanding of the underlying complex interplay of meteorology, emission, and chemistry from global to urban scales in the real atmosphere. Current major 3-D global and regional RT-AQF models in the world are reviewed in terms of model systems, component models, application scales, model inputs, forecast products, horizontal grid resolutions, and model treatments of chemistry and aerosol processes. An important trend of such models is their coupling with an urban model or a computational fluid dynamic model for urban/local scale applications at 1 km or less and with an exposure model to provide real-time public health assessment and exposure predictions. Evaluation protocols are described along with examinations of current forecasting skills and areas with large biases of major RT-AQF models.},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Zhang, Yang and Bocquet, Marc and Mallet, Vivien and Seigneur, Christian and Baklanov, Alexander},
	month = dec,
	year = {2012},
	keywords = {Air quality forecasting, Evaluation methods, Historic milestone, Techniques and tools},
	pages = {632--655},
	file = {ScienceDirect Snapshot:files/590/S1352231012005900.html:text/html},
}

@article{perez_prediction_2001,
	title = {Prediction of {NO} and {NO2} concentrations near a street with heavy traffic in {Santiago}, {Chile}},
	volume = {35},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231000002880},
	doi = {10.1016/S1352-2310(00)00288-0},
	abstract = {Based on NO concentrations and meteorological variables recorded hourly at a point close to an avenue with heavy traffic in the city of Santiago, we are able to build a simple model that allows prediction of NO concentrations several hours in advance. Predicted NO concentrations in conjunction with forecasted meteorological data may be used to predict NO2 concentrations with reasonable accuracy. We compare predictions generated using persistence, linear regressions and multi layer neural networks.},
	number = {10},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Perez, Patricio and Trier, Alex},
	month = apr,
	year = {2001},
	keywords = {Neural networks, Air pollution prediction, Meteorology forecast, NO and NO2},
	pages = {1783--1789},
	file = {perez2001.pdf:files/549/perez2001.pdf:application/pdf;ScienceDirect Snapshot:files/548/S1352231000002880.html:text/html},
}

@article{wang_online_2008,
	series = {Neural {Networks}: {Algorithms} and {Applications50} {Years} of {Artificial} {Intelligence}: a {Neuronal} {Approach4th} {International} {Symposium} on {Neural} {NetworksCampus} {Multidisciplinary} in {Perception} and {Intelligence}},
	title = {Online prediction model based on support vector machine},
	volume = {71},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231207002883},
	doi = {10.1016/j.neucom.2007.07.020},
	abstract = {For time-series forecasting problems, there have been several prediction models to data, but the development of a more accurate model is very difficult because of high non-linear and non-stable relations between input and output data. Almost all the models at hand are not applicable online, although online prediction, especially for air quality parameters forecasting, has very important significance for real-world applications. A support vector machine (SVM), as a novel and powerful machine learning tool, can be used for time-series prediction and has been reported to perform well by some promising results. This paper develops an online SVM model to predict air pollutant levels in an advancing time-series based on the monitored air pollutant database in Hong Kong downtown area. The experimental comparison between the online SVM model and the conventional SVM model (non-online SVM model) demonstrates the effectiveness and efficiency in predicting air quality parameters with different time series.},
	number = {4–6},
	urldate = {2015-09-03},
	journal = {Neurocomputing},
	author = {Wang, Wenjian and Men, Changqian and Lu, Weizhen},
	month = jan,
	year = {2008},
	keywords = {Air pollutant, Online model, Prediction performance, Support vector machine, Time-series forecasting},
	pages = {550--558},
	file = {ScienceDirect Snapshot:files/550/S0925231207002883.html:text/html},
}

@article{kolehmainen_neural_2001,
	title = {Neural networks and periodic components used in air quality forecasting},
	volume = {35},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S135223100000385X},
	doi = {10.1016/S1352-2310(00)00385-X},
	abstract = {Forecasting of air quality parameters is one topic of air quality research today due to the health effects caused by airborne pollutants in urban areas. The work presented here aims at comparing two principally different neural network methods that have been considered as potential tools in that area and assessing them in relation to regression with periodic components. Self-organizing maps (SOM) represent a form of competitive learning in which a neural network learns the structure of the data. Multi-layer perceptrons (MLPs) have been shown to be able to learn complex relationships between input and output variables. In addition, the effect of removing periodic components is evaluated with respect to neural networks. The methods were evaluated using hourly time series of NO2 and basic meteorological variables collected in the city of Stockholm in 1994–1998. The estimated values for forecasting were calculated in three ways: using the periodic components alone, applying neural network methods to the residual values after removing the periodic components, and applying only neural networks to the original data. The results showed that the best forecast estimates can be achieved by directly applying a MLP network to the original data, and thus, that a combination of the periodic regression method and neural algorithms does not give any advantage over a direct application of neural algorithms.},
	number = {5},
	urldate = {2016-03-07},
	journal = {Atmospheric Environment},
	author = {Kolehmainen, M and Martikainen, H and Ruuskanen, J},
	year = {2001},
	keywords = {Nitrogen Dioxide, Model comparison, Multi-Layer Perceptron, Residual, Self-organizing maps},
	pages = {815--825},
	file = {ScienceDirect Full Text PDF:files/551/Kolehmainen et al. - 2001 - Neural networks and periodic components used in ai.pdf:application/pdf;ScienceDirect Snapshot:files/592/S135223100000385X.html:text/html},
}

@article{liu_land_2015,
	title = {Land use regression models coupled with meteorology to model spatial and temporal variability of {NO2} and {PM10} in {Changsha}, {China}},
	volume = {116},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231015301977},
	doi = {10.1016/j.atmosenv.2015.06.056},
	abstract = {Land use regression (LUR) models are widely used in epidemiological studies to assess exposure to air pollution. However, most of the existing LUR studies focus on estimating annual or monthly average concentration of air pollutants, with high spatial but low temporal resolution. In this paper, we combined LUR models with meteorological conditions to estimate daily nitrogen dioxide (NO2) and particulate matter (PM10) concentrations in the urban area of Changsha, China. Seventy-four sites for NO2 and thirty-six sites for PM10 were selected to build LUR models. The LUR models explained 51\% and 62\% of spatial variability for NO2 and PM10. The most important spatial explanatory variables included major roads, residential land and public facilities land, indicating that the spatial distributions of NO2 and PM10 are closely related to traffic conditions and human activities. Meteorological factors were introduced to model the temporal variability of NO2 and PM10 by using meteorological factors regression (MFR) and back propagation neural network (BPNN) modeling techniques. Important temporal explanatory variables included temperature, wind speed, cloud cover and percentage of haze. Pearson's r values between predicted and measured concentrations were much higher in BPNN models than in MFR models. The results demonstrate that the BPNN models showed a better performance than the MFR models in modeling temporal variation of NO2 and PM10. The approach of modeling spatial and temporal variation of air pollutants using LUR models coupled with meteorological conditions has potential usefulness for exposure assessment, especially for medium or short term exposure, in health studies.},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Liu, Wu and Li, Xiaodong and Chen, Zuo and Zeng, Guangming and León, Tomás and Liang, Jie and Huang, Guohe and Gao, Zhihua and Jiao, Sheng and He, Xiaoxiao and Lai, Mingyong},
	month = sep,
	year = {2015},
	keywords = {Land use regression, Meteorological factors, NO2, PM10, Temporal resolution},
	pages = {272--280},
	file = {ScienceDirect Snapshot:files/556/S1352231015301977.html:text/html},
}

@article{de_fouquet_geostatistical_2007,
	title = {Geostatistical characterization of nitrogen dioxide concentration in an urban area: {Part} {II}: {Time} component of the estimation error},
	volume = {41},
	issn = {1352-2310},
	shorttitle = {Geostatistical characterization of nitrogen dioxide concentration in an urban area},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231007004670},
	doi = {10.1016/j.atmosenv.2007.05.027},
	abstract = {The temporal component of the estimation error is usually neglected in calculation of the variance of the error associated with the mapping of annual nitrogen dioxide (NO2) concentration. However, under realistic hypotheses, adding a temporal term to the spatial estimation variance is convenient enough. In a survey using passive diffusion samplers, which was carried out in 2001 on the Mulhouse area, the two measurement phases do not allow calculation of variance of the temporal error. But this error is accessible by means of the few fixed analyzers that continuously measure NO2 concentrations. The statistics of the experimental temporal error are examined and the temporal variogram is calculated. Experimental quadratic error is then compared to the theoretical estimation variance, deduced from the temporal variogram.

The fixed analyzers available in Alsace are then used to “optimize” empirically the duration and the date of the beginning of the temporary measurement phases.

Without increasing the cost of the temporary survey, it is then possible to improve the precision of the maps of annual NO2 concentration, by combining the spatial optimization presented in Part I of the paper, and the temporal optimization presented here.},
	number = {32},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {de Fouquet, Chantal and Gallois, David and Perron, Gilles},
	month = oct,
	year = {2007},
	keywords = {Cartography, Estimation variance, geostatistics, Nitrogen dioxide (NO2), Sampling optimization, Spatial variability, Time variability, Urban air pollution},
	pages = {6691--6700},
	file = {ScienceDirect Snapshot:files/558/S1352231007004670.html:text/html},
}

@article{morabito_fuzzy_2003,
	series = {Neural {Network} {Analysis} of {Complex} {Scientific} {Data}: {Astronomy} and {Geosciences}},
	title = {Fuzzy neural identification and forecasting techniques to process experimental urban air pollution data},
	volume = {16},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608003000194},
	doi = {10.1016/S0893-6080(03)00019-4},
	abstract = {This paper focuses on the processing of experimentally measured pollution data. Measuring locally both air quality parameters and atmospheric data can show how complex can be their interrelations and how they change spatially. Furthermore, apart from physical and biochemical dependencies, two important aspects need to be incorporated in the model, traffic data and topographic information, like presence and configuration of buildings and roads. Since estimating the evolution of pollutant in the urban air can have significant economic impact already on a short term basis as well as relevant consequences on public health on a medium-long term scale, various interdisciplinary researches are under way on this subject. In this work, we pursue two goals. The first one is to derive a representative model of the multivariate relationships that should be able to reproduce local interactions; the second goal of the paper is to predict, when possible, the short term evolution of pollutants in order to prevent the onset of above threshold levels of pollutants that can be dangerous to humans. The threshold levels of interest are fixed by both EU recommendations and regional regulations. As a by-product of the research, we could derive some directives to be supplied to local authorities to properly organize car traffic in advance based on the estimated parameters. The case study here proposed is that of Villa San Giovanni, a small town at the tip of Italy, located just in front of Sicily, on the Messina Strait. This is a significant case, since the city is affected by the heavy traffic directed (and coming from) Sicily. The main results here reported include the short time prediction of the concentration of hydrocarbons (HC) in the local air, the comparison between different methods based on fuzzy neural systems, and the proposal of local models of non-linear interactions among traffic, atmospheric and pollution data. Additionally, comments on a longer horizon forecast are given.},
	number = {3–4},
	urldate = {2015-09-03},
	journal = {Neural Networks},
	author = {Morabito, Francesco Carlo and Versaci, Mario},
	month = apr,
	year = {2003},
	keywords = {Air pollution, Complexity, Fuzzy ellipsoidal systems, Neuro-fuzzy systems, Non-linear systems},
	pages = {493--506},
	file = {ScienceDirect Snapshot:files/594/S0893608003000194.html:text/html},
}

@article{juhos_forecasting_2008,
	title = {Forecasting of traffic origin {NO} and {NO2} concentrations by {Support} {Vector} {Machines} and neural networks using {Principal} {Component} {Analysis}},
	volume = {16},
	issn = {1569-190X},
	url = {http://www.sciencedirect.com/science/article/pii/S1569190X08001585},
	doi = {10.1016/j.simpat.2008.08.006},
	abstract = {The main aim of this paper is to predict NO and NO2 concentrations four days in advance comparing two artificial intelligence learning methods, namely, Multi-Layer Perceptron and Support Vector Machines on two kinds of spatial embedding of the temporal time series. Hourly values of NO and NO2 concentrations, as well as meteorological variables were recorded in a cross-road monitoring station with heavy traffic in Szeged in order to build a model for predicting NO and NO2 concentrations several hours in advance. The prediction of NO and NO2 concentrations was performed partly on the basis of their past values, and partly on the basis of temperature, humidity and wind speed data. Since NO can be predicted more accurately, its values were considered primarily when forecasting NO2. Time series prediction can be interpreted in a way that is suitable for artificial intelligence learning. Two effective learning methods, namely, Multi-Layer Perceptron and Support Vector Regression are used to provide efficient non-linear models for NO and NO2 times series predictions. Multi-Layer Perceptron is widely used to predict these time series, but Support Vector Regression has not yet been applied for predicting NO and NO2 concentrations. Grid search is applied to select the best parameters for the learners. To get rid of the curse of dimensionality of the spatial embedding of the time series Principal Component Analysis is taken to reduce the dimension of the embedded data. Three commonly used linear algorithms were considered as references: one-day persistence, average of several-day persistence and linear regression. Based on the good results of the average of several-day persistence, a prediction scheme was introduced, which forms weighted averages instead of simple ones. The optimization of these weights was performed with linear regression in linear case and with the learning methods mentioned in non-linear case. Concerning the NO predictions, the non-linear learning methods give significantly better predictions than the reference linear methods. In the case of NO2 the improvement of the prediction is considerable; however, it is less notable than for NO.},
	number = {9},
	urldate = {2015-09-03},
	journal = {Simulation Modelling Practice and Theory},
	author = {Juhos, István and Makra, László and Tóth, Balázs},
	month = oct,
	year = {2008},
	keywords = {Multi-Layer Perceptron, Artificial neural networks, Dimension reduction, Forecast, Principal Component Analysis, Support Vector Machines, support vector regression},
	pages = {1488--1502},
	file = {ScienceDirect Snapshot:files/562/S1569190X08001585.html:text/html},
}

@article{kukkonen_extensive_2003,
	title = {Extensive evaluation of neural network models for the prediction of {NO2} and {PM10} concentrations, compared with a deterministic modelling system and measurements in central {Helsinki}},
	volume = {37},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231003005831},
	doi = {10.1016/S1352-2310(03)00583-1},
	abstract = {Five neural network (NN) models, a linear statistical model and a deterministic modelling system (DET) were evaluated for the prediction of urban NO2 and PM10 concentrations. The model evaluation work considered the sequential hourly concentration time series of NO2 and PM10, which were measured at two stations in central Helsinki, from 1996 to 1999. The models utilised selected traffic flow and pre-processed meteorological variables as input data. An imputed concentration dataset was also created, in which the missing values were replaced, in order to obtain a harmonised database that is well suited for the inter-comparison of models. Three statistical criteria were adopted: the index of agreement (IA), the squared correlation coefficient (R2) and the fractional bias. The results obtained with various non-linear NN models show a good agreement with the measured concentration data for NO2; for instance, the annual mean of the IA values and their standard deviations range from 0.86±0.02 to 0.91±0.01. In the case of NO2, the non-linear NN models produce a range of model performance values that are slightly better than those by the DET. NN models generally perform better than the statistical linear model, for predicting both NO2 and PM10 concentrations. In the case of PM10, the model performance statistics of the NN models were not as good as those for NO2 over the entire range of models considered. However, the currently available NN models are neither applicable for predicting spatial concentration distributions in urban areas, nor for evaluating air pollution abatement scenarios for future years.},
	number = {32},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Kukkonen, Jaakko and Partanen, Leena and Karppinen, Ari and Ruuskanen, Juhani and Junninen, Heikki and Kolehmainen, Mikko and Niska, Harri and Dorling, Stephen and Chatterton, Tim and Foxall, Rob and Cawley, Gavin},
	month = oct,
	year = {2003},
	keywords = {NO2, PM10, Model evaluation, Neural network, Urban air},
	pages = {4539--4550},
	file = {ScienceDirect Snapshot:files/598/S1352231003005831.html:text/html},
}

@article{aguilera_evaluation_2013,
	title = {Evaluation of the {CALIOPE} air quality forecasting system for epidemiological research: {The} example of {NO2} in the province of {Girona} ({Spain})},
	volume = {72},
	issn = {1352-2310},
	shorttitle = {Evaluation of the {CALIOPE} air quality forecasting system for epidemiological research},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231013001362},
	doi = {10.1016/j.atmosenv.2013.02.035},
	abstract = {Background
Air quality models are being increasingly used to estimate long-term individual exposures to air pollution in epidemiological studies. Most of them have been evaluated against measurements from a limited number of monitoring stations, which may not properly reflect the exposure characteristics of the study population.
Methods
We evaluated the performance of the high-resolution CALIOPE air quality forecasting system over a large sample of passive measurements of NO2 conducted at 635 home outdoor locations of the Girona province (Spain) during several 4-week sampling campaigns over one year (July 2007–June 2008). Sampling sites were superposed over the 4 km × 4 km CALIOPE grid, and average NO2 modeled concentrations were derived for all measurements conducted during the same sampling campaign at all the sampling sites located within the same grid cell. In addition, the ratio between measured and modeled concentrations for the whole study period at one fixed monitoring station was used to post-process the modeled values at the home outdoor locations.
Results
The correlation between measured and modeled concentrations for the entire study area (which includes urban settings, middle-size towns, and rural areas) was 0.78. Modeled concentrations were underestimated in the whole study area. After correcting the modeled concentrations by the measured to modeled ratio at the fixed station (r = 0.25), they were very similar to the measured concentrations (27.7 μg m−3 and 29.3 μg m−3, respectively). However, the performance of the modeling system depends on the type of subarea and is affected by the sub-grid emission sources.
Conclusions
The evaluation over the heterogenous Girona province showed that CALIOPE is able to reproduce the spatial variability of 4-week NO2 concentrations at the small regional level. CALIOPE output data is a valuable tool to complement study-specific air pollution measurements by incorporating regional spatial variability as well as short- and long-term temporal variability of background pollution in epidemiological research.},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Aguilera, Inmaculada and Basagaña, Xavier and Pay, María Teresa and Agis, David and Bouso, Laura and Foraster, Maria and Rivera, Marcela and Baldasano, José María and Künzli, Nino},
	month = jun,
	year = {2013},
	keywords = {NO2, Model evaluation, Air quality modeling, Passive sampling},
	pages = {134--141},
	file = {ScienceDirect Snapshot:files/600/S1352231013001362.html:text/html},
}

@article{pires_correction_2011,
	title = {Correction methods for statistical models in tropospheric ozone forecasting},
	volume = {45},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S135223101100135X},
	doi = {10.1016/j.atmosenv.2011.02.011},
	abstract = {This study proposes two methods to enhance the performance of statistical models for prediction tropospheric ozone concentrations. The first method corrects the statistical model based on the average daily profile of the model errors in training set. The second method estimates the model error by making the analogy with three basic modes of feedback control: proportional, integral and derivative. These correction methods were tested with multiple linear regression (MLR) and artificial neural networks (ANN) for prediction of hourly average tropospheric ozone (O3) concentrations.

The inputs of the models were the hourly average concentrations of sulphur dioxide (SO2), carbon monoxide (CO), nitrogen oxide (NO), nitrogen dioxide (NO2) and O3, and some meteorological variables (temperature – T; relative humidity – RH; and wind speed – WS) measured 24 h before. The analysed period was from May to June 2003 divided in training and test periods.

ANN presented slightly better performance than MLR model for prediction of O3 concentrations. Both models presented improvements with the proposed correction methods. The first method achieved the highest improvements with ANN model. However, the second method was the one that obtained the best predictions of hourly average O3 concentrations with the correction of MLR model.},
	number = {14},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Pires, J. C. M. and Martins, F. G.},
	month = may,
	year = {2011},
	keywords = {Multiple linear regression, Air pollution modelling, artificial neural network, Correction methods, Tropospheric ozone},
	pages = {2413--2417},
	file = {ScienceDirect Snapshot:files/602/S135223101100135X.html:text/html},
}

@article{elangasinghe_complex_2014,
	title = {Complex time series analysis of {PM10} and {PM2}.5 for a coastal site using artificial neural network modelling and k-means clustering},
	volume = {94},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231014003239},
	doi = {10.1016/j.atmosenv.2014.04.051},
	abstract = {This paper uses artificial neural networks (ANN), combined with k-means clustering, to understand the complex time series of PM10 and PM2.5 concentrations at a coastal location of New Zealand based on data from a single site. Out of available meteorological parameters from the network (wind speed, wind direction, solar radiation, temperature, relative humidity), key factors governing the pattern of the time series concentrations were identified through input sensitivity analysis performed on the trained neural network model. The transport pathways of particulate matter under these key meteorological parameters were further analysed through bivariate concentration polar plots and k-means clustering techniques. The analysis shows that the external sources such as marine aerosols and local sources such as traffic and biomass burning contribute equally to the particulate matter concentrations at the study site. These results are in agreement with the results of receptor modelling by the Auckland Council based on Positive Matrix Factorization (PMF). Our findings also show that contrasting concentration–wind speed relationships exist between marine aerosols and local traffic sources resulting in very noisy and seemingly large random PM10 concentrations. The inclusion of cluster rankings as an input parameter to the ANN model showed a statistically significant (p \&lt; 0.005) improvement in the performance of the ANN time series model and also showed better performance in picking up high concentrations. For the presented case study, the correlation coefficient between observed and predicted concentrations improved from 0.77 to 0.79 for PM2.5 and from 0.63 to 0.69 for PM10 and reduced the root mean squared error (RMSE) from 5.00 to 4.74 for PM2.5 and from 6.77 to 6.34 for PM10. The techniques presented here enable the user to obtain an understanding of potential sources and their transport characteristics prior to the implementation of costly chemical analysis techniques or advanced air dispersion models.},
	urldate = {2015-05-29},
	journal = {Atmospheric Environment},
	author = {Elangasinghe, M. A. and Singhal, N. and Dirks, K. N. and Salmond, J. A. and Samarasinghe, S.},
	month = sep,
	year = {2014},
	keywords = {Air quality modelling, artificial neural network, K-means clustering, Marine aerosols},
	pages = {106--116},
	file = {ScienceDirect Snapshot:files/603/S1352231014003239.html:text/html},
}

@article{cheon_bayesian_2009,
	title = {Bayesian networks based rare event prediction with sensor data},
	volume = {22},
	issn = {0950-7051},
	url = {http://www.sciencedirect.com/science/article/pii/S0950705109000392},
	doi = {10.1016/j.knosys.2009.02.004},
	abstract = {A Bayesian network is a powerful graphical model. It is advantageous for real-world data analysis and finding relations among variables. Knowledge presentation and rule generation, based on a Bayesian approach, have been studied and reported in many research papers across various fields. Since a Bayesian network has both causal and probabilistic semantics, it is regarded as an ideal representation to combine background knowledge and real data. Rare event predictions have been performed using several methods, but remain a challenge. We design and implement a Bayesian network model to forecast daily ozone states. We evaluate the proposed Bayesian network model, comparing it to traditional decision tree models, to examine its utility.},
	number = {5},
	urldate = {2015-09-03},
	journal = {Knowledge-Based Systems},
	author = {Cheon, Seong-Pyo and Kim, Sungshin and Lee, So-Young and Lee, Chong-Bum},
	month = jul,
	year = {2009},
	keywords = {Bayesian network, Ozone forecasting, Rare event prediction},
	pages = {336--343},
	file = {ScienceDirect Snapshot:files/604/S0950705109000392.html:text/html},
}

@article{fasbender_bayesian_2009,
	title = {Bayesian data fusion for space–time prediction of air pollutants: {The} case of {NO2} in {Belgium}},
	volume = {43},
	issn = {1352-2310},
	shorttitle = {Bayesian data fusion for space–time prediction of air pollutants},
	url = {http://www.sciencedirect.com/science/article/pii/S135223100900452X},
	doi = {10.1016/j.atmosenv.2009.05.036},
	abstract = {In the beginning of the 21st century, it is obvious that health and environmental matters are among the most important political and societal topics. The various kinds of pollution (e.g. air pollution, soil pollution, water contamination) are responsible for significant health and environmental degradation. In order to adequately deal with pollution issues, it is important to better understand the acting processes and to be able to account for specific knowledge about the pollutant. Thanks to this, it will be possible to forecast pollutant concentrations so that efficient actions can be rapidly taken. Based on a Bayesian data fusion (BDF) framework, the present paper proposes a methodology for air pollutant forecasting using the space–time properties of the process and several secondary information sources that contribute to a better understanding of the pollutant behavior (e.g. meteorological variables and anthropogenic activities). Consequently, the present work can contribute to improving the representation and the forecast of pollutant fields. Moreover the developed approach also permits to predict the probability of exceeding a given threshold, as required in official regulations for some pollutants (e.g. the European directives). The BDF framework is applied here to the case of space–time predictions of air concentrations of nitrogen dioxide (NO2) in Belgium. After a detailed description of some specific assumptions, results showed that BDF is able to successfully account for secondary information sources, thus leading to meaningful NO2 predictions.},
	number = {30},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Fasbender, D. and Brasseur, O. and Bogaert, P.},
	month = sep,
	year = {2009},
	keywords = {Data merging, Kriging, Stochastic method},
	pages = {4632--4645},
	file = {ScienceDirect Snapshot:files/570/S135223100900452X.html:text/html},
}

@article{feng_artificial_2015,
	title = {Artificial neural networks forecasting of {PM2}.5 pollution using air mass trajectory based geographic model and wavelet transformation},
	volume = {107},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231015001491},
	doi = {10.1016/j.atmosenv.2015.02.030},
	abstract = {In the paper a novel hybrid model combining air mass trajectory analysis and wavelet transformation to improve the artificial neural network (ANN) forecast accuracy of daily average concentrations of PM2.5 two days in advance is presented. The model was developed from 13 different air pollution monitoring stations in Beijing, Tianjin, and Hebei province (Jing-Jin-Ji area). The air mass trajectory was used to recognize distinct corridors for transport of “dirty” air and “clean” air to selected stations. With each corridor, a triangular station net was constructed based on air mass trajectories and the distances between neighboring sites. Wind speed and direction were also considered as parameters in calculating this trajectory based air pollution indicator value. Moreover, the original time series of PM2.5 concentration was decomposed by wavelet transformation into a few sub-series with lower variability. The prediction strategy applied to each of them and then summed up the individual prediction results. Daily meteorological forecast variables as well as the respective pollutant predictors were used as input to a multi-layer perceptron (MLP) type of back-propagation neural network. The experimental verification of the proposed model was conducted over a period of more than one year (between September 2013 and October 2014). It is found that the trajectory based geographic model and wavelet transformation can be effective tools to improve the PM2.5 forecasting accuracy. The root mean squared error (RMSE) of the hybrid model can be reduced, on the average, by up to 40 percent. Particularly, the high PM2.5 days are almost anticipated by using wavelet decomposition and the detection rate (DR) for a given alert threshold of hybrid model can reach 90\% on average. This approach shows the potential to be applied in other countries’ air quality forecasting systems.},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Feng, Xiao and Li, Qi and Zhu, Yajie and Hou, Junxiong and Jin, Lingyan and Wang, Jingjie},
	month = apr,
	year = {2015},
	keywords = {Artificial neural networks, Air mass trajectory based geographic model, PM2.5 forecasting, Wavelet transformation},
	pages = {118--128},
	file = {ScienceDirect Full Text PDF:files/607/Feng et al. - 2015 - Artificial neural networks forecasting of PM2.5 po.pdf:application/pdf;ScienceDirect Snapshot:files/606/S1352231015001491.html:text/html},
}

@article{domanska_application_2012,
	title = {Application of fuzzy time series models for forecasting pollution concentrations},
	volume = {39},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S095741741200036X},
	doi = {10.1016/j.eswa.2012.01.023},
	abstract = {In the paper a model to predict the concentrations of particulate matter PM10, PM2.5, SO2, NO, CO and O3 for a chosen number of hours forward is proposed. The method requires historical data for a large number of points in time, particularly weather forecast data, actual weather data and pollution data. The idea is that by matching forecast data with similar forecast data in the historical data set it is possible then to obtain actual weather data and through this pollution data. To aggregate time points with similar forecast data determined by a distance function, fuzzy numbers are generated from the forecast data, covering forecast data and actual data. Again using a distance function, actual data is compared with the fuzzy number to determine how the grade of membership is. The model was prepared in such a way that all the data which is usually imprecise, chaotic, uncertain can be used. The model is used in Poland by the Institute of Meteorology and by Water Management, and by the Voivodship Inspector for Environmental Protection. It forecast selected pollution concentrations for all areas of Poland.},
	number = {9},
	urldate = {2015-09-03},
	journal = {Expert Systems with Applications},
	author = {Domańska, D. and Wojtylak, M.},
	month = jul,
	year = {2012},
	keywords = {Expert system, prediction, PM10, Air pollution forecasting, Data Mining, Fuzzy numbers, Fuzzy weather forecast},
	pages = {7673--7679},
	file = {ScienceDirect Snapshot:files/572/S095741741200036X.html:text/html},
}

@article{kurt_online_2008,
	series = {Assessment of {Urban} and {Regional} {Air} {Quality} and its {ImpactsSelected} papers submitted to the {Third} {International} {Symposium} on {Air} {Quality} {Management} at {Urban}, {Regional} and {Global} {Scales} ({AQM2005}) and 14th {IUAPPA} {Regional} {Conference} held in {Istanbul} on {September} 26-30, 2005},
	title = {An online air pollution forecasting system using neural networks},
	volume = {34},
	issn = {0160-4120},
	url = {http://www.sciencedirect.com/science/article/pii/S0160412007002310},
	doi = {10.1016/j.envint.2007.12.020},
	abstract = {In this work, an online air pollution forecasting system for Greater Istanbul Area is developed. The system predicts three air pollution indicator (SO2, PM10 and CO) levels for the next three days (+ 1, + 2, and + 3 days) using neural networks. AirPolTool, a user-friendly website (http://airpol.fatih.edu.tr), publishes + 1, + 2, and + 3 days predictions of air pollutants updated twice a day. Experiments presented in this paper show that quite accurate predictions of air pollutant indicator levels are possible with a simple neural network. It is shown that further optimizations of the model can be achieved using different input parameters and different experimental setups. Firstly, + 1, + 2, and + 3 days' pollution levels are predicted independently using same training data, then + 2 and + 3 days are predicted cumulatively using previously days predicted values. Better prediction results are obtained in the cumulative method. Secondly, the size of training data base used in the model is optimized. The best modeling performance with minimum error rate is achieved using 3–15 past days in the training data set. Finally, the effect of the day of week as an input parameter is investigated. Better forecasts with higher accuracy are observed using the day of week as an input parameter.},
	number = {5},
	urldate = {2015-09-03},
	journal = {Environment International},
	author = {Kurt, Atakan and Gulbagci, Betul and Karaca, Ferhat and Alagha, Omar},
	month = jul,
	year = {2008},
	keywords = {prediction, Neural networks, Air pollution, meteorology, Turkey},
	pages = {592--598},
	file = {1-s2.0-S0160412007002310-main.pdf:files/610/1-s2.0-S0160412007002310-main.pdf:application/pdf;ScienceDirect Snapshot:files/609/S0160412007002310.html:text/html},
}

@article{taheri_shahraiyni_new_2015,
	title = {A new structure identification scheme for {ANFIS} and its application for the simulation of virtual air pollution monitoring stations in urban areas},
	volume = {41},
	issn = {0952-1976},
	url = {http://www.sciencedirect.com/science/article/pii/S0952197615000494},
	doi = {10.1016/j.engappai.2015.02.010},
	abstract = {Parameter and structure identifications are necessary in any modelling which aims to achieve a generalised model. Although ANFIS (Adaptive Network-based Fuzzy Inference System) employs well-known parameter-identification techniques, it needs to structure identification techniques for the determination of an optimum number of fuzzy rules and the selection of significant input variables from among the candidate input variables. In this study, a new structure identification scheme is developed and introduced, which is simultaneously capable of the selection of significant input variables and the determination of an optimum number of rules. This new structure identification was joined to ANFIS, and this joined modelling framework was applied to the simulation of virtual air-pollution monitoring stations in Berlin. In this study, 18 virtual particulate matter stations were simulated using the particulate matter data of some of the current stations. In other words, the particulate matter monitoring network of Berlin has been intensified. The evaluation of simulated virtual stations shows that, although the uncertainty of daily particulate matter measurement is about 10 percent, the simulated virtual stations can estimate the mean daily particulate matter with less than 10 percent of error. Mean absolute error and root mean square error of the simulations are less than 2.4 and 3.4 µg/m3, respectively. The correlation coefficient of the simulation results was more than 0.94. In addition, the range of mean bias error is between −1.0 and 0.5 µg/m3, and the range of factor of exceedance is between −14.8 and 10.8 percent. It means that the simulated virtual stations have a small bias. These results demonstrated the appropriate performance of the joined new structure identification scheme and ANFIS for development of a virtual air pollution monitoring network.},
	urldate = {2015-05-29},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Taheri Shahraiyni, Hamid and Sodoudi, Sahar and Kerschbaumer, Andreas and Cubasch, Ulrich},
	month = may,
	year = {2015},
	keywords = {Air pollution, ANFIS, Structure identification, Urban areas, Virtual stations},
	pages = {175--182},
	file = {ScienceDirect Full Text PDF:files/574/Taheri Shahraiyni et al. - 2015 - A new structure identification scheme for ANFIS an.pdf:application/pdf;ScienceDirect Snapshot:files/573/S0952197615000494.html:text/html},
}

@article{heo_new_2004,
	title = {A new method of ozone forecasting using fuzzy expert and neural network systems},
	volume = {325},
	issn = {0048-9697},
	url = {http://www.sciencedirect.com/science/article/pii/S0048969703006624},
	doi = {10.1016/j.scitotenv.2003.11.009},
	abstract = {This study describes the method of forecasting daily maximum ozone concentrations at four monitoring sites in Seoul, Korea. The forecasting tools developed are fuzzy expert and neural network systems. The hourly data for air pollutants and meteorological variables, obtained both at the surface and at the high elevation (500 hPa) stations of Seoul City for the period of 1989–1999, were analyzed. Two types of forecast models are developed. The first model, Part I, uses a fuzzy expert system and forecasts the possibility of high ozone levels (equal to or above 80 ppb) occurring on the next day. The second model, Part II, uses a neural network system to forecast the daily maximum concentration of ozone on the following day. The forecasting system includes a correction function so that the existing model can be updated whenever a new ozone episode appears. The accuracy of the forecasting system has been improved continuously through verification and augmentation.},
	number = {1–3},
	urldate = {2015-09-03},
	journal = {Science of The Total Environment},
	author = {Heo, Jeong-Sook and Kim, Dong-Sool},
	month = jun,
	year = {2004},
	keywords = {Fuzzy expert system, Neural network system, Ozone, Ozone forecasting system},
	pages = {221--237},
	file = {ScienceDirect Snapshot:files/575/S0048969703006624.html:text/html},
}

@article{russo_air_2013,
	title = {Air quality prediction using optimal neural networks with stochastic variables},
	volume = {79},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231013006031},
	doi = {10.1016/j.atmosenv.2013.07.072},
	abstract = {We apply recent methods in stochastic data analysis for discovering a set of few stochastic variables that represent the relevant information on a multivariate stochastic system, used as input for artificial neural network models for air quality forecast. We show that using these derived variables as input variables for training the neural networks it is possible to significantly reduce the amount of input variables necessary for the neural network model, without considerably changing the predictive power of the model. The reduced set of variables including these derived variables is therefore proposed as an optimal variable set for training neural network models in forecasting geophysical and weather properties. Finally, we briefly discuss other possible applications of such optimized neural network models.},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Russo, Ana and Raischel, Frank and Lind, Pedro G.},
	month = nov,
	year = {2013},
	keywords = {Neural networks, Environmental research, Pollutants, Stochastic systems},
	pages = {822--830},
	file = {ScienceDirect Snapshot:files/611/S1352231013006031.html:text/html},
}

@article{diaz-robles_hybrid_2008,
	title = {A hybrid {ARIMA} and artificial neural networks model to forecast particulate matter in urban areas: {The} case of {Temuco}, {Chile}},
	volume = {42},
	issn = {1352-2310},
	shorttitle = {A hybrid {ARIMA} and artificial neural networks model to forecast particulate matter in urban areas},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231008006523},
	doi = {10.1016/j.atmosenv.2008.07.020},
	abstract = {Air quality time series consists of complex linear and non-linear patterns and are difficult to forecast. Box–Jenkins Time Series (ARIMA) and multilinear regression (MLR) models have been applied to air quality forecasting in urban areas, but they have limited accuracy owing to their inability to predict extreme events. Artificial neural networks (ANN) can recognize non-linear patterns that include extremes. A novel hybrid model combining ARIMA and ANN to improve forecast accuracy for an area with limited air quality and meteorological data was applied to Temuco, Chile, where residential wood burning is a major pollution source during cold winters, using surface meteorological and PM10 measurements. Experimental results indicated that the hybrid model can be an effective tool to improve the PM10 forecasting accuracy obtained by either of the models used separately, and compared with a deterministic MLR. The hybrid model was able to capture 100\% and 80\% of alert and pre-emergency episodes, respectively. This approach demonstrates the potential to be applied to air quality forecasting in other cities and countries.},
	number = {35},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Díaz-Robles, Luis A. and Ortega, Juan C. and Fu, Joshua S. and Reed, Gregory D. and Chow, Judith C. and Watson, John G. and Moncada-Herrera, Juan A.},
	month = nov,
	year = {2008},
	keywords = {Neural networks, ARIMA, Hybrid, Particulate matter forecasting, Temuco},
	pages = {8331--8340},
	file = {ScienceDirect Snapshot:files/612/S1352231008006523.html:text/html},
}

@article{robeson_conditional_1989,
	title = {A conditional probability density function for forecasting ozone air quality data},
	volume = {23},
	issn = {0004-6981},
	url = {http://www.sciencedirect.com/science/article/pii/0004698189900164},
	doi = {10.1016/0004-6981(89)90016-4},
	abstract = {Probabilistic forecasts are often employed to estimate the potential for high pollutant concentrations. To develop a probabilistic forecast of ozone concentrations, we suggest that use be made of the inherent properties of seasonality and autocorrelation in O3 time series. A non-stationary, autocorrelated stochastic process is used to simulate a conditional probability density function (p.d.f.) which quantifies the effects of seasonality and autocorrelation. To illustrate the utility of such a model, the simulated conditional p.d.f. is shown to be clearly superior to an ordinary p.d.f. developed from summer ozone data.},
	number = {3},
	urldate = {2016-03-07},
	journal = {Atmospheric Environment (1967)},
	author = {Robeson, S. M. and Steyn, D. G.},
	year = {1989},
	keywords = {Forecasting, autocorrelation, non-stationarity, O, seasonality, simulation},
	pages = {689--692},
	file = {robeson1989.pdf:files/614/robeson1989.pdf:application/pdf;ScienceDirect Snapshot:files/613/0004698189900164.html:text/html},
}

@article{brunelli_two-days_2007,
	title = {Two-days ahead prediction of daily maximum concentrations of {SO2}, {O3}, {PM10}, {NO2}, {CO} in the urban area of {Palermo}, {Italy}},
	volume = {41},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231006012696},
	doi = {10.1016/j.atmosenv.2006.12.013},
	abstract = {Artificial neural networks are functional alternative techniques in modelling the intricate vehicular exhaust emission dispersion phenomenon. Pollutant predictions are notoriously complex when using either deterministic or stochastic models, which explains why this model was developed using a neural network. Neural networks have the ability to learn about non-linear relationships between the used variables. In this paper a recurrent neural network (Elman model) based forecaster for the prediction of daily maximum concentrations of SO2, O3, PM10, NO2, CO in the city of Palermo is proposed. The effectiveness of the presented forecaster was tested using a time series recorded between 1 January 2003 to 31 December 2004 in eight monitoring stations in urban area of Palermo (Italy). Experimental trials show that the developed and tuned model is appropriate, giving small values of root mean square error (RMSE) , mean absolute error (MAE) and mean square error (MSE). In addition, the related correlation coefficient ranges from 0.72 to 0.97 for each forecasted pollutant, underlying a small difference between the forecasted and the measured values. The above results make the proposed forecaster a powerful tool for pollution management systems.},
	number = {14},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Brunelli, U. and Piazza, V. and Pignato, L. and Sorbello, F. and Vitabile, S.},
	month = may,
	year = {2007},
	keywords = {Automatic pollutant management systems, Neural based forecasting models, Recurrent neural networks},
	pages = {2967--2995},
	file = {ScienceDirect Snapshot:files/585/S1352231006012696.html:text/html},
}

@article{wang_temporal_2013,
	title = {Temporal stability of land use regression models for traffic-related air pollution},
	volume = {64},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231012009272},
	doi = {10.1016/j.atmosenv.2012.09.056},
	abstract = {Background
Land-use regression (LUR) is a cost-effective approach for predicting spatial variability in ambient air pollutant concentrations with high resolution. Models have been widely used in epidemiological studies and are often applied to time periods before or after the period of air quality monitoring used in model development. However, it is unclear how well such models perform when extrapolated over time.
Objective
The objective of this study was to assess the temporal stability of LUR models over a period of 7 years in Metro Vancouver, Canada.
Methods
A set of NO and NO2 LUR models based on 116 measurements were developed in 2003. In 2010, we made 116 measurements again, of which 73 were made at the exact same location as in 2003. We then developed 2010 models using updated data for the same predictor variables used in 2003, and also explored additional variables. Four methods were used to derive model predictions over 7 years, and predictions were compared with measurements to assess the temporal stability of LUR models.
Results
The correlation between 2003 NO and 2010 NO measurements was 0.87 with a mean (sd) decrease of 11.3 (9.9) ppb. For NO2, the correlation was 0.74, with a mean (sd) decrease of 2.4 (3.2) ppb. 2003 and 2010 LUR models explained similar amounts of spatial variation (R2 = 0.59 and R2 = 0.58 for NO; R2 = 0.52 and R2 = 0.63 for NO2, in 2003 and in 2010 respectively). The 2003 models explained more variability in the 2010 measurements (R2 = 0.58–0.60 for NO; R2 = 0.52–0.61 for NO2) than the 2010 models explained in the 2003 measurements (R2 = 0.50–0.55 for NO; R2 = 0.44–0.49 for NO2), and the 2003 models explained as much variability in the 2010 measurements as they did in the 2003 measurements.
Conclusion
LUR models are able to provide reliable estimates over a period of 7 years in Metro Vancouver. When concentrations and their variability are decreasing over time, the predictive power of LUR models is likely to remain the same or to improve in forecasting scenarios, but to decrease in hind-casting scenarios.},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Wang, Rongrong and Henderson, Sarah B. and Sbihi, Hind and Allen, Ryan W. and Brauer, Michael},
	month = jan,
	year = {2013},
	keywords = {Exposure assessment, Land use regression (LUR), Temporal stability, Traffic-related air pollution, Vancouver},
	pages = {312--319},
	file = {ScienceDirect Snapshot:files/545/S1352231012009272.html:text/html},
}

@article{khokhlov_short-range_2008,
	title = {Short-range forecast of atmospheric pollutants using non-linear prediction method},
	volume = {42},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231008006249},
	doi = {10.1016/j.atmosenv.2008.06.023},
	abstract = {In this paper chaotic behavior in the nitrogen dioxide and sulphurous anhydride concentration time series at two sites in Gdansk region is investigated. To reconstruct an attractor, the time delay and embedding dimension are needed. The former is determined by the methods of autocorrelation function and average mutual information, and the latter is calculated by means of correlation dimension method and algorithm of false nearest neighbors. It was shown that the low-dimensional chaos existed in the time series under investigation. The spectrum of Lyapunov exponents was reconstructed as well as both Kaplan–Yorke dimension and Kolmogorov entropy that inversely proportional to the predictability limit are calculated. Non-linear prediction method is used for the time series. It is shown that even though the simple procedure is used to construct the non-linear model, the results are quite satisfactory.},
	number = {31},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Khokhlov, Valeriy N. and Glushkov, Alexander V. and Loboda, Nataliya S. and Bunyakova, Yulia Y.},
	month = oct,
	year = {2008},
	keywords = {Atmospheric pollutants, Chaos theory, Lyapunov exponents, Non-linear modelling},
	pages = {7284--7292},
	file = {ScienceDirect Snapshot:files/546/S1352231008006249.html:text/html},
}

@article{cheng_predicting_2011,
	title = {Predicting daily ozone concentration maxima using fuzzy time series based on a two-stage linguistic partition method},
	volume = {62},
	issn = {0898-1221},
	url = {http://www.sciencedirect.com/science/article/pii/S0898122111005244},
	doi = {10.1016/j.camwa.2011.06.044},
	abstract = {Air pollution is a result of global warming, greenhouse effects, and acid rain. Especially in highly industrialization areas, air pollution has become a major environmental issue. Poor air quality has both acute and chronic effects on human health. The detrimental effects of ambient ozone on human health and the Earth’s ecosystem continue to be a national concern in Taiwan. The pollutant standard index (PSI) has been adopted to assess the degree of air pollution in Taiwan. The standardized daily air quality report provides a simple number on a scale of 0 to 500 related to the health effects of air quality levels. The report focuses on health and the current PSI subindices to reflect measured ozone (O3) concentrations. Therefore, this study uses the O3 attribute to evaluate air quality. In an effort to forecast daily maximum ozone concentrations, many researchers have developed daily ozone forecasting models. However, this continuing worldwide environmental problem suggests the need for more accurate models. This paper proposes two new fuzzy time series based on a two-stage linguistic partition method to predict air quality with daily maximum O3 concentration: Stage 1, use the fuzzy time series based on the cumulative probability distribution approach (CPDA) to partition the universe of discourse into seven intervals; Stage 2, use two linguistic partition methods, the CPDA and the uniform discretion method (UDM), to repartition each interval into three subintervals. To verify the forecasting performance of the proposed methods in detail, the practical collected data is used as and evaluating dataset; five other methodologies (AR, MA, ARMA, Chen’s and Yu’s) are used as comparison models. The proposed methods both show a greatly improved performance in daily maximal ozone concentration prediction accuracy compared with the other models.},
	number = {4},
	urldate = {2015-09-03},
	journal = {Computers \& Mathematics with Applications},
	author = {Cheng, Ching-Hsue and Huang, Sue-Fen and Teoh, Hia-Jong},
	month = aug,
	year = {2011},
	keywords = {Air quality, Cumulative probability distribution approach, Fuzzy time series, Pollutant standards index, Uniform discretion method},
	pages = {2016--2028},
	file = {ScienceDirect Snapshot:files/591/S0898122111005244.html:text/html},
}

@article{gardner_neural_1999,
	title = {Neural network modelling and prediction of hourly {NOx} and {NO2} concentrations in urban air in {London}},
	volume = {33},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231098002301},
	doi = {10.1016/S1352-2310(98)00230-1},
	abstract = {Multilayer perceptron (MLP) neural networks were trained to model hourly NOx and NO2 pollutant concentrations in Central London from basic hourly meteorological data. Results have shown that the models perform well when compared to previous attempts to model the same pollutants using regression based models. This work also illustrates that MLP neural networks are capable of resolving complex patterns of source emissions without any explicit external guidance.},
	number = {5},
	urldate = {2016-03-07},
	journal = {Atmospheric Environment},
	author = {Gardner, M. W. and Dorling, S. R.},
	month = feb,
	year = {1999},
	keywords = {Air quality modelling, Multilayer Perceptron, artificial neural network, Nitrogen oxides, Primary pollutant},
	pages = {709--719},
	file = {ScienceDirect Full Text PDF:files/553/Gardner y Dorling - 1999 - Neural network modelling and prediction of hourly .pdf:application/pdf;ScienceDirect Snapshot:files/552/S1352231098002301.html:text/html},
}

@article{hrust_neural_2009,
	title = {Neural network forecasting of air pollutants hourly concentrations using optimised temporal averages of meteorological variables and pollutant concentrations},
	volume = {43},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231009006669},
	doi = {10.1016/j.atmosenv.2009.07.048},
	abstract = {The new method for the forecasting hourly concentrations of air pollutants is presented in the paper. The method was developed for a site in urban residential area in city of Zagreb, Croatia, for four air pollutants (NO2, O3, CO and PM10). Meteorological variables and concentrations of the respective pollutant were taken as predictors. A novel approach, based on families of univariate regression models, was employed in selecting the averaging intervals for input variables. For each variable and each averaging period between 1 and 97 h, a separate model was built. By inspecting values of the coefficient of correlation between measured and modelled concentrations, optimal averaging periods for each variable were selected. A new dataset for building the forecasting model was then calculated as temporal moving averages (running means) of former variables. A multi-layer perceptron type of neural networks is used as the forecasting model. Index of agreement, calculated for the entire dataset including the data for model building, ranged from 0.91 to 0.97 for the respective pollutants. As suggested by the analysis of the relative importance of the input variables, different agreements for different pollutants are likely due to different sources and production mechanisms of investigated pollutants. A comparison of the new method with more traditional method, which takes hourly averages of the forecast hour as input variables, showed similar or better performance. The model was developed for the purpose of public-health-oriented air quality forecasting, aiming to use a numerical weather forecast model for the prediction of the part of input data yet unknown at the forecasting time. It is to expect that longer term averages used as inputs in the proposed method will contribute to smaller input errors and the greater accuracy of the model.},
	number = {35},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Hrust, Lovro and Klaić, Zvjezdana Bencetić and Križan, Josip and Antonić, Oleg and Hercog, Predrag},
	month = nov,
	year = {2009},
	keywords = {Air quality forecasting, Model input selection, Multi-layer perceptron neural networks},
	pages = {5588--5596},
	file = {ScienceDirect Snapshot:files/554/S1352231009006669.html:text/html},
}

@article{kiesewetter_modelling_2015,
	title = {Modelling {PM2}.5 impact indicators in {Europe}: {Health} effects and legal compliance},
	issn = {1364-8152},
	shorttitle = {Modelling {PM2}.5 impact indicators in {Europe}},
	url = {http://www.sciencedirect.com/science/article/pii/S1364815215000808},
	doi = {10.1016/j.envsoft.2015.02.022},
	abstract = {Long-term exposure to fine particulate matter (PM2.5) has been shown to have significant negative impacts on human health. It is estimated that current levels of air pollution shorten the statistical life expectancy of European citizens by several months. The GAINS integrated assessment model calculates shortening of life expectancy from population exposure to PM2.5 using epidemiologically-derived health impact functions. In addition, GAINS estimates PM2.5 concentrations at 1875 air quality monitoring stations located in diverse environments ranging from remote background locations to busy street canyons. In this article, different approaches to dealing with the PM2.5 pollution problem are compared. We assess for the present and future the attainment of EU and WHO air quality standards for PM2.5 and estimate the loss of life expectancy under different policy scenarios developed for the ongoing revision of the EU Air Quality Legislation.},
	urldate = {2015-06-01},
	journal = {Environmental Modelling \& Software},
	author = {Kiesewetter, Gregor and Schoepp, Wolfgang and Heyes, Chris and Amann, Markus},
	year = {2015},
	keywords = {Air quality, GAINS, Integrated assessment, Particulate Matter, Premature mortality},
	file = {ScienceDirect Full Text PDF:files/593/Kiesewetter et al. - Modelling PM2.5 impact indicators in Europe Healt.pdf:application/pdf;ScienceDirect Snapshot:files/555/S1364815215000808.html:text/html},
}

@article{siwek_improving_2012,
	title = {Improving the accuracy of prediction of {PM10} pollution by the wavelet transformation and an ensemble of neural predictors},
	volume = {25},
	issn = {0952-1976},
	url = {http://www.sciencedirect.com/science/article/pii/S0952197611002119},
	doi = {10.1016/j.engappai.2011.10.013},
	abstract = {The paper presents the application of wavelet transformation and neural network ensemble to the accurate forecasting of the daily average concentration of particulate matter of diameter up to 10 μm (PM10). Few neural predictors are applied: the multilayer perceptron, radial basis function, Elman network and support vector machine as well as one linear ARX model. They are used for prediction in combination with wavelet decomposition, forming many individual prediction results that will be combined in an ensemble. The important role in presented approach fulfills the wavelet transformation and the integration of this ensemble. We have proposed solution applying the additional neural network responsible for the final forecast (integration of all particular prediction results). The numerical experiments for prediction of the daily concentration of the PM10 pollution in Warsaw are presented. They have shown good overall accuracy of prediction in terms of all investigated measures of quality.},
	number = {6},
	urldate = {2015-09-03},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Siwek, K. and Osowski, S.},
	month = sep,
	year = {2012},
	keywords = {Neural networks, Wavelet transformation, Ensemble of predictors, PM10 prediction},
	pages = {1246--1258},
	file = {ScienceDirect Snapshot:files/557/S0952197611002119.html:text/html},
}

@article{ibarra-berastegi_diagnosis_2008,
	title = {From diagnosis to prognosis for forecasting air pollution using neural networks: {Air} pollution monitoring in {Bilbao}},
	volume = {23},
	issn = {1364-8152},
	shorttitle = {From diagnosis to prognosis for forecasting air pollution using neural networks},
	url = {http://www.sciencedirect.com/science/article/pii/S1364815207001740},
	doi = {10.1016/j.envsoft.2007.09.003},
	abstract = {This work focuses on the prediction of hourly levels up to 8 h ahead for five pollutants (SO2, CO, NO2, NO and O3) and six locations in the area of Bilbao (Spain). To that end, 216 models based on neural networks (NNs) were built. The database used to fit the NNs were historical records of the traffic, meteorological and air pollution networks existing in the area corresponding to year 2000. Then, the models were tested on data from the same networks but corresponding to year 2001. At a first stage, for each of the 216 cases, 100 models based on different types of neural networks were built using data corresponding to year 2000. The final identification of the best model was made under the criteria of simultaneously having at a 95\% confidence level the best values of R2, d1, FA2 and RMSE when applied to data of year 2001. The number of hourly cases in which due to gaps in data predictions were possible range from 11\% to 38\% depending on the sensor. Depending on the pollutant, location and number of hours ahead the prediction is made, different types of models were selected. The use of these models based on NNs can provide Bilbao's air pollution network originally designed for diagnosis purposes, with short-term, real time forecasting capabilities. The performance of these models at the different sensors in the area range from a maximum value of R2 = 0.88 for the prediction of NO2 1 h ahead, to a minimum value of R2 = 0.15 for the prediction of ozone 8 h ahead. These boundaries and the limitation in the number of cases that predictions are possible represent the maximum forecasting capability that Bilbao's network can provide in real-life operating conditions.},
	number = {5},
	urldate = {2015-09-03},
	journal = {Environmental Modelling \& Software},
	author = {Ibarra-Berastegi, Gabriel and Elias, Ana and Barona, Astrid and Saenz, Jon and Ezcurra, Agustin and Diaz de Argandoña, Javier},
	month = may,
	year = {2008},
	keywords = {Air pollution forecasting, Air quality network, Bilbao, Fluid mechanics, Neural networks, Photochemistry, Traffic network},
	pages = {622--637},
	file = {ScienceDirect Snapshot:files/443/S1364815207001740.html:text/html;ScienceDirect Snapshot:files/595/S1364815207001740.html:text/html},
}

@article{ryan_forecasting_1995,
	title = {Forecasting severe ozone episodes in the {Baltimore} metropolitan area},
	volume = {29},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/1352231094003022},
	doi = {10.1016/1352-2310(94)00302-2},
	abstract = {In order to protect sensitive individuals and support possible episodic emissions control efforts, a pilot program was undertaken to forecast episodes of high ozone concentrations for 24–72 h in the Baltimore metropolitan area. This program utilized Classification and Regression Tree (CART) algorithms as well as standard regression analysis and expert forecasts. All approaches tend to underpredict peak O3 concentrations at 24 h. The low bias varied from 5 ppbv for the expert forecast to 7 ppbv for the CART forecast. The standard error for forecast O3 varied from 17 ppbv for the regression forecast to 23 for the CART forecast. For high O3 (\&gt; 120 ppbv) events, the expert forecast has the best success with a detection rate of 50\%, and skill scores varying from 0.40 to 0.50. For expert forecasts of greater than 115 ppbv, verifying against observations of greater than 120 ppbv, the detection rate rises to 75\% with skill scores of 0.60–0.66. One of the principal sources of forecast error was underprediction of mid-day surface temperature by the standard meteorological models during extremely warm episodes. When perfect forecasts of meteorological parameters are utilized, the forecast skill of the objective measures increases to approximately the skill of the expert forecasts. Improvements can be made by utilizing more recent data for initialization of the ozone regression equations and the use of local forecasts to supplement temperature forecasts during warm periods.},
	number = {17},
	urldate = {2016-03-07},
	journal = {Atmospheric Environment},
	author = {Ryan, William F.},
	month = sep,
	year = {1995},
	pages = {2387--2398},
	file = {ScienceDirect Full Text PDF:files/596/Ryan - 1995 - Forecasting severe ozone episodes in the Baltimore.pdf:application/pdf;ScienceDirect Snapshot:files/559/1352231094003022.html:text/html},
}

@article{ziomas_forecasting_1995,
	title = {Forecasting peak pollutant levels from meteorological variables},
	volume = {29},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/135223109500131H},
	doi = {10.1016/1352-2310(95)00131-H},
	abstract = {The main objective of this paper is to present analytical models relating maximum pollutant concentrations in urban areas with meteorological and other variables. The analysis is based on measurements from Greater Athens Area and is restricted in only one pollutant of special interest, namely N02. The meteorological variables, used in analytical modeling for forecasting pollution concentrations, cover the most important atmospheric processes favoring pollution episodes. The selection of the variables was based both on extensive correlation analysis and on the existing knowledge from the scientific literature. The evaluation of the developed forecasting models showed that their degree of success is promising. The final model equations derived are simple and they can be used easily for operational forecasts from the air quality management authorities.},
	number = {24},
	urldate = {2016-03-07},
	journal = {Atmospheric Environment},
	author = {Ziomas, Ioannis C. and Melas, Dimitrios and Zerefos, Christos S. and Bais, Alkiviadis F. and Paliatsos, Athanasios G.},
	year = {1995},
	pages = {3703--3711},
	file = {ScienceDirect Full Text PDF:files/561/Ziomas et al. - 1995 - Forecasting peak pollutant levels from meteorologi.pdf:application/pdf;ScienceDirect Snapshot:files/560/135223109500131H.html:text/html},
}

@article{osowski_forecasting_2007,
	title = {Forecasting of the daily meteorological pollution using wavelets and support vector machine},
	volume = {20},
	issn = {0952-1976},
	url = {http://www.sciencedirect.com/science/article/pii/S0952197606001904},
	doi = {10.1016/j.engappai.2006.10.008},
	abstract = {The paper presents the method of daily air pollution forecasting by using support vector machine (SVM) and wavelet decomposition. Based on the observed data of NO2, CO, SO2 and dust, for the past years and actual meteorological parameters, like wind, temperature, humidity and pressure, we propose the forecasting approach, applying the neural network of SVM type, working in the regression mode. To obtain the acceptable accuracy of forecast we decompose the measured time series data into wavelet representation and predict the wavelet coefficients. On the basis of these predicted values the final forecasting is prepared. The paper presents the results of numerical experiments on the basis of the measurements made by the meteorological stations, situated in the northern region of Poland.},
	number = {6},
	urldate = {2015-09-03},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Osowski, Stanislaw and Garanty, Konrad},
	month = sep,
	year = {2007},
	keywords = {Support vector machine, Generalization ability, Neural network predictors, Pollution forecasting, Wavelet decomposition},
	pages = {745--755},
	file = {ScienceDirect Snapshot:files/563/S0952197606001904.html:text/html},
}

@article{lin_forecasting_2011,
	title = {Forecasting concentrations of air pollutants by logarithm support vector regression with immune algorithms},
	volume = {217},
	issn = {0096-3003},
	url = {http://www.sciencedirect.com/science/article/pii/S0096300310011859},
	doi = {10.1016/j.amc.2010.11.055},
	abstract = {The need to minimize the potential impact of air pollutants on humans has made the accurate prediction of concentrations of air pollutants a crucial subject in environmental research. Support vector regression (SVR) models have been successfully employed to solve time series problems in many fields. The use of SVR models for forecasting concentrations of air pollutants has not been widely investigated. Data preprocessing procedures and the parameter selection of SVR models can radically influence forecasting performance. This study proposes a support vector regression with logarithm preprocessing procedure and immune algorithms (SVRLIA) model which takes advantage of the structural risk minimization of SVR models, the data smoothing of preprocessing procedures, and the optimization of immune algorithms, in order to more accurately forecast concentrations of air pollutants. Three pollutants, namely particulate matter (PM10), nitrogen oxide, (NOx), and nitrogen dioxide (NO2), are collected and examined to determine the feasibility of the developed SVRLIA model. Experimental results reveal that the SVRLIA model can accurately forecast concentrations of air pollutants.},
	number = {12},
	urldate = {2015-09-03},
	journal = {Applied Mathematics and Computation},
	author = {Lin, Kuo-Ping and Pai, Ping-Feng and Yang, Shun-Ling},
	month = feb,
	year = {2011},
	keywords = {support vector regression, Concentrations of air pollutants, Data preprocessing, Immune algorithms, Parameter selection},
	pages = {5318--5327},
	file = {ScienceDirect Snapshot:files/564/S0096300310011859.html:text/html},
}

@article{kurt_forecasting_2010,
	title = {Forecasting air pollutant indicator levels with geographic models 3 days in advance using neural networks},
	volume = {37},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S095741741000504X},
	doi = {10.1016/j.eswa.2010.05.093},
	abstract = {An early warning system for air quality control requires an accurate and dependable forecasting of pollutants in the air. In this study methods based on geographic forecasting models using neural networks (GFM\_NN) are presented. The air pollutant data from 10 different air quality monitoring stations in Istanbul was used in forecasting sulfur dioxide (SO2), carbon monoxide (CO) and particulate matter (PM10) levels 3 days in advance for the Besiktas district. Daily meteorological forecasts as well as the air pollutant indicator values were used as input to feed-forward back-propagation neural networks. The experimental verification of the models was conducted in one-year period between August 2005 and August 2006. The observed and forecasted bands were used to compute the forecasting error. The simplest geographic model proposed uses the observed air pollution indicator values from a selected neighboring district. Where as the second model uses two neighboring districts instead of one. A third model considers the distance between the triangulating districts and the district whose air pollutant level is being forecasted. Each model is tested with at least two different sets of sites. The findings are quite satisfactory. When the right neighboring districts are chosen, the geographic models always yield lower error than non-geographic models. The distance-based geographic model produces considerably lower error than the non-geographic plain model. We argue that models proposed here can be used in urban air pollution forecasting.},
	number = {12},
	urldate = {2015-09-03},
	journal = {Expert Systems with Applications},
	author = {Kurt, Atakan and Oktay, Ayşe Betül},
	month = dec,
	year = {2010},
	keywords = {Forecasting, prediction, Neural networks, PM10, Air pollution, Data Mining, Air quality, CO, Environmental expert systems, Geographic forecasting, SO2, Spatial classification},
	pages = {7986--7992},
	file = {ScienceDirect Snapshot:files/597/S095741741000504X.html:text/html},
}

@article{niska_evolving_2004,
	series = {Intelligent {Control} and {Signal} {Processing}},
	title = {Evolving the neural network model for forecasting air pollution time series},
	volume = {17},
	issn = {0952-1976},
	url = {http://www.sciencedirect.com/science/article/pii/S0952197604000119},
	doi = {10.1016/j.engappai.2004.02.002},
	abstract = {The modelling of real-world processes such as air quality is generally a difficult task due to both their chaotic and non-linear phenomenon and high dimensional sample space. Despite neural networks (NN) have been used successfully in this domain, the selection of network architecture is still problematic and time consuming task when developing a model for practical situation. This paper presents a study where a parallel genetic algorithm (GA) is used for selecting the inputs and designing the high-level architecture of a multi-layer perceptron model for forecasting hourly concentrations of nitrogen dioxide at a busy urban traffic station in Helsinki. In addition, the tuning of GA's parameters for the problem is considered in experimental way. The results showed that the GA is a capable tool for tackling the practical problems of neural network design. However, it was observed that the evaluation of NN models is a computationally expensive process, which set limits for the search techniques.},
	number = {2},
	urldate = {2015-09-03},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Niska, Harri and Hiltunen, Teri and Karppinen, Ari and Ruuskanen, Juhani and Kolehmainen, Mikko},
	month = mar,
	year = {2004},
	keywords = {Urban air pollution, Feed-forward networks, Parallel genetic algorithms, Time series forecasting},
	pages = {159--167},
	file = {ScienceDirect Snapshot:files/599/S0952197604000119.html:text/html},
}

@article{niska_evaluation_2005,
	title = {Evaluation of an integrated modelling system containing a multi-layer perceptron model and the numerical weather prediction model {HIRLAM} for the forecasting of urban airborne pollutant concentrations},
	volume = {39},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231005006242},
	doi = {10.1016/j.atmosenv.2005.07.035},
	abstract = {In this paper, a multi-layer perceptron (MLP) model and the Finnish variant of the numerical weather prediction model HIRLAM (High Resolution Limited Area Model) were integrated and evaluated for the forecasting in time of urban pollutant concentrations. The forecasts of the combination of the MLP and HIRLAM models are compared with the corresponding forecasts of the MLP models that utilise meteorologically pre-processed input data. A novel input selection method based on the use of a multi-objective genetic algorithm (MOGA) is applied in conjunction with the sensitivity analysis to reduce the excessively large number of potential meteorological input variables; its use improves the performance of the MLP model. The computed air quality forecasts contain the sequential hourly time series of the concentrations of nitrogen dioxide (NO2) and fine particulate matter (PM2.5) from May 2000 to April 2003; the corresponding concentrations have also been measured at two urban air quality stations in Helsinki. The results obtained with the MLP models that use HIRLAM forecasts show fairly good overall agreement for both pollutants. The model performance is substantially better, when the HIRLAM forecasts are used, compared with those obtained both using either HIRLAM analysis data or meteorological pre-processor, for both pollutants. The performance of the currently widely used statistical forecasting methods (such as those based on neural networks) could therefore be significantly improved by using the forecasts of NWP models, instead of the conventionally utilised directly measured or meteorological pre-processed input data. However, the performance of all operational models considered is relatively worse in the course of air pollution episodes.},
	number = {35},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Niska, Harri and Rantamäki, Minna and Hiltunen, Teri and Karppinen, Ari and Kukkonen, Jaakko and Ruuskanen, Juhani and Kolehmainen, Mikko},
	month = nov,
	year = {2005},
	keywords = {Air quality forecasting, Multi-Layer Perceptron, Model input selection, Genetic algorithms, Numerical weather prediction},
	pages = {6524--6536},
	file = {niska2005.pdf:files/566/niska2005.pdf:application/pdf;ScienceDirect Snapshot:files/565/S1352231005006242.html:text/html},
}

@article{vlachogianni_evaluation_2011,
	title = {Evaluation of a multiple regression model for the forecasting of the concentrations of {NOx} and {PM10} in {Athens} and {Helsinki}},
	volume = {409},
	issn = {0048-9697},
	url = {http://www.sciencedirect.com/science/article/pii/S0048969711000064},
	doi = {10.1016/j.scitotenv.2010.12.040},
	abstract = {Forecasting models based on stepwise multiple linear regression (MLR) have been developed for Athens and Helsinki. The predictor variables were the hourly concentrations of pollutants (NO, NO2, NOx, CO, O3, PM2.5 and PM10) and the meteorological variables (ambient temperature, wind speed/direction, and relative humidity) and in case of Helsinki also Monin-Obukhov length and mixing height of the present day. The variables to be forecasted are the maximum hourly concentrations of PM10 and NOx, and the daily average PM10 concentrations of the next day. The meteorological pre-processing model MPP-FMI was used for computing the Monin-Obukhov length and the mixing height. The limitations of such statistical models include the persistence of both the meteorological and air quality situation; the model cannot account for rapid changes (on a temporal scale of hours or less than a day) that are commonly associated, e.g., with meteorological fronts, or episodes of a long-range transport origin. We have selected the input data for the model from one urban background and one urban traffic station both in Athens and Helsinki, in 2005. We have used various statistical evaluation parameters to analyze the performance of the models, and inter-compared the performance of the predictions for both cities. Forecasts from the MLR model were also compared to those from an Artificial Neural Network model (ANN) to investigate, if there are substantial gains that might justify the additional computational effort. The best predictor variables for both cities were the concentrations of NOx and PM10 during the evening hours as well as wind speed, and the Monin-Obukhov length. In Athens, the index of agreement (IA) for NOx ranged from 0.77 to 0.84 and from 0.69 to 0.72, in the warm and cold periods of the year. In Helsinki, the corresponding values of IA ranged from 0.32 to 0.82 and from 0.67 to 0.86 for the warm and cold periods. In case of Helsinki the model accuracy was expectedly better on the average, when Monin-Obukhov length and mixing height were included as predictor variables. The models provide better forecasts of the daily average concentration, compared with the maximum hourly concentration for PM10. The results derived by the ANN model where only slightly better than the ones derived by the MLR methodology. The results therefore suggest that the MLR methodology is a useful and fairly accurate tool for regulatory purposes.},
	number = {8},
	urldate = {2015-09-03},
	journal = {Science of The Total Environment},
	author = {Vlachogianni, A. and Kassomenos, P. and Karppinen, Ari and Karakitsios, S. and Kukkonen, Jaakko},
	month = mar,
	year = {2011},
	keywords = {Air quality forecasting, PM10, Model evaluation, Multiple regression analysis, NOx},
	pages = {1559--1571},
	file = {ScienceDirect Snapshot:files/567/S0048969711000064.html:text/html},
}

@article{debry_ensemble_2014,
	title = {Ensemble forecasting with machine learning algorithms for ozone, nitrogen dioxide and {PM10} on the {Prev}'{Air} platform},
	volume = {91},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231014002349},
	doi = {10.1016/j.atmosenv.2014.03.049},
	abstract = {This paper presents the application of an ensemble forecasting approach to the Prev’Air operational platform. This platform aims at forecasting maps, on a daily basis, for ozone, nitrogen dioxide and particulate matter. It relies on several air quality models which differ by their physical parameterizations, their input data and numerical strategies, so that one model may perform better with respect to observations for a given pollutant, at a given time and location. We apply sequential aggregation methods to this ensemble of models, which has already proved good potential in previous research papers. Compared to these studies, the novelties of this paper are the variety of models, the real operational context, which requires robustness assessment, and the application to several pollutants. In this paper, we first introduce the ensemble forecasting methods and the operational platform Prev’Air along with its models. Then, the sequential aggregation performance and robustness are assessed using two different data sets.

The results with the discounted ridge regression method show that the errors of the forecasts are respectively reduced by at least 29\%, 35\% and 19\% for hourly, daily and peak O3 concentrations, by 19\%, 26\% and 20\% for hourly, daily and peak NO2 concentrations, and finally by 17\%, 19\% and 11\% for hourly, daily and peak PM10 concentrations. At last, we give a first insight of the ensemble ability to forecast threshold exceedances.},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Debry, E. and Mallet, V.},
	month = jul,
	year = {2014},
	keywords = {Nitrogen Dioxide, Ozone, Particulate Matter, Chemical transport models, Ensemble forecast, Operational forecasting, Sequential aggregation, Threshold exceedance},
	pages = {71--84},
	file = {ScienceDirect Snapshot:files/568/S1352231014002349.html:text/html},
}

@article{chen_ensemble_2013,
	title = {Ensemble and enhanced {PM10} concentration forecast model based on stepwise regression and wavelet analysis},
	volume = {74},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231013002471},
	doi = {10.1016/j.atmosenv.2013.04.002},
	abstract = {An ensemble and enhanced PM10 (particulate matter with a diameter less than 10 μm) concentration forecast model was established in eastern China based on data from 2005 to 2009. The enhanced model consists of a single stepwise regression forecast model and a combined forecast model based on wavelet decomposition and stepwise regression. Six individual forecast results were obtained with a combined model that can predict PM10 concentrations at multiple scales. By decomposing variables into detailed and approximated components in six scales and with the application of stepwise regression, the best-fitted forecast models were established in each component of the different scales. Then, the predicted results of the detail and approximation components were reconstructed in each scale as the enhanced prediction. A regional model was established for eastern China. The accuracy rate of each forecasted result by the regional model was calculated using testing data from 2010 based on the needs of operational forecasting. Precision evaluations were also performed. A comparatively higher accuracy was obtained by the combined model. The advantage of predicting the PM10 concentration with the combined model had wide spatial and temporal suitability. An enhanced forecast model was established for each city of eastern China with improvements, where all the predicted results in each city were evaluated by the accuracy rate and precision validation. In each city, the best-fitted model with the highest precision was selected and combined in an ensemble. The ensemble and enhanced forecast model had a significant improvement in accuracy rate and the highest precision of PM10 concentration forecasting in eastern China.},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Chen, Yuanyuan and Shi, Runhe and Shu, Shijie and Gao, Wei},
	month = aug,
	year = {2013},
	keywords = {Enhanced model, Ensemble model, PM10 concentration, Stepwise regression, Wavelet analysis},
	pages = {346--359},
	file = {ScienceDirect Snapshot:files/569/S1352231013002471.html:text/html},
}

@article{balaguer_ballester_effective_2002,
	title = {Effective 1-day ahead prediction of hourly surface ozone concentrations in eastern {Spain} using linear models and neural networks},
	volume = {156},
	issn = {0304-3800},
	url = {http://www.sciencedirect.com/science/article/pii/S0304380002001278},
	doi = {10.1016/S0304-3800(02)00127-8},
	abstract = {The aim of this research was to develop pure predictive models in order to provide 24 h advance forecasts of the hourly ozone concentration for the rural site of Carcagente (Valencia, Spain) and the urban sites of Paterna (Valencia, Spain) and Alcoy (Alicante, Spain) over 4 years from 1996 to 1999. The peculiarity of the model presented here is that it uses past and previously predicted information of inputs exclusively, thus being this is the first genuine 24 h advance O3 predictive model with neural networks. We used autoregressive-moving average with exogenous inputs (ARMAX), multilayer perceptrons and FIR neural networks. Five performance measures yield reasonably good results in the three sampling sites. The results indicate that the models developed predict the O3 time series more effectively compared with previous procedures based on dynamical system theory. The neural network's models yield better results than linear models when exogenous inputs are included. The prediction accuracy of these models enables, for the first time, an effective warning to be made in cases where EU public information threshold values are exceeded.},
	number = {1},
	urldate = {2015-09-03},
	journal = {Ecological Modelling},
	author = {Balaguer Ballester, E and Camps i Valls, G and Carrasco-Rodriguez, J. L and Soria Olivas, E and del Valle-Tascon, S},
	month = oct,
	year = {2002},
	keywords = {Neural networks, Ozone forecasting, ARMAX, Atmospheric pollution, Public advisories},
	pages = {27--41},
	file = {ScienceDirect Snapshot:files/601/S0304380002001278.html:text/html},
}

@article{gardner_artificial_1998,
	title = {Artificial neural networks (the multilayer perceptron)—a review of applications in the atmospheric sciences},
	volume = {32},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231097004470},
	doi = {10.1016/S1352-2310(97)00447-0},
	abstract = {Artificial neural networks are appearing as useful alternatives to traditional statistical modelling techniques in many scientific disciplines. This paper presents a general introduction and discussion of recent applications of the multilayer perceptron, one type of artificial neural network, in the atmospheric sciences.},
	number = {14–15},
	urldate = {2016-03-08},
	journal = {Atmospheric Environment},
	author = {Gardner, M. W and Dorling, S. R},
	month = aug,
	year = {1998},
	keywords = {Artificial intelligence, Statistical modelling, Neural network, backpropagation},
	pages = {2627--2636},
	file = {gardner1998.pdf:files/571/gardner1998.pdf:application/pdf;ScienceDirect Snapshot:files/605/S1352231097004470.html:text/html},
}

@article{westerlund_application_2014,
	title = {Application of air quality combination forecasting to {Bogota}},
	volume = {89},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231014001113},
	doi = {10.1016/j.atmosenv.2014.02.015},
	abstract = {The bulk of existing work on the statistical forecasting of air quality is based on either neural networks or linear regressions, which are both subject to important drawbacks. In particular, while neural networks are complicated and prone to in-sample overfitting, linear regressions are highly dependent on the specification of the regression function. The present paper shows how combining linear regression forecasts can be used to circumvent all of these problems. The usefulness of the proposed combination approach is verified using both Monte Carlo simulation and an extensive application to air quality in Bogota, one of the largest and most polluted cities in Latin America.},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Westerlund, Joakim and Urbain, Jean-Pierre and Bonilla, Jorge},
	month = jun,
	year = {2014},
	keywords = {Neural networks, Air quality forecasting, Bogota, Forecast combination},
	pages = {22--28},
	file = {ScienceDirect Snapshot:files/608/S1352231014001113.html:text/html},
}

@article{baldasano_annual_2011,
	title = {An annual assessment of air quality with the {CALIOPE} modeling system over {Spain}},
	volume = {409},
	issn = {0048-9697},
	url = {http://www.sciencedirect.com/science/article/pii/S0048969711000787},
	doi = {10.1016/j.scitotenv.2011.01.041},
	abstract = {The CALIOPE project, funded by the Spanish Ministry of the Environment, aims at establishing an air quality forecasting system for Spain. With this goal, CALIOPE modeling system was developed and applied with high resolution (4 km × 4 km, 1 h) using the HERMES emission model (including emissions of resuspended particles from paved roads) specifically built up for Spain. The present study provides an evaluation and the assessment of the modeling system, coupling WRF-ARW/HERMES/CMAQ/BSC-DREAM8b for a full-year simulation in 2004 over Spain. The evaluation focuses on the capability of the model to reproduce the temporal and spatial distribution of gas phase species (NO2, O3, and SO2) and particulate matter (PM10) against ground-based measurements from the Spanish air quality monitoring network. The evaluation of the modeling results on an hourly basis shows a strong dependency of the performance of the model on the type of environment (urban, suburban and rural) and the dominant emission sources (traffic, industrial, and background). The O3 chemistry is best represented in summer, when mean hourly variability and high peaks are generally well reproduced. The mean normalized error and bias meet the recommendations proposed by the United States Environmental Protection Agency (US-EPA) and the European regulations. Modeled O3 shows higher performance for urban than for rural stations, especially at traffic stations in large cities, since stations influenced by traffic emissions (i.e., high-NOx environments) are better characterized with a more pronounced daily variability. NOx/O3 chemistry is better represented under non-limited-NO2 regimes. SO2 is mainly produced from isolated point sources (power generation and transformation industries) which generate large plumes of high SO2 concentration affecting the air quality on a local to national scale where the meteorological pattern is crucial. The contribution of mineral dust from the Sahara desert through the BSC-DREAM8b model helps to satisfactorily reproduce episodic high PM10 concentration peaks at background stations. The model assessment indicates that one of the main air quality-related problems in Spain is the high level of O3. A quarter of the Iberian Peninsula shows more than 30 days exceeding the value 120 μg m−3 for the maximum 8-h O3 concentration as a consequence of the transport of O3 precursors downwind to/from the Madrid and Barcelona metropolitan areas, and industrial areas and cities in the Mediterranean coast.},
	number = {11},
	urldate = {2015-09-04},
	journal = {Science of The Total Environment},
	author = {Baldasano, J. M. and Pay, M. T. and Jorba, O. and Gassó, S. and Jiménez-Guerrero, P.},
	month = may,
	year = {2011},
	keywords = {Model evaluation, Air quality, High resolution, O3 exceedances, Spain},
	pages = {2163--2178},
	file = {ScienceDirect Full Text PDF:files/577/Baldasano et al. - 2011 - An annual assessment of air quality with the CALIO.pdf:application/pdf;ScienceDirect Snapshot:files/576/S0048969711000787.html:text/html},
}

@article{mallet_air_2008,
	series = {Advanced {Numerical} {Algorithms} for {Large}-{Scale} {Computations}},
	title = {Air quality modeling: {From} deterministic to stochastic approaches},
	volume = {55},
	issn = {0898-1221},
	shorttitle = {Air quality modeling},
	url = {http://www.sciencedirect.com/science/article/pii/S089812210700733X},
	doi = {10.1016/j.camwa.2007.11.004},
	abstract = {The objective of this article is to investigate the topics related to uncertainties in air quality modeling. A first point is the evaluation of uncertainties for model outputs: Monte Carlo methods and sensitivity analysis are powerful methods for assessing the impact of uncertainties due to model inputs. A second point is devoted to ensemble modeling with multi-models approaches. According to the wide spread in the model outputs, using a unique model, tuned to a small set of observational data, is not relevant in this field. On the basis of ensemble simulations, improved forecasts are given by appropriate algorithms to combine the set of models. The results applied to air quality modeling at continental scale with the Polyphemus system illustrate these methods. The first estimates of uncertainties in inverse modeling experiments are also proposed.},
	number = {10},
	urldate = {2015-09-03},
	journal = {Computers \& Mathematics with Applications},
	author = {Mallet, Vivien and Sportisse, Bruno},
	month = may,
	year = {2008},
	keywords = {Air quality, Ensemble forecast, Data assimilation, Sensitivity analysis, Uncertainty analysis},
	pages = {2329--2337},
	file = {ScienceDirect Full Text PDF:files/579/Mallet and Sportisse - 2008 - Air quality modeling From deterministic to stocha.pdf:application/pdf;ScienceDirect Snapshot:files/578/S089812210700733X.html:text/html},
}

@article{chelani_air_2006,
	title = {Air quality forecasting using a hybrid autoregressive and nonlinear model},
	volume = {40},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S135223100501071X},
	doi = {10.1016/j.atmosenv.2005.11.019},
	abstract = {The usual practices of air quality time-series forecasting are based on applying the models that deal with either the linear or nonlinear patterns. As the linear or nonlinear behavior of the time series is not known in advance, one applies the number of models and finally selects the one, which provides the most accurate results. The air pollutant concentration time series contain patterns that are not purely linear or nonlinear and applying either technique may give inadequate results. This study aims to develop a hybrid methodology that can deal with both the linear and nonlinear structure of the time series. The hybrid model is developed using the combination of autoregressive integrated moving average model, which deals with linear patterns and nonlinear dynamical model. To demonstrate the utility of the proposed technique, nitrogen dioxide concentration observed at a site in Delhi during 1999 to 2003 was utilized. The individual linear and nonlinear models were also applied in order to examine the performance of the hybrid model. The performance is compared for one-step and multi-step ahead forecasts using the error statistics such as mean absolute percentage error and relative error. It is observed that hybrid model outperforms the individual linear and nonlinear models. The exploitation of unique features of linear and nonlinear models makes it a powerful technique to predict the air pollutant concentrations.},
	number = {10},
	urldate = {2015-09-03},
	journal = {Atmospheric Environment},
	author = {Chelani, Asha B. and Devotta, S.},
	month = mar,
	year = {2006},
	keywords = {Time-series forecasting, ARIMA, Hybrid model, Nonlinear dynamics},
	pages = {1774--1780},
	file = {ScienceDirect Snapshot:files/580/S135223100501071X.html:text/html},
}

@misc{noauthor_air_nodate,
	type = {Publication},
	title = {Air {Implementation} {Pilot} - {Lessons} learnt from the implementation of air quality legislation at urban level —},
	url = {http://www.eea.europa.eu/publications/air-implementation-pilot-2013},
	abstract = {Almost three quarters of Europeans live in cities. The air quality in our cities is therefore of significant importance to the health of Europeans. Considerable
progress has been made in the past twenty years in improving urban air quality, but issues remain. A number of different air pollutants such as nitrogen dioxide, particulate matter, and ozone remain above regulated levels, posing a threat to human health. This report describes a European pilot project to help identify and address the reasons underlying this 'gap' in implementation of air quality policy
in 12 European cities, and thereby draw lessons of wider relevance.},
	language = {en},
	urldate = {2015-09-04},
	file = {Air Implementation Pilot.pdf:files/582/Air Implementation Pilot.pdf:application/pdf;Snapshot:files/581/air-implementation-pilot-2013.html:text/html},
}

@article{yildirim_adaptive_2006,
	title = {Adaptive neuro-fuzzy based modelling for prediction of air pollution daily levels in city of {Zonguldak}},
	volume = {63},
	issn = {0045-6535},
	url = {http://www.sciencedirect.com/science/article/pii/S004565350501163X},
	doi = {10.1016/j.chemosphere.2005.08.070},
	abstract = {Air pollution is a growing problem arising from domestic heating, high density of vehicle traffic, electricity production, and expanding commercial and industrial activities, all increasing in parallel with urban population. Monitoring and forecasting of air quality parameters in the urban area are important due to health impact. Artificial intelligent techniques are successfully used in modelling of highly complex and non-linear phenomena. In this study, adaptive neuro-fuzzy logic method has been proposed to estimate the impact of meteorological factors on SO2 and total suspended particular matter (TSP) pollution levels over an urban area. The model forecasts satisfactorily the trends in SO2 and TSP concentration levels, with performance between 75–90\% and 69–80 \%, respectively.},
	number = {9},
	urldate = {2015-09-03},
	journal = {Chemosphere},
	author = {Yildirim, Yilmaz and Bayramoglu, Mahmut},
	month = jun,
	year = {2006},
	keywords = {Fuzzy logic, Neural networks, Modelling, SO2 pollution, Total suspended particle (TSP) pollution},
	pages = {1575--1582},
	file = {ScienceDirect Snapshot:files/584/S004565350501163X.html:text/html;yildirim2006.pdf:files/583/yildirim2006.pdf:application/pdf},
}

@article{breiman_statistical_2001,
	title = {Statistical {Modeling}: {The} {Two} {Cultures} (with comments and a rejoinder by the author)},
	volume = {16},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Statistical {Modeling}},
	url = {http://projecteuclid.org/euclid.ss/1009213726},
	doi = {10.1214/ss/1009213726},
	abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
	number = {3},
	urldate = {2017-03-08},
	journal = {Statistical Science},
	author = {Breiman, Leo},
	month = aug,
	year = {2001},
	mrnumber = {MR1874152},
	zmnumber = {1059.62505},
	pages = {199--231},
	file = {euclid.ss.1009213726.pdf:files/511/euclid.ss.1009213726.pdf:application/pdf;euclid.ss.1009213726.pdf:files/615/euclid.ss.1009213726.pdf:application/pdf;Snapshot:files/515/1009213726.html:text/html;Snapshot:files/616/1009213726.html:text/html},
}

@article{salcedo-sanz_spatial_2009,
	title = {Spatial regression analysis of {NOx} and {O3} concentrations in {Madrid} urban area using {Radial} {Basis} {Function} networks},
	volume = {99},
	issn = {0169-7439},
	url = {http://www.sciencedirect.com/science/article/pii/S016974390900152X},
	doi = {10.1016/j.chemolab.2009.07.012},
	abstract = {This paper discusses the performance of Radial Basis Function networks (RBF) in a problem of spatial regression of pollutants in Madrid. Specifically, the spatial regression of NOx and O3 is considered, in such a way that, starting from a set of measuring points provided by the air quality monitoring network of Madrid, the complete surface of the pollutants in the city is obtained. This pollutant surface can be used as an initial step for modeling intra-urban pollution using land-use regression techniques for example. Also, different works has used a pollutant surface to study the patterns of pollution in different cities in the world and also to establish their air monitoring networks under mathematical criteria. The paper is focussed in analyzing the performance of RBF networks to obtain this first pollutant surface, so different RBF training algorithms are tested in this paper. Specifically, evolutionary-based RBF training algorithms are described, and compared with classical training algorithms for RBF networks with Gaussian kernels. The inclusion of meteorological variables in the RBF networks are also discussed in the paper. The experimental part of the article studies real results of the application of RBF networks to obtain a first pollutant surface of NOx and O3, using the data of the air pollution monitoring network of Madrid and the meteorological network of the city.},
	number = {1},
	urldate = {2016-11-29},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Salcedo-Sanz, S. and Portilla-Figueras, J. A. and Ortiz-Garcia, E. G. and Perez-Bellido, A. M. and Garcia-Herrera, R. and Elorrieta, J. I.},
	month = nov,
	year = {2009},
	keywords = {NOx, Concentration of pollutants, Geographic information system, LUR models, O3, RBF networks, Spatial regression},
	pages = {79--90},
	file = {ScienceDirect Snapshot:files/618/S016974390900152X.html:text/html},
}

@article{kim_review_2015,
	title = {A review on the human health impact of airborne particulate matter},
	volume = {74},
	issn = {0160-4120},
	url = {http://www.sciencedirect.com/science/article/pii/S0160412014002992},
	doi = {10.1016/j.envint.2014.10.005},
	abstract = {Particulate matter (PM) is a key indicator of air pollution brought into the air by a variety of natural and human activities. As it can be suspended over long time and travel over long distances in the atmosphere, it can cause a wide range of diseases that lead to a significant reduction of human life. The size of particles has been directly linked to their potential for causing health problems. Small particles of concern include “inhalable coarse particles” with a diameter of 2.5 to 10 μm and “fine particles” smaller than 2.5 μm in diameter. As the source–effect relationship of PM remains unclear, it is not easy to define such effects from individual sources such as long-range transport of pollution. Because of the potent role of PM and its associated pollutants, detailed knowledge of their human health impacts is of primary importance. This paper summarizes the basic evidence on the health effects of particulate matter. An in-depth analysis is provided to address the implications for policy-makers so that more stringent strategies can be implemented to reduce air pollution and its health effects.},
	urldate = {2016-10-24},
	journal = {Environment International},
	author = {Kim, Ki-Hyun and Kabir, Ehsanul and Kabir, Shamin},
	month = jan,
	year = {2015},
	keywords = {PM10, Particulate Matter, Human health, Particle Size, PM2.5},
	pages = {136--143},
	file = {ScienceDirect Snapshot:files/619/S0160412014002992.html:text/html},
}

@misc{center_for_history_and_new_media_zotero_nodate,
	title = {Zotero {Quick} {Start} {Guide}},
	url = {http://zotero.org/support/quick_start_guide},
	author = {{Center for History and New Media}},
}

@incollection{ferro_deterministic_2011,
	title = {Deterministic {Forecasts} of {Extreme} {Events} and {Warnings}},
	copyright = {Copyright © 2012 John Wiley \& Sons, Ltd},
	isbn = {978-1-119-96000-3},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/9781119960003.ch10/summary},
	abstract = {This chapter contains sections titled:

* Introduction
* Forecasts of extreme events
* Warnings
* Acknowledgements},
	language = {en},
	urldate = {2016-09-01},
	booktitle = {Forecast {Verification}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Ferro, Christopher A. T. and Stephenson, David B.},
	editor = {Jolliffe, Ian T. and Stephenson, David B.},
	year = {2011},
	keywords = {counts of hits, misses, false alarms, deterministic forecasts, warnings, ECMWF, Finley tornado forecasts, forecast verification, national weather services' warnings, threat score or CSI, correct forecasts, weather-related warnings, WMO's THORPEX programme},
	pages = {185--201},
	file = {Snapshot:files/622/summary.html:text/html},
}

@article{ferro_extremal_2011,
	title = {Extremal {Dependence} {Indices}: {Improved} {Verification} {Measures} for {Deterministic} {Forecasts} of {Rare} {Binary} {Events}},
	volume = {26},
	issn = {0882-8156},
	shorttitle = {Extremal {Dependence} {Indices}},
	url = {http://journals.ametsoc.org/doi/abs/10.1175/WAF-D-10-05030.1},
	doi = {10.1175/WAF-D-10-05030.1},
	abstract = {Verifying forecasts of rare events is challenging, in part because traditional performance measures degenerate to trivial values as events become rarer. The extreme dependency score was proposed recently as a nondegenerating measure for the quality of deterministic forecasts of rare binary events. This measure has some undesirable properties, including being both easy to hedge and dependent on the base rate. A symmetric extreme dependency score was also proposed recently, but this too is dependent on the base rate. These two scores and their properties are reviewed and the meanings of several properties, such as base-rate dependence and complement symmetry that have caused confusion are clarified. Two modified versions of the extreme dependency score, the extremal dependence index, and the symmetric extremal dependence index, are then proposed and are shown to overcome all of its shortcomings. The new measures are nondegenerating, base-rate independent, asymptotically equitable, harder to hedge, and have regular isopleths that correspond to symmetric and asymmetric relative operating characteristic curves.},
	number = {5},
	urldate = {2016-09-01},
	journal = {Weather and Forecasting},
	author = {Ferro, Christopher A. T. and Stephenson, David B.},
	month = apr,
	year = {2011},
	pages = {699--713},
	file = {Full Text PDF:/home/jlaznarte/Zotero/storage/VKC3C5DF/waf-d-10-05030%2E1.pdf:application/pdf;Snapshot:files/624/WAF-D-10-05030.html:text/html},
}

@article{j.m_threshold_2011,
	title = {A {THRESHOLD} {AUTOREGRESSIVE} {ASYMMETRIC} {STOCHASTIC} {VOLATILITY} {STRATEGY} {TO} {ALERT} {OF} {VIOLATIONS} {OF} {THE} {AIR} {QUALITY} {STANDARDS}},
	volume = {5},
	url = {http://en.journals.sid.ir/ViewPaper.aspx?ID=189146},
	abstract = {Download Free Full-Text of an article A THRESHOLD AUTOREGRESSIVE ASYMMETRIC STOCHASTIC VOLATILITY STRATEGY TO ALERT OF VIOLATIONS OF THE AIR QUALITY STANDARDS},
	language = {En},
	number = {1},
	urldate = {2016-08-17},
	author = {J.m, Montero Lorenzo and M.c, Garcia Centeno and G, Fernandez Aviles},
	month = jan,
	year = {2011},
	pages = {23--32},
	file = {Full Text PDF:/home/jlaznarte/Zotero/storage/W3CHH3PC/J.m et al. - 2011 - A THRESHOLD AUTOREGRESSIVE ASYMMETRIC STOCHASTIC V.pdf:application/pdf;Snapshot:/home/jlaznarte/Zotero/storage/ZIFA5GUE/ViewPaper.html:text/html},
}

@article{lertxundi-manterola_modelling_2009,
	title = {Modelling of nitrogen dioxide ({NO2}) and fine particulate matter ({PM10}) air pollution in the metropolitan areas of {Barcelona} and {Bilbao}, {Spain}},
	volume = {20},
	issn = {1099-095X},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/env.939/abstract},
	doi = {10.1002/env.939},
	abstract = {Ecological studies in epidemiology present some limitations on assessing the exposure of environmental factors. The main objective of this study was to describe a statistical methodology that could help on more accurately assessing the relationship between air pollution and health responses at very small spatial scale. In particular, we modelled the spatial distribution of nitrogen dioxide (NO2) and of particles with a diameter of less than 10 µm in two study areas, the metropolitan areas of Barcelona (MABa) and Bilbao (MABi). The areas chosen for this project can be distinguished for having similar population density and urban surface, as well as high road traffic intensity. The geographical variability of the pollutants could differ greatly due to very different orographic and climatic conditions. Three contributions of this study could be mentioned. First of all, the spatial distribution of the values of each air pollutant was spread over the entire study region on a small area basis, that is census track in our case. Secondly, a stochastic interpolation method, kriging, was used. Thirdly, instead of estimating the average value of the pollutant at the centroid of the area for the entire study period, the value was estimated for each and every day of the corresponding period. As regards the main results, in both areas it can be observed that those census tracks with a greater concentration of pollutants are those that were estimated more accurately. This suggests the existence of a persistent emission source in those tracks, very probably those roads with greater road traffic intensity. Copyright © 2008 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5},
	urldate = {2016-08-17},
	journal = {Environmetrics},
	author = {Lertxundi-Manterola, Aitana and Saez, Marc},
	month = aug,
	year = {2009},
	keywords = {Air pollution, Kriging, census tracks, nitrogen dioxide and fine particulate matter, urban road traffic},
	pages = {477--493},
	file = {Full Text PDF:/home/jlaznarte/Zotero/storage/ZHGHXVJH/Lertxundi-Manterola y Saez - 2009 - Modelling of nitrogen dioxide (NO2) and fine parti.pdf:application/pdf;Snapshot:/home/jlaznarte/Zotero/storage/4MMGP3Q4/abstract.html:text/html},
}

@article{mendes_simple_2002,
	title = {A simple spatio-temporal procedure for the prediction of air pollution levels},
	volume = {16},
	issn = {1099-128X},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/cem.766/abstract},
	doi = {10.1002/cem.766},
	abstract = {In this paper we study the spatio-temporal behaviour of air pollutants measured daily over the city of Lisbon, Portugal. Our specific aim is to predict air pollutant levels in time and space over a fine grid of locations based on observations from a small number of monitoring sites. Our suggested prediction procedure is based on the simple and intuitive idea of first making predictions in time at the monitoring sites and then extending these predictions in space to locations other than the monitoring sites using kriging methods. Copyright © 2002 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {12},
	urldate = {2016-08-17},
	journal = {Journal of Chemometrics},
	author = {Mendes, Jorge M. and Turkman, Kamil F.},
	month = dec,
	year = {2002},
	keywords = {Kriging, multivariate time series, spatio-temporal models},
	pages = {623--632},
	file = {Full Text PDF:/home/jlaznarte/Zotero/storage/7I8722V8/Mendes y Turkman - 2002 - A simple spatio-temporal procedure for the predict.pdf:application/pdf;Snapshot:/home/jlaznarte/Zotero/storage/PW83IIUV/abstract.html:text/html},
}

@incollection{ferreira_air_2000,
	title = {Air {Quality} {Monitoring} and {Management} in {Lisbon}},
	copyright = {©2000 Springer Science+Business Media Dordrecht},
	isbn = {978-94-010-3796-9 978-94-010-0932-4},
	url = {http://link.springer.com/chapter/10.1007/978-94-010-0932-4_48},
	abstract = {The environmental decision-making process is related with the interpretation of data both in spatial and temporal dimensions. This paper presents a methodology that integrates the time-space framework of air quality data to infer the temporal pattern and spatial variability that could be interpreted for environmental decision purposes. Variograms that accommodate time and space lags were used for the analysis and proved to be effective. Its environmental meaning, in particular its relationship with traffic patterns is discussed. Data from air quality monitoring stations located in the central part of Lisbon were used in this study. It describes a strategy to identify the type of vehicles responsible for certain pollutant levels, particularly for nitrogen oxides, and discusses the application of new air quality European legislation to the city of Lisbon, Portugal.},
	language = {en},
	urldate = {2016-08-17},
	booktitle = {Urban {Air} {Quality}: {Measurement}, {Modelling} and {Management}},
	publisher = {Springer Netherlands},
	author = {Ferreira, Francisco and Tente, Hugo and Torres, Pedro and Cardoso, Sérgio and Palma-Oliveira, José M.},
	editor = {Sokhi, Ranjeet S. and José, Roberto San and Moussiopoulos, Nicolas and Berkowicz, Ruwim},
	year = {2000},
	doi = {10.1007/978-94-010-0932-4_48},
	keywords = {Kriging, air quality management, air quality monitoring, Atmospheric Protection/Air Quality Control/Air Pollution, Atmospheric Sciences, demand side management, Measurement Science and Instrumentation, Physical Chemistry, traffic pollution},
	pages = {443--450},
	file = {Snapshot:/home/jlaznarte/Zotero/storage/XGN6ZUWP/10.html:text/html},
}

@article{de_kassteele_model_2006,
	title = {A model for external drift kriging with uncertain covariates applied to air quality measurements and dispersion model output},
	volume = {17},
	issn = {1099-095X},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/env.771/abstract},
	doi = {10.1002/env.771},
	abstract = {We present a method that combines uncertain air quality measurements with uncertain secondary information from an atmospheric dispersion model. The method combines external drift kriging and a measurement error (ME) model, and uses Bayesian techniques for inference. An illustration with simulated data shows what can theoretically be expected. The method is flexible for assigning different error variances to both the primary information and secondary information at each location. Next, we address actual NO2 data collected at an urban and a rural site in the Netherlands. Uncertainty assessments in terms of exceeding air quality standards are given. The study shows that biased uncertain secondary information can be used successfully in a spatial interpolation study at the national scale. Copyright © 2005 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {4},
	urldate = {2016-08-17},
	journal = {Environmetrics},
	author = {de Kassteele, Jan van and Stein, Alfred},
	month = jun,
	year = {2006},
	keywords = {geostatistics, atmospheric dispersion models, Bayesian hierarchical models, external drift kriging, measurement error models, uncertainty assessment},
	pages = {309--322},
	file = {Full Text PDF:/home/jlaznarte/Zotero/storage/7EDVV7H8/de Kassteele y Stein - 2006 - A model for external drift kriging with uncertain .pdf:application/pdf;Snapshot:/home/jlaznarte/Zotero/storage/46SFNCXC/abstract.html:text/html},
}

@article{montero-lorenzo_spatio-temporal_2013,
	title = {A spatio-temporal geostatistical approach to predicting pollution levels: {The} case of mono-nitrogen oxides in {Madrid}},
	volume = {37},
	issn = {0198-9715},
	shorttitle = {A spatio-temporal geostatistical approach to predicting pollution levels},
	url = {http://www.sciencedirect.com/science/article/pii/S0198971512000580},
	doi = {10.1016/j.compenvurbsys.2012.06.005},
	abstract = {In spite of the effort made in the last years, NOx is still one of the main pollution problems in large cities. This is why the literature related to predicting NOx levels is certainly extensive. However, most of this literature does not take into account the spatio-temporal dependencies of such NOx levels. As spatio-temporal dependencies are a core aspect of pollution, we propose both a spatio-temporal kriging and a functional kriging strategy to incorporate such dependencies into the prediction procedure. We also use an innovative method for estimating the parameters of the non separable space–time covariance function involved in the spatio-temporal kriging strategy, which significantly reduces the computational burden of traditional likelihood-based methods. The empirical study focuses on Madrid City and is backed by a massive hourly database. Results indicate that the functional strategy outperforms the spatio-temporal procedure at non peripheral sites, which is a remarkable finding due to the high computational requirements of spatio-temporal kriging.},
	urldate = {2016-08-17},
	journal = {Computers, Environment and Urban Systems},
	author = {Montero-Lorenzo, José-María and Fernández-Avilés, Gema and Mondéjar-Jiménez, José and Vargas-Vargas, Manuel},
	month = jan,
	year = {2013},
	keywords = {Composite weighted likelihood, Functional kriging, NOx, Pollution, Spatio-temporal kriging},
	pages = {95--106},
	file = {1-s2.0-S0198971512000580-main.pdf:files/643/1-s2.0-S0198971512000580-main.pdf:application/pdf;ScienceDirect Full Text PDF:/home/jlaznarte/Zotero/storage/X2HWE4HT/Montero-Lorenzo et al. - 2013 - A spatio-temporal geostatistical approach to predi.pdf:application/pdf;ScienceDirect Snapshot:files/482/S0198971512000580.html:text/html;ScienceDirect Snapshot:/home/jlaznarte/Zotero/storage/BJJ2ABJP/S0198971512000580.html:text/html;ScienceDirect Snapshot:files/644/S0198971512000580.html:text/html},
}

@misc{veronesi_r_2015,
	title = {R tutorial for {Spatial} {Statistics}: {Spatio}-{Temporal} {Kriging} in {R}},
	shorttitle = {R tutorial for {Spatial} {Statistics}},
	url = {http://r-video-tutorial.blogspot.com.es/2015/08/spatio-temporal-kriging-in-r.html},
	urldate = {2016-08-17},
	journal = {R tutorial for Spatial Statistics},
	author = {Veronesi, Fabio},
	month = aug,
	year = {2015},
	keywords = {Air pollution, Kriging, interpolation, opensense, sensor, sensors, spatio-temporal, zurich},
	file = {Blogspot Snapshot:/home/jlaznarte/Zotero/storage/53TWGDJH/spatio-temporal-kriging-in-r.html:text/html},
}

@misc{noauthor_rpubs_nodate,
	title = {{RPubs} - {An} {Introduction} to {Kriging} in {R}},
	url = {https://rpubs.com/nabilabd/118172},
	urldate = {2016-07-19},
	file = {RPubs - An Introduction to Kriging in R:files/637/118172.html:text/html},
}

@article{mueller_statistical_2016,
	title = {Statistical modelling of particle number concentration in {Zurich} at high spatio-temporal resolution utilizing data from a mobile sensor network},
	volume = {126},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231015305409},
	doi = {10.1016/j.atmosenv.2015.11.033},
	abstract = {Highly resolved pollution maps are a valuable resource for many issues related to air quality including exposure modelling and urban planning. We present an approach for their generation based on data from a mobile sensor network and statistical modelling.

An extensive record of particle number concentrations (PNCs) spanning more than 1.5 years was compiled by the tram-based OpenSense mobile sensor network in the City of Zurich. The sensor network consists of 10 sensor nodes installed on the roof of trams operating on different services according to their regular operation schedules. We developed a statistical modelling approach based on Generalized Additive models (GAMs) utilizing the PNC data obtained along the tram tracks as well as georeferenced information as predictor variables. Our approach includes a variable selection algorithm to ensure that individual models rely on the optimal set of predictor variables. Our models have high temporal and spatial resolutions of 30 min and 10 m by 10 m, respectively, and allow the spatial prediction of PNC in the municipal area of Zurich.

We applied our approach to PNC data from two dedicated time periods: July–Sept. 2013 and Dec. 2013–Feb. 2014. The models strongly rely on traffic related predictor variables (vehicle counts) and, due to the hilly topography of Zurich, on elevation. We assessed the model performance by leave-one-out cross-validation and by comparing PNC predictions to measurements at fixed reference sites and to PNC measurements obtained by pedestrians. Model predictions reproduce well the main features of the PNC field in environment types similar to those passed by individual trams. Model performance is worse at elevated background locations probably due to the weak coverage of similar spots by the tram network.

We end the paper by outlining a route finding algorithm which utilizes the highly resolved PNC maps providing the exposure minimal route for cyclists.},
	urldate = {2016-07-18},
	journal = {Atmospheric Environment},
	author = {Mueller, M. D. and Hasenfratz, David and Saukh, Olga and Fierz, Martin and Hueglin, Christoph},
	month = feb,
	year = {2016},
	keywords = {Statistical modelling, Geoinformation, Mobile sensor network, Particle number concentration (PNC), Pollution maps, Urban environment},
	pages = {171--181},
	file = {ScienceDirect Snapshot:files/439/S1352231015305409.html:text/html},
}

@article{graler_spatio-temporal_2011,
	title = {Spatio-temporal analysis and interpolation of {PM10} measurements in {Europe}},
	volume = {10},
	url = {http://www.academia.edu/download/45737129/Spatio-temporal_analysis_and_interpolati20160518-11284-954hqv.pdf},
	urldate = {2016-07-18},
	journal = {ETC/ACM Technical Paper},
	author = {Gräler, Benedikt and Gerharz, Lydia and Pebesma, Edzer},
	year = {2011},
	file = {Spatio-temporal_analysis_and_interpolati.pdf:files/438/Spatio-temporal_analysis_and_interpolati.pdf:application/pdf},
}

@article{paci_spatio-temporal_2013,
	title = {Spatio-temporal modeling for real-time ozone forecasting},
	volume = {4},
	issn = {2211-6753},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3760439/},
	doi = {10.1016/j.spasta.2013.04.003},
	abstract = {The accurate assessment of exposure to ambient ozone concentrations is important for informing the public and pollution monitoring agencies about ozone levels that may lead to adverse health effects. High-resolution air quality information can offer significant health benefits by leading to improved environmental decisions. A practical challenge facing the U.S. Environmental Protection Agency (USEPA) is to provide real-time forecasting of current 8-hour average ozone exposure over the entire conterminous United States. Such real-time forecasting is now provided as spatial forecast maps of current 8-hour average ozone defined as the average of the previous four hours, current hour, and predictions for the next three hours. Current 8-hour average patterns are updated hourly throughout the day on the EPA-AIRNow web site., The contribution here is to show how we can substantially improve upon current real-time forecasting systems. To enable such forecasting, we introduce a downscaler fusion model based on first differences of real-time monitoring data and numerical model output. The model has a flexible coefficient structure and uses an efficient computational strategy to fit model parameters. Our hybrid computational strategy blends continuous background updated model fitting with real-time predictions. Model validation analyses show that we are achieving very accurate and precise ozone forecasts.},
	urldate = {2016-07-01},
	journal = {Spatial statistics},
	author = {Paci, Lucia and Gelfand, Alan E. and Holland, David M.},
	month = may,
	year = {2013},
	pmid = {24010052},
	pmcid = {PMC3760439},
	pages = {79--93},
	file = {PubMed Central Full Text PDF:files/440/Paci et al. - 2013 - Spatio-temporal modeling for real-time ozone forec.pdf:application/pdf},
}

@article{pace_method_2000,
	title = {A method for spatial–temporal forecasting with an application to real estate prices},
	volume = {16},
	issn = {0169-2070},
	url = {http://www.sciencedirect.com/science/article/pii/S0169207099000473},
	doi = {10.1016/S0169-2070(99)00047-3},
	abstract = {Using 5243 housing price observations during 1984–92 from Baton Rouge, this manuscript demonstrates the substantial benefits obtained by modeling the spatial as well as the temporal dependence of the errors. Specifically, the spatial–temporal autoregression with 14 variables produced 46.9\% less SSE than a 12-variable regression using simple indicator variables for time. More impressively, the spatial–temporal regression with 14 variables displayed 8\% lower SSE than a regression using 211 variables attempting to control for the housing characteristics, time, and space via continuous and indicator variables. One-step ahead forecasts document the utility of the proposed spatial–temporal model. In addition, the manuscript illustrates techniques for rapidly computing the estimates based upon an interesting decomposition for modeling spatial and temporal effects. The decomposition maximizes the use of sparsity in some of the matrices and consequently accelerates computations. In fact, the model uses the frequent transactions in the housing market to help simplify computations. The techniques employed also have applications to other dimensions and metrics.},
	number = {2},
	urldate = {2016-06-30},
	journal = {International Journal of Forecasting},
	author = {Pace, R. Kelley and Barry, Ronald and Gilley, Otis W. and Sirmans, C. F.},
	month = apr,
	year = {2000},
	keywords = {Modeling, Real estate, Spatial–temporal forecasting},
	pages = {229--246},
	file = {ScienceDirect Snapshot:files/442/S0169207099000473.html:text/html},
}

@misc{noauthor_diagnosis_nodate,
	title = {From diagnosis to prognosis for forecasting air pollution using neural networks: {Air} pollution monitoring in {Bilbao}},
	url = {http://www.sciencedirect.com/science/article/pii/S1364815207001740},
	urldate = {2016-06-30},
	file = {From diagnosis to prognosis for forecasting air pollution using neural networks\: Air pollution monitoring in Bilbao:files/444/S1364815207001740.html:text/html},
}

@article{sahu_bayesian_2005,
	title = {A {Bayesian} kriged {Kalman} model for short-term forecasting of air pollution levels},
	volume = {54},
	issn = {1467-9876},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9876.2005.00480.x/abstract},
	doi = {10.1111/j.1467-9876.2005.00480.x},
	abstract = {Summary.  Short-term forecasts of air pollution levels in big cities are now reported in news-papers and other media outlets. Studies indicate that even short-term exposure to high levels of an air pollutant called atmospheric particulate matter can lead to long-term health effects. Data are typically observed at fixed monitoring stations throughout a study region of interest at different time points. Statistical spatiotemporal models are appropriate for modelling these data. We consider short-term forecasting of these spatiotemporal processes by using a Bayesian kriged Kalman filtering model. The spatial prediction surface of the model is built by using the well-known method of kriging for optimum spatial prediction and the temporal effects are analysed by using the models underlying the Kalman filtering method. The full Bayesian model is implemented by using Markov chain Monte Carlo techniques which enable us to obtain the optimal Bayesian forecasts in time and space. A new cross-validation method based on the Mahalanobis distance between the forecasts and observed data is also developed to assess the forecasting performance of the model implemented.},
	language = {en},
	number = {1},
	urldate = {2016-06-30},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Sahu, Sujit K. and Mardia, Kanti V.},
	month = jan,
	year = {2005},
	keywords = {Kriging, Bending energy, Gibbs sampler, Kalman filter, Markov chain Monte Carlo methods, Spatial temporal modelling, State space model},
	pages = {223--244},
	file = {Snapshot:files/445/abstract\;jsessionid=2B4BEFA9869911A5B2C704C7CA8D0EA0.html:text/html},
}

@article{wang_spatial_2014,
	title = {Spatial {Distribution} {Characteristics} and {Hourly} {Forecast} of {PM}\&lt;sub\&gt;2.5\&lt;/sub\&gt; {Pollution} in {Summer} in {Beijing}},
	volume = {1073-1076},
	issn = {1662-8985},
	url = {http://www.scientific.net/AMR.1073-1076.874},
	doi = {10.4028/www.scientific.net/AMR.1073-1076.874},
	urldate = {2016-06-30},
	journal = {Advanced Materials Research},
	author = {Wang, Li Sha and Li, Jie and Zhang, Li Na and Chen, Yuan Hua and Liu, Yu and Chen, Yu Ping},
	month = dec,
	year = {2014},
	pages = {874--880},
}

@techreport{mcsharry_methodology_2009,
	title = {Methodology for the evaluation of probabilistic forecasts},
	number = {Deliverable Dp-6.2},
	institution = {European Commission},
	author = {McSharry, Patrick E and Pinson, Pierre and Girard, Robin},
	month = oct,
	year = {2009},
}

@article{pinson_non-parametric_2007,
	title = {Non-parametric probabilistic forecasts of wind power: required properties and evaluation},
	volume = {10},
	issn = {1099-1824},
	shorttitle = {Non-parametric probabilistic forecasts of wind power},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/we.230/abstract},
	doi = {10.1002/we.230},
	abstract = {Predictions of wind power production for horizons up to 48–72 h ahead comprise a highly valuable input to the methods for the daily management or trading of wind generation. Today, users of wind power predictions are not only provided with point predictions, which are estimates of the conditional expectation of the wind generation for each look-ahead time, but also with uncertainty estimates given by probabilistic forecasts. In order to avoid assumptions on the shape of predictive distributions, these probabilistic predictions are produced from non-parametric methods, and then take the form of a single or a set of quantile forecasts. The required and desirable properties of such probabilistic forecasts are defined and a framework for their evaluation is proposed. This framework is applied for evaluating the quality of two statistical methods producing full predictive distributions from point predictions of wind power. These distributions are defined by a number of quantile forecasts with nominal proportions spanning the unit interval. The relevance and interest of the introduced evaluation framework are discussed. Copyright © 2007 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {6},
	urldate = {2016-03-17},
	journal = {Wind Energy},
	author = {Pinson, Pierre and Nielsen, Henrik Aa. and Møller, Jan K. and Madsen, Henrik and Kariniotakis, George N.},
	month = nov,
	year = {2007},
	keywords = {Probabilistic forecasting, quality evaluation, reliability, resolution, sharpness, skill, Uncertainty, wind power},
	pages = {497--516},
	file = {Snapshot:files/448/abstract.html:text/html;Snapshot:files/450/abstract.html:text/html},
}

@misc{ayuntamiento_de_madrid_sistema_nodate,
	title = {Sistema de {Vigilancia} de la {Calidad} del {Aire}},
	url = {http://www.mambiente.munimadrid.es/sica/scripts/index.php},
	urldate = {2016-03-10},
	author = {{Ayuntamiento de Madrid}},
	file = {Sistema de Vigilancia de la Calidad del Aire:files/451/index.html:text/html},
}

@article{bjornar_bremnes_probabilistic_2004,
	title = {Probabilistic {Forecasts} of {Precipitation} in {Terms} of {Quantiles} {Using} {NWP} {Model} {Output}},
	volume = {132},
	issn = {0027-0644, 1520-0493},
	url = {http://journals.ametsoc.org/doi/abs/10.1175/1520-0493%282004%29132%3C0338%3APFOPIT%3E2.0.CO%3B2},
	doi = {10.1175/1520-0493(2004)132<0338:PFOPIT>2.0.CO;2},
	language = {en},
	number = {1},
	urldate = {2016-03-09},
	journal = {Monthly Weather Review},
	author = {Bjørnar Bremnes, John},
	month = jan,
	year = {2004},
	pages = {338--347},
}

@article{cade_gentle_2003,
	title = {A gentle introduction to quantile regression for ecologists},
	volume = {1},
	issn = {1540-9309},
	url = {http://onlinelibrary.wiley.com/doi/10.1890/1540-9295(2003)001[0412:AGITQR]2.0.CO;2/abstract},
	doi = {10.1890/1540-9295(2003)001[0412:AGITQR]2.0.CO;2},
	abstract = {Quantile regression is a way to estimate the conditional quantiles of a response variable distribution in the linear model that provides a more complete view of possible causal relationships between variables in ecological processes. Typically, all the factors that affect ecological processes are not measured and included in the statistical models used to investigate relationships between variables associated with those processes. As a consequence, there may be a weak or no predictive relationship between the mean of the response variable (y) distribution and the measured predictive factors (X). Yet there may be stronger, useful predictive relationships with other parts of the response variable distribution. This primer relates quantile regression estimates to prediction intervals in parametric error distribution regression models (eg least squares), and discusses the ordering characteristics, interval nature, sampling variation, weighting, and interpretation of the estimates for homogeneous and heterogeneous regression models.},
	language = {en},
	number = {8},
	urldate = {2016-03-09},
	journal = {Frontiers in Ecology and the Environment},
	author = {Cade, Brian S. and Noon, Barry R.},
	month = oct,
	year = {2003},
	pages = {412--420},
	file = {Full Text PDF:files/452/Cade and Noon - 2003 - A gentle introduction to quantile regression for e.pdf:application/pdf;Snapshot:files/453/abstract.html:text/html},
}

@book{fitzenberger_economic_2002,
	address = {Heidelberg},
	title = {Economic {Applications} of {Quantile} {Regression}},
	isbn = {978-3-7908-2502-2 978-3-662-11592-3},
	url = {http://link.springer.com/10.1007/978-3-662-11592-3},
	language = {en},
	urldate = {2016-03-09},
	publisher = {Physica-Verlag HD},
	editor = {Fitzenberger, Bernd and Koenker, Roger and Machado, José A. F.},
	year = {2002},
}

@book{fitzenberger_bernd_koenker_roger_machado_jose_a.f._economic_2002,
	series = {Studies in {Empirical} {Economics}},
	title = {Economic {Applications} of {Quantile} {Regression}},
	url = {http://www.springer.com/cn/book/9783790814484},
	abstract = {Quantile regression has emerged as an essential statistical tool of contemporary empirical economics and biostatistics. Complementing classical least...},
	urldate = {2016-03-09},
	publisher = {Physica-Verlag Heidelberg},
	author = {{Fitzenberger, Bernd, Koenker, Roger, Machado, Jose A.F.}},
	year = {2002},
	file = {Snapshot:files/454/9783790814484.html:text/html},
}

@article{yu_quantile_2003,
	title = {Quantile regression: applications and current research areas},
	volume = {52},
	issn = {1467-9884},
	shorttitle = {Quantile regression},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9884.00363/abstract},
	doi = {10.1111/1467-9884.00363},
	abstract = {Summary. Quantile regression offers a more complete statistical model than mean regression and now has widespread applications. Consequently, we provide a review of this technique. We begin with an introduction to and motivation for quantile regression. We then discuss some typical application areas. Next we outline various approaches to estimation. We finish by briefly summarizing some recent research areas.},
	language = {en},
	number = {3},
	urldate = {2016-03-09},
	journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
	author = {Yu, Keming and Lu, Zudi and Stander, Julian},
	month = oct,
	year = {2003},
	keywords = {Check function, Conditional distribution, Quantile, Regression fitting, Skew distribution},
	pages = {331--350},
	file = {Snapshot:files/456/abstract.html:text/html},
}

@article{zhang_review_2014,
	title = {Review on probabilistic forecasting of wind power generation},
	volume = {32},
	issn = {1364-0321},
	url = {http://www.sciencedirect.com/science/article/pii/S1364032114000446},
	doi = {10.1016/j.rser.2014.01.033},
	abstract = {The randomness and intermittence of wind resources is the biggest challenge in the integration of wind power into the power system. Accurate forecasting of wind power generation is an efficient tool to deal with such problem. Conventional wind power forecasting produces a value, or the conditional expectation of wind power output at a time point in the future. However, any prediction involves inherent uncertainty. In recent years, several probabilistic forecasting approaches have been reported in wind power forecasting studies. Compared to currently wide-used point forecasts, probabilistic forecasts could provide additional quantitative information on the uncertainty associated with wind power generation. For decision-makings in the uncertainty environment, probabilistic forecasts are optimal inputs. A review of state-of-the-art methods and new developments in wind power probabilistic forecasting is presented in this paper. Firstly, three different representations of wind power uncertainty are briefly introduced. Then, different forecasting methods are discussed. These methods are classified into three categories in terms of uncertainty representation, i.e. probabilistic forecasts (parametric and non-parametric), risk index forecasts and space-time scenario forecasts. Finally, requirements and the overall framework of the uncertainty forecasting evaluation are summarized. In addition, this article also describes current challenges and future developments associated with wind power probabilistic prediction.},
	urldate = {2016-03-09},
	journal = {Renewable and Sustainable Energy Reviews},
	author = {Zhang, Yao and Wang, Jianxue and Wang, Xifan},
	month = apr,
	year = {2014},
	keywords = {Probabilistic forecasting, Decision-making, Forecasting evaluation, Parametric and non-parametric density, Stochastic optimization, Uncertainty forecasting, Uncertainty representation},
	pages = {255--270},
	file = {ScienceDirect Snapshot:files/458/S1364032114000446.html:text/html},
}

@article{dutot_24-h_2007,
	title = {A 24-h forecast of ozone peaks and exceedance levels using neural classifiers and weather predictions},
	volume = {22},
	issn = {1364-8152},
	url = {http://www.sciencedirect.com/science/article/pii/S1364815206001976},
	doi = {10.1016/j.envsoft.2006.08.002},
	abstract = {A neural network combined to a neural classifier is used in a real time forecasting of hourly maximum ozone in the centre of France, in an urban atmosphere. This neural model is based on the MultiLayer Perceptron (MLP) structure. The inputs of the statistical network are model output statistics of the weather predictions from the French National Weather Service. These predicted meteorological parameters are very easily available through an air quality network. The lead time used in this forecasting is (t + 24) h. Efforts are related to a regularisation method which is based on a Bayesian Information Criterion-like and to the determination of a confidence interval of forecasting. We offer a statistical validation between various statistical models and a deterministic chemistry-transport model. In this experiment, with the final neural network, the ozone peaks are fairly well predicted (in terms of global fit), with an Agreement Index = 92\%, the Mean Absolute Error = the Root Mean Square Error = 15 μg m−3 and the Mean Bias Error = 5 μg m−3, where the European threshold of the hourly ozone is 180 μg m−3.

To improve the performance of this exceedance forecasting, instead of the previous model, we use a neural classifier with a sigmoid function in the output layer. The output of the network ranges from [0,1] and can be interpreted as the probability of exceedance of the threshold. This model is compared to a classical logistic regression. With this neural classifier, the Success Index of forecasting is 78\% whereas it is from 65\% to 72\% with the classical MLPs. During the validation phase, in the Summer of 2003, six ozone peaks above the threshold were detected. They actually were seven.

Finally, the model called NEUROZONE is now used in real time. New data will be introduced in the training data each year, at the end of September. The network will be re-trained and new regression parameters estimated. So, one of the main difficulties in the training phase – namely the low frequency of ozone peaks above the threshold in this region – will be solved.},
	number = {9},
	urldate = {2016-03-09},
	journal = {Environmental Modelling \& Software},
	author = {Dutot, Alain-Louis and Rynkiewicz, Joseph and Steiner, Frédy E. and Rude, Julien},
	month = sep,
	year = {2007},
	keywords = {Multilayer Perceptron, artificial neural network, Confidence interval of prediction, Neural classifier, Ozone modelling, Regularisation method, Statistical stepwise method},
	pages = {1261--1269},
	file = {ScienceDirect Full Text PDF:files/459/Dutot et al. - 2007 - A 24-h forecast of ozone peaks and exceedance leve.pdf:application/pdf;ScienceDirect Snapshot:files/460/S1364815206001976.html:text/html},
}

@article{francke_estimation_2008,
	title = {Estimation of suspended sediment concentration and yield using linear models, random forests and quantile regression forests},
	volume = {22},
	copyright = {Copyright © 2008 John Wiley \& Sons, Ltd.},
	issn = {1099-1085},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/hyp.7110/abstract},
	doi = {10.1002/hyp.7110},
	abstract = {For sediment yield estimation, intermittent measurements of suspended sediment concentration (SSC) have to be interpolated to derive a continuous sedigraph. Traditionally, sediment rating curves (SRCs) based on univariate linear regression of discharge and SSC (or the logarithms thereof) are used but alternative approaches (e.g. fuzzy logic, artificial neural networks, etc.) exist. This paper presents a comparison of the applicability of traditional SRCs, generalized linear models (GLMs) and non-parametric regression using Random Forests (RF) and Quantile Regression Forests (QRF) applied to a dataset of SSC obtained for four subcatchments (0·08, 41, 145 and 445 km2) in the Central Spanish Pyrenees. The observed SSCs are highly variable and range over six orders of magnitude. For these data, traditional SRCs performed inadequately due to the over-simplification of relating SSC solely to discharge. Instead, the multitude of acting processes required more flexibility to model these nonlinear relationships. Thus, alternative advanced machine learning techniques that have been successfully applied in other disciplines were tested. GLMs provide the option of including other relevant process variables (e.g. rainfall intensities and temporal information) but require the selection of the most appropriate predictors. For the given datasets, the investigated variable selection methods produced inconsistent results. All proposed GLMs showed an inferior performance, whereas RF and QRF proved to be very robust and performed favourably for reproducing sediment dynamics. QRF additionally provides estimates on the accuracy of the predictions and thus allows the assessment of uncertainties in the estimated sediment yield that is not commonly found in other methods. The capabilities of RF and QRF concerning the interpretation of predictor effects are also outlined. Copyright © 2008 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {25},
	urldate = {2016-03-08},
	journal = {Hydrological Processes},
	author = {Francke, T. and López-Tarazón, J.a. and Schröder, B.},
	month = dec,
	year = {2008},
	keywords = {generalized linear model, Quantile Regression Forests, Random forests, sediment rating curve, suspended sediment concentration},
	pages = {4892--4904},
	file = {Snapshot:files/462/abstract\;jsessionid=DABC22FB2EE7974077193737174444F7.html:text/html},
}

@incollection{tung_extensions_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Extensions to {Quantile} {Regression} {Forests} for {Very} {High}-{Dimensional} {Data}},
	copyright = {©2014 Springer International Publishing Switzerland},
	isbn = {978-3-319-06604-2 978-3-319-06605-9},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-06605-9_21},
	abstract = {This paper describes new extensions to the state-of-the-art regression random forests Quantile Regression Forests (QRF) for applications to high-dimensional data with thousands of features. We propose a new subspace sampling method that randomly samples a subset of features from two separate feature sets, one containing important features and the other one containing less important features. The two feature sets partition the input data based on the importance measures of features. The partition is generated by using feature permutation to produce raw importance feature scores first and then applying p-value assessment to separate important features from the less important ones. The new subspace sampling method enables to generate trees from bagged sample data with smaller regression errors. For point regression, we choose the prediction value of Y from the range between two quantiles Q 0.05 and Q 0.95 instead of the conditional mean used in regression random forests. Our experiment results have shown that random forests with these extensions outperformed regression random forests and quantile regression forests in reduction of root mean square residuals.},
	language = {en},
	number = {8444},
	urldate = {2016-03-08},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer International Publishing},
	author = {Tung, Nguyen Thanh and Huang, Joshua Zhexue and Khan, Imran and Li, Mark Junjie and Williams, Graham},
	editor = {Tseng, Vincent S. and Ho, Tu Bao and Zhou, Zhi-Hua and Chen, Arbee L. P. and Kao, Hung-Yu},
	month = may,
	year = {2014},
	doi = {10.1007/978-3-319-06605-9_21},
	keywords = {Data Mining, Quantile Regression Forests, Artificial Intelligence (incl. Robotics), Data Mining and Knowledge Discovery, High-dimensional Data, Information Storage and Retrieval, Regression Random Forests},
	pages = {247--258},
	file = {Snapshot:files/464/10.html:text/html;tung2014.pdf:/tmp/tung2014.pdf:application/pdf},
}

@article{meinshausen_quantile_2006,
	title = {Quantile {Regression} {Forests}},
	volume = {7},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1248547.1248582},
	abstract = {Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power.},
	urldate = {2016-03-08},
	journal = {J. Mach. Learn. Res.},
	author = {Meinshausen, Nicolai},
	month = dec,
	year = {2006},
	pages = {983--999},
	file = {7-983-meinshausen.pdf:files/465/7-983-meinshausen.pdf:application/pdf},
}

@book{koenker_quantile_2005,
	series = {Econometric {Society} {Monographs}},
	title = {Quantile regression},
	url = {https://books.google.es/books?hl=en&lr=&id=hdkt7V4NXsgC&oi=fnd&pg=PP1&dq=koenker+quantile+regression+book&ots=FttnheE6vD&sig=tFd6lKjHLSxX0bNYZIJPpGk4ym4},
	number = {38},
	urldate = {2016-03-08},
	publisher = {Cambridge university press},
	author = {Koenker, Roger},
	year = {2005},
	file = {[PDF] from preterhuman.net:files/466/Koenker - 2005 - Quantile regression.pdf:application/pdf;Snapshot:files/467/books.html:text/html},
}

@misc{european_commission_air_2008,
	title = {Air {Quality} {Standards} according to {Directive} 2008/50/{EC}},
	url = {http://ec.europa.eu/environment/air/quality/standards.htm},
	urldate = {2016-03-03},
	author = {{European Commission}},
	year = {2008},
	file = {Standards - Air Quality - Environment - European Commission:files/489/standards.html:text/html},
}

@inproceedings{linfoot_cloud_nodate,
	address = {New Orleans, USA},
	title = {A {CLOUD} {DETECTION} {ALGORITHM} {APPLIED} {TO} {A} {WHOLE} {SKY} {IMAGER} {INSTRUMENT} {USING} {NEURAL} {NETWORKS}},
	url = {http://ams.confex.com/ams/88Annual/techprogram/paper_129536.htm},
	booktitle = {Sixth {Conference} on {Artificial} {Intelligence} {Applications} to {Environmental} {Science}},
	author = {Linfoot, A. and Alliss, R. J},
	keywords = {cloud detection},
	file = {Linfoot2008cloud.pdf:files/484/Linfoot2008cloud.pdf:application/pdf},
}

@misc{noauthor_eur-lex_nodate,
	title = {{EUR}-{Lex} - {32008L0050} - {EN} - {EUR}-{Lex}},
	url = {http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32008L0050},
	urldate = {2016-03-03},
	file = {EUR-Lex - 32008L0050 - EN - EUR-Lex:files/488/TXT.html:text/html},
}

@article{wang_traffic_2007,
	title = {Traffic restrictions associated with the {Sino}-{African} summit: {Reductions} of {NOx} detected from space},
	volume = {34},
	issn = {1944-8007},
	shorttitle = {Traffic restrictions associated with the {Sino}-{African} summit},
	url = {http://onlinelibrary.wiley.com/doi/10.1029/2007GL029326/abstract},
	doi = {10.1029/2007GL029326},
	abstract = {Aggressive measures were instituted by the Beijing municipal authorities to restrict vehicular traffic in the Chinese capital during the recent Sino-African Summit. We show that reductions in associated emissions of NOx were detected by the Dutch-Finnish Ozone Monitoring Instrument (OMI) aboard the Aura satellite. Interpretation of these data using a 3-dimensional chemical transport model indicates that emissions of NOx were reduced by 40\% over the period of November 4 to 6, 2006, for which the restrictions were in place.},
	language = {en},
	number = {8},
	urldate = {2016-03-03},
	journal = {Geophysical Research Letters},
	author = {Wang, Yuxuan and McElroy, Michael B. and Boersma, K. Folkert and Eskes, Henk J. and Veefkind, J. Pepijn},
	month = apr,
	year = {2007},
	keywords = {NO2, 0322 Constituent sources and sinks, 0345 Pollution: urban and regional, 0365 Troposphere: composition and chemistry, 0480 Remote sensing, Beijing, NOX emissions, Ozone Monitoring Instrument, traffic restrictions},
	pages = {L08814},
	file = {Full Text PDF:files/490/Wang et al. - 2007 - Traffic restrictions associated with the Sino-Afri.pdf:application/pdf;Snapshot:files/491/abstract.html:text/html},
}

@article{strand_nitrogen_1997,
	title = {Nitrogen dioxide exposure enhances asthmatic reaction to inhaled allergen in subjects with asthma.},
	volume = {155},
	issn = {1073-449X},
	url = {http://www.atsjournals.org/doi/abs/10.1164/ajrccm.155.3.9117021},
	doi = {10.1164/ajrccm.155.3.9117021},
	abstract = {We investigated whether exposure to a low level (490 micrograms/m3) of nitrogen dioxide (NO2) affects bronchial responsiveness to allergen and enhances allergen-induced increase in airway responsiveness to histamine. Eighteen subjects with asthma and allergy to pollen were exposed at rest to either purified air or NO2 for 30 min followed 4 h later by an allergen inhalation challenge. Responsiveness to histamine was measured the day after. Lung function during NO2 exposure and allergen challenge was measured by plethysmography and after exposure by a portable spirometer hourly. The order of exposure to NO2 and air was randomized and separated by at least 2 wk. The asthmatic reaction during the late phase was enhanced by NO2, and peak expiratory flow after allergen challenge was on average 6.6\% lower (p = 0.02) after NO2 than after air exposure. The number of subjects having a late asthmatic reaction (fall in FEV1 {\textgreater} 15\%) was seven after air and 10 after NO2 (NS). Peripheral blood samples were analyzed for differential cell counts before and after NO2/allergen and serum levels of eosinophil cationic protein (ECP). NO2 effect on lung function was neither associated with an increase in eosinophil numbers nor with ECP levels. NO2 did not affect lung function before allergen challenge, early asthmatic reaction, and allergen-induced increase in responsiveness to histamine. These results indicate that short exposure to an ambient level of NO2 followed several hours later by allergen inhalation enhances allergen-induced late asthmatic reaction.},
	number = {3},
	urldate = {2016-03-03},
	journal = {American Journal of Respiratory and Critical Care Medicine},
	author = {Strand, V and Rak, S and Svartengren, M and Bylin, G},
	month = mar,
	year = {1997},
	pages = {881--887},
	file = {Snapshot:files/492/ajrccm.155.3.html:text/html},
}

@article{brunekreef_air_1997,
	title = {Air {Pollution} from {Truck} {Traffic} and {Lung} {Function} in {Children} {Living} near {Motorways}},
	volume = {8},
	issn = {1044-3983},
	url = {http://www.jstor.org/stable/3702257},
	abstract = {The contribution of motorized traffic to air pollution is widely recognized, but relatively few studies have looked at the respiratory health status of subjects living near busy roads. We studied children in six areas located near major motorways in the Netherlands. We measured lung function in the children, and we assessed their exposure to traffic-related air pollution using separate traffic counts for automobiles and trucks. We also measured air pollution in the children's schools. Lung function was associated with truck traffic density but had a lesser association with automobile traffic density. The association was stronger in children living closest ({\textless}300 m) to the motorways. Lung function was also associated with the concentration of black smoke, measured inside the schools, as a proxy for diesel exhaust particles. The associations were stronger in girls than in boys. The results indicate that exposure to traffic-related air pollution, in particular diesel exhaust particles, may lead to reduced lung function in children living near major motorways.},
	number = {3},
	urldate = {2016-03-03},
	journal = {Epidemiology},
	author = {Brunekreef, Bert and Janssen, Nicole A. H. and de Hartog, Jeroen and Harssema, Hendrik and Knape, Mirjam and van Vliet, Patricia},
	year = {1997},
	pages = {298--303},
}

@misc{noauthor_air_nodate-1,
	title = {Air {Pollution} from {Truck} {Traffic} and {Lung} {Function} in {Children} {Living} near {Motorways} on {JSTOR}},
	url = {http://www.jstor.org/stable/3702257?seq=1#page_scan_tab_contents},
	urldate = {2016-03-03},
	file = {Air Pollution from Truck Traffic and Lung Function in Children Living near Motorways on JSTOR:files/493/3702257.html:text/html},
}

@article{pershagen_air_1995,
	title = {Air pollution involving nitrogen dioxide exposure and wheezing bronchitis in children},
	volume = {24},
	issn = {0300-5771},
	abstract = {BACKGROUND: A population-based case-control study was performed in Stockholm to assess the influence of air pollution on the occurrence of severe wheezing bronchitis in children.
METHODS: The study included 197 children aged 4 months to 4 years, who were hospitalized because of breathing difficulties with wheezing, and 350 population controls. Information on potential risk factors for childhood wheezing and a residential history was obtained at home interview with parents. Outdoor nitrogen dioxide (NO2) concentrations at home addresses and day care centres from birth on were estimated from validated models, mainly using data on traffic intensity from municipal registers.
RESULTS: The risk of wheezing bronchitis was related to time-weighted mean outdoor NO2 exposure in girls (P = 0.02), but not in boys. A gas stove in the home appeared to be a risk factor primarily for girls. All analyses controlled for parental asthma and maternal smoking, which were independent risk factors for wheezing bronchitis.
CONCLUSIONS: The results suggest that exposure to combustion products containing NO2 may be of particular importance for the development of wheezing bronchitis in girls.},
	language = {eng},
	number = {6},
	journal = {International Journal of Epidemiology},
	author = {Pershagen, G. and Rylander, E. and Norberg, S. and Eriksson, M. and Nordvall, S. L.},
	month = dec,
	year = {1995},
	pmid = {8824856},
	keywords = {Nitrogen Dioxide, Air pollution, Air Pollutants, Bronchitis, Case-Control Studies, Child, Preschool, Environmental Exposure, Female, Humans, Infant, Male, Risk, Sweden},
	pages = {1147--1153},
}

@misc{world_health_organization_factsheet_2014,
	title = {Factsheet num. 313: {Ambient} (outdoor) air quality and health},
	url = {http://www.who.int/mediacentre/factsheets/fs313/en/},
	abstract = {WHO fact sheet on ambient (outdoor) air quality guidelines: includes key facts, definition, health effects, guideline values and WHO response.},
	urldate = {2016-03-03},
	journal = {WHO},
	author = {{World Health Organization}},
	month = mar,
	year = {2014},
	file = {Snapshot:files/495/en.html:text/html},
}

@article{bentayeb_association_2015,
	title = {Association between long-term exposure to air pollution and mortality in {France}: {A} 25-year follow-up study},
	volume = {85},
	issn = {0160-4120},
	shorttitle = {Association between long-term exposure to air pollution and mortality in {France}},
	url = {http://www.sciencedirect.com/science/article/pii/S0160412015300349},
	doi = {10.1016/j.envint.2015.08.006},
	abstract = {Long-term exposure to air pollution (AP) has been shown to have an impact on mortality in numerous countries, but since 2005 no data exists for France.
We analyzed the association between long-term exposure to air pollution and mortality at the individual level in a large French cohort followed from 1989 to 2013.
The study sample consisted of 20,327 adults working at the French national electricity and gas company EDF-GDF. Annual exposure to PM10, PM10–2.5, PM2.5, NO2, O3, SO2, and benzene was assessed for the place of residence of participants using a chemistry-transport model and taking residential history into account. Hazard ratios were estimated using a Cox proportional-hazards regression model, adjusted for selected individual and contextual risk factors. Hazard ratios were computed for an interquartile range (IQR) increase in air pollutant concentrations.
The cohort recorded 1967 non-accidental deaths. Long-term exposures to baseline PM2.5, PM10-25, NO2 and benzene were associated with an increase in non-accidental mortality (Hazard Ratio, HR = 1.09; 95\% CI: 0.99, 1.20 per 5.9 μg/m3, PM10-25; HR = 1.09;
95\% CI: 1.04, 1.15 per 2.2 μg/m3, NO2: HR = 1.14; 95\% CI: 0.99, 1.31 per 19.3 μg/m3 and benzene: HR = 1.10; 95\% CI: 1.00, 1.22 per 1.7 μg/m3).The strongest association was found for PM10: HR = 1.14; 95\% CI: 1.05, 1.25 per 7.8 μg/m3. PM10, PM10-25 and SO2 were associated with non-accidental mortality when using time varying exposure. No significant associations were observed between air pollution and cardiovascular and respiratory mortality.
Long-term exposure to fine particles, nitrogen dioxide, sulfur dioxide and benzene is associated with an increased risk of non-accidental mortality in France. Our results strengthen existing evidence that outdoor air pollution is a significant environmental risk factor for mortality. Due to the limited sample size and the nature of our study (occupational), further investigations are needed in France with a larger representative population sample.},
	urldate = {2016-03-03},
	journal = {Environment International},
	author = {Bentayeb, Malek and Wagner, Verene and Stempfelet, Morgane and Zins, Marie and Goldberg, Marcel and Pascal, Mathilde and Larrieu, Sophie and Beaudeau, Pascal and Cassadou, Sylvie and Eilstein, Daniel and Filleul, Laurent and Le Tertre, Alain and Medina, Sylvia and Pascal, Laurence and Prouvost, Helene and Quénel, Philippe and Zeghnoun, Abdelkrim and Lefranc, Agnes},
	month = dec,
	year = {2015},
	keywords = {Air pollution, Cohort study, Epidemiology, France, Mortality},
	pages = {5--14},
	file = {1-s2.0-S0160412015300349-main.pdf:files/496/1-s2.0-S0160412015300349-main.pdf:application/pdf;ScienceDirect Snapshot:files/497/S0160412015300349.html:text/html},
}

@article{spiegelhalter_visualizing_2011,
	title = {Visualizing {Uncertainty} {About} the {Future}},
	volume = {333},
	copyright = {Copyright © 2011, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/333/6048/1393},
	doi = {10.1126/science.1191181},
	abstract = {We are all faced with uncertainty about the future, but we can get the measure of some uncertainties in terms of probabilities. Probabilities are notoriously difficult to communicate effectively to lay audiences, and in this review we examine current practice for communicating uncertainties visually, using examples drawn from sport, weather, climate, health, economics, and politics. Despite the burgeoning interest in infographics, there is limited experimental evidence on how different types of visualizations are processed and understood, although the effectiveness of some graphics clearly depends on the relative numeracy of an audience. Fortunately, it is increasingly easy to present data in the form of interactive visualizations and in multiple types of representation that can be adjusted to user needs and capabilities. Nonetheless, communicating deeper uncertainties resulting from incomplete or disputed knowledge—or from essential indeterminacy about the future—remains a challenge.},
	language = {en},
	number = {6048},
	urldate = {2016-02-22},
	journal = {Science},
	author = {Spiegelhalter, David and Pearson, Mike and Short, Ian},
	month = sep,
	year = {2011},
	pmid = {21903802},
	keywords = {probabilistic forecasts},
	pages = {1393--1400},
	file = {Snapshot:files/500/1393.html:text/html},
}

@article{genuer_variable_2010,
	title = {Variable selection using random forests},
	volume = {31},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865510000954},
	doi = {10.1016/j.patrec.2010.03.014},
	abstract = {This paper proposes, focusing on random forests, the increasingly used statistical method for classification and regression problems introduced by Leo Breiman in 2001, to investigate two classical issues of variable selection. The first one is to find important variables for interpretation and the second one is more restrictive and try to design a good parsimonious prediction model. The main contribution is twofold: to provide some experimental insights about the behavior of the variable importance index based on random forests and to propose a strategy involving a ranking of explanatory variables using the random forests score of importance and a stepwise ascending variable introduction strategy.},
	number = {14},
	urldate = {2016-02-19},
	journal = {Pattern Recognition Letters},
	author = {Genuer, Robin and Poggi, Jean-Michel and Tuleau-Malot, Christine},
	month = oct,
	year = {2010},
	keywords = {Regression, Random forests, Classification, High dimensional data, Variable importance, Variable selection},
	pages = {2225--2236},
	file = {ScienceDirect Full Text PDF:files/502/Genuer et al. - 2010 - Variable selection using random forests.pdf:application/pdf;ScienceDirect Snapshot:files/503/S0167865510000954.html:text/html},
}

@techreport{gibbons_quantile_2014,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Quantile {Regression} for {Peak} {Demand} {Forecasting}},
	url = {http://papers.ssrn.com/abstract=2485657},
	abstract = {We demonstrate that annual peak demand days are characterized by both extreme values of predictors (such as weather) and large unpredictable "shocks" to demand. OLS approaches incorporate the former feature, but not the latter, leading OLS to produce downwardly-biased estimates of the annual peak. We develop a new estimation procedure, optimal forecast quantile regression (OFQR), that uses quantile regression to estimate a model of daily peak demand, then uses a loss function framework to estimate a quantile to predict the annual peak. We compare the results of the OLS and OFQR estimation approaches for 32 utility zones. While the OFQR approach is unbiased, OLS under-forecasts by nearly 5\% on average. Further, OFQR reduces the average absolute percent error by 43\%. A bootstrapping procedure generates forecast intervals with accurate 95\% coverage in sample and 87\% coverage out of sample.},
	number = {ID 2485657},
	urldate = {2016-02-19},
	institution = {Social Science Research Network},
	author = {Gibbons, Charles and Faruqui, Ahmad},
	month = jul,
	year = {2014},
	keywords = {probabilistic forecasts, Ahmad  Faruqui, application, Charles  Gibbons, Quantile Regression for Peak Demand Forecasting, SSRN},
	file = {Gibbons and Faruqui.pdf:files/504/Gibbons and Faruqui.pdf:application/pdf;Snapshot:files/505/papers.html:text/html},
}

@article{hyndman_density_2010,
	title = {Density {Forecasting} for {Long}-{Term} {Peak} {Electricity} {Demand}},
	volume = {25},
	issn = {0885-8950},
	doi = {10.1109/TPWRS.2009.2036017},
	abstract = {Long-term electricity demand forecasting plays an important role in planning for future generation facilities and transmission augmentation. In a long-term context, planners must adopt a probabilistic view of potential peak demand levels. Therefore density forecasts (providing estimates of the full probability distributions of the possible future values of the demand) are more helpful than point forecasts, and are necessary for utilities to evaluate and hedge the financial risk accrued by demand variability and forecasting uncertainty. This paper proposes a new methodology to forecast the density of long-term peak electricity demand. Peak electricity demand in a given season is subject to a range of uncertainties, including underlying population growth, changing technology, economic conditions, prevailing weather conditions (and the timing of those conditions), as well as the general randomness inherent in individual usage. It is also subject to some known calendar effects due to the time of day, day of week, time of year, and public holidays. A comprehensive forecasting solution is described in this paper. First, semi-parametric additive models are used to estimate the relationships between demand and the driver variables, including temperatures, calendar effects and some demographic and economic variables. Then the demand distributions are forecasted by using a mixture of temperature simulation, assumed future economic scenarios, and residual bootstrapping. The temperature simulation is implemented through a new seasonal bootstrapping method with variable blocks. The proposed methodology has been used to forecast the probability distribution of annual and weekly peak electricity demand for South Australia since 2007. The performance of the methodology is evaluated by comparing the forecast results with the actual demand of the summer 2007-2008.},
	number = {2},
	journal = {IEEE Transactions on Power Systems},
	author = {Hyndman, R.J. and Fan, Shu},
	month = may,
	year = {2010},
	keywords = {time series, simulation, load forecasting, probability, risk management, application, demand distributions, demand forecasting, demand variability, demographic variables, Density forecast, Density forecasting, economic variables, financial risk, forecasting uncertainty, future economic scenarios, future generation facilities, long-term demand forecasting, long-term peak electricity demand, power system economics, probability distributions, residual bootstrapping, temperature simulation, transmission augmentation},
	pages = {1142--1153},
	file = {IEEE Xplore Abstract Record:files/507/articleDetails.html:text/html;IEEE Xplore Full Text PDF:files/506/Hyndman y Fan - 2010 - Density Forecasting for Long-Term Peak Electricity.pdf:application/pdf},
}

@article{burgette_exploratory_2011,
	title = {Exploratory quantile regression with many covariates: an application to adverse birth outcomes},
	volume = {22},
	issn = {1531-5487},
	shorttitle = {Exploratory quantile regression with many covariates},
	doi = {10.1097/EDE.0b013e31822908b3},
	abstract = {Covariates may affect continuous responses differently at various points of the response distribution. For example, some exposure might have minimal impact on conditional means, whereas it might lower conditional 10th percentiles sharply. Such differential effects can be important to detect. In studies of the determinants of birth weight, for instance, it is critical to identify exposures like the one above, since low birth weight is a risk factor for later health problems. Effects of covariates on the tails of distributions can be obscured by models (such as linear regression) that estimate conditional means; however, effects on tails can be detected by quantile regression. We present 2 approaches for exploring high-dimensional predictor spaces to identify important predictors for quantile regression. These are based on the lasso and elastic net penalties. We apply the approaches to a prospective cohort study of adverse birth outcomes that includes a wide array of demographic, medical, psychosocial, and environmental variables. Although tobacco exposure is known to be associated with lower birth weights, the analysis suggests an interesting interaction effect not previously reported: tobacco exposure depresses the 20th and 30th percentiles of birth weight more strongly when mothers have high levels of lead in their blood compared with those who have low blood lead levels.},
	language = {eng},
	number = {6},
	journal = {Epidemiology (Cambridge, Mass.)},
	author = {Burgette, Lane F. and Reiter, Jerome P. and Miranda, Marie Lynn},
	month = nov,
	year = {2011},
	pmid = {21968775},
	keywords = {Female, Humans, application, Causality, Data Interpretation, Statistical, Infant, Low Birth Weight, Infant, Newborn, Linear Models, Pregnancy, Pregnancy Outcome, Premature Birth, Prenatal Exposure Delayed Effects, regression analysis},
	pages = {859--866},
}

@article{mondiana_statictical_2012,
	title = {Statictical {Downscaling} {Modeling} with {Quantile} {Regression} to {Estimate} {Extreme} {Precipitation} ({A} {Case} {Study} in {Bangkir} {Station}, {Indramayu}).},
	url = {http://repository.ipb.ac.id/handle/123456789/61568},
	abstract = {Statistical downscaling (SD) is a technique used to model the relationship between global-scale data and local-scale data with statistics model. The global-scale outcomes of Global Circulation Model (GCM) are used as independent variables in SD. Various methods of SD include multiple regression analysis, principal component regression analysis and artificial neural networks. However, these methods can not accurately predict extreme events. Quantile regression can be used to detect extreme conditions, both extreme dry and extreme wet. The aim of this study was to predict the extreme event and its probabability. The data of independent variables used were monthly rainfall of the district Indramayu Bangkir station. Quantile regression method was used to predict extreme rainfall and logistic regression to estimate the chances of extreme events. Quantile regression models formed had correct prediction rate in the 90th quantile in February. The probability of extreme rainfall events in quantile 75 was high in November, December, January, February and March. However, the occurrence probability in quantile 90 and 95, was only high in February. The prediction and probability of extreme rainfall based on quantile regression models and logistic regression showed similar trend with the data pattern observed in extreme conditions.},
	urldate = {2016-02-17},
	author = {Mondiana, Yani Quarta},
	year = {2012},
	file = {Full Text PDF:files/513/Mondiana - 2012 - Statictical Downscaling Modeling with Quantile Reg.pdf:application/pdf;Snapshot:files/514/61568.html:text/html},
}

@article{koenker_quantile_2006,
	title = {Quantile {Autoregression}},
	volume = {101},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1198/016214506000000672},
	doi = {10.1198/016214506000000672},
	abstract = {We consider quantile autoregression (QAR) models in which the autoregressive coefficients can be expressed as monotone functions of a single, scalar random variable. The models can capture systematic influences of conditioning variables on the location, scale, and shape of the conditional distribution of the response, and thus constitute a significant extension of classical constant coefficient linear time series models in which the effect of conditioning is confined to a location shift. The models may be interpreted as a special case of the general random-coefficient autoregression model with strongly dependent coefficients. Statistical properties of the proposed model and associated estimators are studied. The limiting distributions of the autoregression quantile process are derived. QAR inference methods are also investigated. Empirical applications of the model to the U.S. unemployment rate, short-term interest rate, and gasoline prices highlight the model's potential.},
	number = {475},
	urldate = {2016-02-16},
	journal = {Journal of the American Statistical Association},
	author = {Koenker, Roger and Xiao, Zhijie},
	month = sep,
	year = {2006},
	pages = {980--990},
	file = {Quantile Autoregression.pdf:files/510/Quantile Autoregression.pdf:application/pdf;Snapshot:files/516/016214506000000672.html:text/html},
}

@article{sun_real-time_2012,
	title = {Real-{Time} {Prediction} of {Waiting} {Time} in the {Emergency} {Department}, {Using} {Quantile} {Regression}},
	volume = {60},
	issn = {0196-0644},
	url = {http://www.annemergmed.com/article/S0196064412002624/abstract},
	doi = {10.1016/j.annemergmed.2012.03.011},
	abstract = {Study objective
Emergency department (ED) waiting times can affect patient satisfaction and quality of care. We develop and validate a model that predicts an individual patient's median and 95th percentile waiting time by using only data available at triage.
Methods
From the existing ED information system, we extracted date and time of triage completion, start time of emergency physician consultation, and patient acuity category (1=most urgent, 3=least urgent). Quantile regression was applied for model development and parameter estimation by using visits from January 2011. We assessed absolute prediction error, defined as the median difference between the 50th percentile (median) predicted waiting time and actual waiting time, and the proportion of underestimated prediction, defined as the percentage of patients whose actual waiting time exceeded the 95th percentile prediction. The model was validated retrospectively with June 2010 data and prospectively with data from April to June 2011 after integration with the existing ED information system.
Results
The derivation set included 13,200 ED visits; 903 (6.8\%) were patient acuity category 1, 5,530 (41.9\%) were patient acuity category 2, and 6,767 (51.3\%) were patient acuity category 3. The median and 95th percentile waiting times were 17 and 57 minutes for patient acuity category 2 and 21 and 89 minutes for patient acuity category 3, respectively. The final model used predictors of patient acuity category, patient queue sizes, and flow rates only. In the retrospective validation, 5.9\% of patient acuity category 2 and 5.4\% of category 3 waiting times were underestimated. The median absolute prediction error was 11.9 minutes (interquantile range [IQR] 5.9 to 22.1 minutes) for patient acuity category 2 and 15.7 minutes (IQR 7.5 to 30.1 minutes) for category 3. In prospective validation, 4.3\% of patient acuity category 2 and 5.8\% of category 3 waiting times were underestimated. The median absolute prediction error was 9.2 minutes (IQR 4.4 to 15.1 minutes) for patient acuity category 2 and 12.9 minutes (IQR 6.5 to 22.5 minutes) for category 3.
Conclusion
Using only a few data elements available at triage, the model predicts individual patients' waiting time with good accuracy.},
	language = {English},
	number = {3},
	urldate = {2016-02-16},
	journal = {Annals of Emergency Medicine},
	author = {Sun, Yan and Teow, Kiok Liang and Heng, Bee Hoon and Ooi, Chee Kheong and Tay, Seow Yian},
	month = sep,
	year = {2012},
	pages = {299--308},
	file = {PIIS0196064412002624.pdf:files/508/PIIS0196064412002624.pdf:application/pdf;Snapshot:files/517/abstract.html:text/html},
}

@article{soyiri_forecasting_2012,
	title = {Forecasting peak asthma admissions in {London}: an application of quantile regression models},
	volume = {57},
	issn = {0020-7128, 1432-1254},
	shorttitle = {Forecasting peak asthma admissions in {London}},
	url = {http://link.springer.com/article/10.1007/s00484-012-0584-0},
	doi = {10.1007/s00484-012-0584-0},
	abstract = {Asthma is a chronic condition of great public health concern globally. The associated morbidity, mortality and healthcare utilisation place an enormous burden on healthcare infrastructure and services. This study demonstrates a multistage quantile regression approach to predicting excess demand for health care services in the form of asthma daily admissions in London, using retrospective data from the Hospital Episode Statistics, weather and air quality. Trivariate quantile regression models (QRM) of asthma daily admissions were fitted to a 14-day range of lags of environmental factors, accounting for seasonality in a hold-in sample of the data. Representative lags were pooled to form multivariate predictive models, selected through a systematic backward stepwise reduction approach. Models were cross-validated using a hold-out sample of the data, and their respective root mean square error measures, sensitivity, specificity and predictive values compared. Two of the predictive models were able to detect extreme number of daily asthma admissions at sensitivity levels of 76 \% and 62 \%, as well as specificities of 66 \% and 76 \%. Their positive predictive values were slightly higher for the hold-out sample (29 \% and 28 \%) than for the hold-in model development sample (16 \% and 18 \%). QRMs can be used in multistage to select suitable variables to forecast extreme asthma events. The associations between asthma and environmental factors, including temperature, ozone and carbon monoxide can be exploited in predicting future events using QRMs.},
	language = {en},
	number = {4},
	urldate = {2016-02-16},
	journal = {International Journal of Biometeorology},
	author = {Soyiri, Ireneous N. and Reidpath, Daniel D. and Sarran, Christophe},
	month = aug,
	year = {2012},
	keywords = {meteorology, application, Animal Physiology, Asthma, Biophysics and Biological Physics, Emergency department, Environment, general, Environmental Health, Health forecast, Hospital admission, Lag, Plant Physiology, Predictive model},
	pages = {569--578},
	file = {art%3A10.1007%2Fs00484-012-0584-0.pdf:files/509/art%3A10.1007%2Fs00484-012-0584-0.pdf:application/pdf;Snapshot:files/518/10.html:text/html},
}

@article{soyiri_use_2013,
	title = {The {Use} of {Quantile} {Regression} to {Forecast} {Higher} {Than} {Expected} {Respiratory} {Deaths} in a {Daily} {Time} {Series}: {A} {Study} of {New} {York} {City} {Data} 1987-2000},
	volume = {8},
	shorttitle = {The {Use} of {Quantile} {Regression} to {Forecast} {Higher} {Than} {Expected} {Respiratory} {Deaths} in a {Daily} {Time} {Series}},
	url = {http://dx.doi.org/10.1371/journal.pone.0078215},
	doi = {10.1371/journal.pone.0078215},
	abstract = {Forecasting higher than expected numbers of health events provides potentially valuable insights in its own right, and may contribute to health services management and syndromic surveillance. This study investigates the use of quantile regression to predict higher than expected respiratory deaths.Data taken from 70,830 deaths occurring in New York were used. Temporal, weather and air quality measures were fitted using quantile regression at the 90th-percentile with half the data (in-sample). Four QR models were fitted: an unconditional model predicting the 90th-percentile of deaths (Model 1), a seasonal / temporal (Model 2), a seasonal, temporal plus lags of weather and air quality (Model 3), and a seasonal, temporal model with 7-day moving averages of weather and air quality. Models were cross-validated with the out of sample data. Performance was measured as proportionate reduction in weighted sum of absolute deviations by a conditional, over unconditional models; i.e., the coefficient of determination (R1).The coefficient of determination showed an improvement over the unconditional model between 0.16 and 0.19. The greatest improvement in predictive and forecasting accuracy of daily mortality was associated with the inclusion of seasonal and temporal predictors (Model 2). No gains were made in the predictive models with the addition of weather and air quality predictors (Models 3 and 4). However, forecasting models that included weather and air quality predictors performed slightly better than the seasonal and temporal model alone (i.e., Model 3 {\textgreater} Model 4 {\textgreater} Model 2)This study provided a new approach to predict higher than expected numbers of respiratory related-deaths. The approach, while promising, has limitations and should be treated at this stage as a proof of concept.},
	number = {10},
	urldate = {2016-02-16},
	journal = {PLoS ONE},
	author = {Soyiri, Ireneous N. and Reidpath, Daniel D.},
	month = oct,
	year = {2013},
	keywords = {probabilistic forecasts, application},
	pages = {e78215},
	file = {PLoS Full Text PDF:files/519/Soyiri and Reidpath - 2013 - The Use of Quantile Regression to Forecast Higher .pdf:application/pdf},
}

@article{martinez-silva_forecasting_2016,
	title = {Forecasting {SO} $_{\textrm{2}}$ pollution incidents by means of quantile curves based on additive models: {FORECASTING} {SO} $_{\textrm{2}}$ {INCIDENTS} {BY} {MEANS} {OF} {QC} {BASED} {ON} {AM}},
	issn = {11804009},
	shorttitle = {Forecasting {SO} $_{\textrm{2}}$ pollution incidents by means of quantile curves based on additive models},
	url = {http://doi.wiley.com/10.1002/env.2384},
	doi = {10.1002/env.2384},
	language = {en},
	urldate = {2016-02-16},
	journal = {Environmetrics},
	author = {Martínez-Silva, I. and Roca-Pardiñas, J. and Ordóñez, C.},
	year = {2016},
	keywords = {probabilistic forecasts, application},
	pages = {n/a--n/a},
}

@article{ayuntamiento_de_madrid_acuerdos_2015,
	title = {Acuerdos de la sesión ordinaria del {Pleno} celebrada el día 25 de noviembre de 2015},
	volume = {7569},
	url = {http://www.madrid.es/portales/munimadrid/es/Inicio/El-Ayuntamiento/El-Pleno/Actividad-del-Pleno-y-las-Comisiones/3-Acuerdos-del-Pleno-ordinario-de-25-de-noviembre-de-2015?vgnextfmt=default&vgnextoid=086df2e1792f1510VgnVCM2000000c205a0aRCRD&vgnextchannel=fa9e1f2f2b40e210VgnVCM2000000c205a0aRCRD&idioma=es&numPagina=0},
	journal = {Boletín Oficial del Ayuntamiento de Madrid},
	author = {{Ayuntamiento de Madrid}},
	month = jan,
	year = {2015},
	pages = {7},
	file = {boam7569_3.pdf:files/520/boam7569_3.pdf:application/pdf},
}

@misc{smart_citizen_smart_2016,
	title = {Smart {Citizen}: {Citizen} {Science} {Platform} for participatory processes of the people in the cities},
	shorttitle = {Smart {Citizen}},
	url = {https://smartcitizen.me/#sck},
	abstract = {Smart Citizen is a platform to generate participatory processes of the people in the cities. Connecting data, people and knowledge, the objective of the platform is to serve as a node for building productive open indicators and distributed tools, and thereafter the collective construction of the city for its own inhabitants. \#smartcitizenkit @smartcitizen \#fablabbcn \#hangar \#iot},
	urldate = {2016-01-11},
	author = {{Smart Citizen}},
	year = {2016},
	file = {Snapshot:files/522/smartcitizen.me.html:text/html},
}

@article{el_confidencial_autobuses_2015,
	chapter = {Noticias de Madrid},
	title = {Los autobuses de {Madrid}, nuevos medidores de la calidad del aire y niveles de polen},
	url = {http://www.elconfidencial.com/espana/madrid/2015-10-13/autobuses-madrid-calidad-aire-polen_1058172/},
	abstract = {Los autobuses de Madrid, nuevos medidores de la calidad del aire y niveles de polen. Noticias de Madrid. La EMT desarrolla una plataforma big data para ciudades inteligentes que recoge información a través de diferentes sensores para su tratamiento posterior y su difusión a los ciudadanos},
	urldate = {2016-01-11},
	journal = {El Confidencial},
	author = {{El Confidencial}},
	month = oct,
	year = {2015},
	file = {Snapshot:files/521/autobuses-madrid-calidad-aire-polen_1058172.html:text/html},
}

@misc{noauthor_smart_nodate,
	title = {Smart {Citizen} - {Sensores} ciudadanos},
	url = {http://goteo.org/project/smart-citizen-sensores-ciudadanos},
	abstract = {Conviértete en un sensor inteligente de la ciudad, generando y compartiendo datos reales en abierto sobre contaminación del aire, acústica y muchos más},
	urldate = {2016-01-11},
	journal = {Goteo.org},
	file = {Snapshot:files/523/smart-citizen-sensores-ciudadanos.html:text/html},
}

@misc{environmental_research_group_kings_college_london_london_2016,
	title = {London {Air} {Quality} {Network}},
	url = {http://www.londonair.org.uk/LondonAir/Default.aspx},
	urldate = {2016-01-11},
	author = {{Environmental Research Group, King's College London}},
	year = {2016},
	file = {London Air Quality Network || The comprehensive source of information about air pollution in London || Home:files/524/Default.html:text/html},
}

@misc{ayuntamiento_de_madrid_ayuntamiento_2015,
	title = {El {Ayuntamiento} estudia el nuevo {Protocolo} con la oposición y expertos - {Ayuntamiento} de {Madrid}},
	url = {http://www.madrid.es/portales/munimadrid/es/Inicio/Actualidad/Noticias/El-Ayuntamiento-estudia-el-nuevo-Protocolo-con-la-oposicion-y-expertos?vgnextfmt=default&vgnextoid=f82cc1723dc81510VgnVCM1000000b205a0aRCRD&vgnextchannel=a12149fa40ec9410VgnVCM100000171f5a0aRCRD},
	urldate = {2016-01-11},
	author = {{Ayuntamiento de Madrid}},
	month = dec,
	year = {2015},
	file = {El Ayuntamiento estudia el nuevo Protocolo con la oposición y expertos - Ayuntamiento de Madrid:files/525/El-Ayuntamiento-estudia-el-nuevo-Protocolo-con-la-oposicion-y-expertos.html:text/html},
}

@techreport{ayuntamiento_de_madrid_informe_2015,
	title = {Informe de {Evaluación} del {Plan} de {Calidad} del {Aire} de la {Ciudad} de {Madrid} 2011 -- 2015},
	institution = {Dirección General de Sostenibilidad, Área de Gobierno de Medio Ambiente, Seguridad y Movilidad},
	author = {{Ayuntamiento de Madrid}},
	month = mar,
	year = {2015},
	file = {EvalPlanCalAire20112015.pdf:files/526/EvalPlanCalAire20112015.pdf:application/pdf},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Abstract} ({Authors}) - {Enhancing} distributed generation penetration in smart grids through dynamic ratings},
	url = {http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?reload=true&arnumber=6652458},
	urldate = {2015-12-12},
	file = {IEEE Xplore Abstract (Authors) - Enhancing distributed generation penetration in smart grids through dynamic ratings:files/527/abstractAuthors.html:text/html},
}

@misc{noauthor_ieee_nodate-1,
	title = {{IEEE} {Xplore} {Abstract} - {Large} penetration of distributed productions: {Dynamic} line rating and flexible generation, a must re...},
	url = {http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6302480&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6302480},
	urldate = {2015-12-11},
	file = {IEEE Xplore Abstract - Large penetration of distributed productions\: Dynamic line rating and flexible generation, a must re...:files/529/login.html:text/html},
}

@article{lumbreras_assessment_2008,
	title = {Assessment of vehicle emissions projections in {Madrid} ({Spain}) from 2004 to 2012 considering several control strategies},
	volume = {42},
	issn = {0965-8564},
	url = {http://www.sciencedirect.com/science/article/pii/S0965856408000141},
	doi = {10.1016/j.tra.2008.01.026},
	abstract = {Road transport is a major source of air pollutant emissions in European cities. Moreover, vehicle exhaust emissions have been the cause of much concern about the effects of urban air pollution on human health. Local authorities need to develop strategies to control vehicular emissions through technological and socioeconomical measures. For this reason, an efficiency assessment of possible future measures to reduce air pollution is required for future traffic planning, regulatory and fiscal initiatives. This paper presents the assessment of several mobility and technology scenarios that can be used for emission reductions in Madrid (Spain) in the period 2004–2012. Pollutants considered are those related to typical air quality problems in urban areas in Europe (SO2, NOx, NMVOC, heavy metals, CO and particulate matter) and CO2 as a greenhouse gas.

Results show an expected increase in mobility but a decreasing trend in future traffic-related emissions, except for CO2. This reduction is due to technological improvements linked to European Legislation for road vehicles (Euro Standards). CO2 emissions are expected to increase because the technological improvements will not be able to counteract the effect of the large mobility increase. With regard to control strategies, the most effective measure for emission reductions is fleet renewal. According to the hypotheses made in the paper, this would reduce, on average, the pollutant emission by 16.04\%. With regard to CO2 emissions, the use of biofuels and the decrease in mobility are the most effective measures.},
	number = {4},
	urldate = {2015-09-07},
	journal = {Transportation Research Part A: Policy and Practice},
	author = {Lumbreras, J. and Valdés, M. and Borge, R. and Rodríguez, M. E.},
	month = may,
	year = {2008},
	keywords = {Air pollution, Emission control, Mobility plans, Urban air quality, Vehicle emissions projections},
	pages = {646--658},
	file = {ScienceDirect Snapshot:files/536/S0965856408000141.html:text/html},
}

@article{lumbreras_computation_2009,
	title = {Computation of uncertainty for atmospheric emission projections from key pollutant sources in {Spain}},
	volume = {43},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231008009795},
	doi = {10.1016/j.atmosenv.2008.10.048},
	abstract = {Emission projections are important for environmental policy, both to evaluate the effectiveness of abatement strategies and to determine legislation compliance in the future. Moreover, including uncertainty is an essential added value for decision makers. In this work, projection values and their associated uncertainty are computed for pollutant emissions corresponding to the most significant activities from the national atmospheric emission inventory in Spain. Till now, projections had been calculated under three main scenarios: “without measures” (WoM), “with measures” (WM) and “with additional measures” (WAM). For the first one, regression techniques had been applied, which are inadequate for time-dependent data. For the other scenarios, values had been computed taking into account expected activity growth, as well as policies and measures. However, only point forecasts had been computed. In this work statistical methodology has been applied for: a) Inclusion of projection intervals for future time points, where the width of the intervals is a measure of uncertainty. b) For the WoM scenario, ARIMA models are applied to model the dynamics of the processes. c) In the WM scenario, bootstrap is applied as an additional non-parametric tool, which does not rely on distributional assumptions and is thus more general. The advantages of using ARIMA models for the WoM scenario including uncertainty are shown. Moreover, presenting the WM scenario allows observing if projected emission values fall within the intervals, thus showing if the measures to be taken to reach the scenario imply a significant improvement. Results also show how bootstrap techniques incorporate stochastic modelling to produce forecast intervals for the WM scenario.},
	number = {8},
	urldate = {2015-09-07},
	journal = {Atmospheric Environment},
	author = {Lumbreras, Julio and García-Martos, Carolina and Mira, José and Borge, Rafael},
	month = mar,
	year = {2009},
	keywords = {Uncertainty, ARIMA models, Bootstrap, Emission projections, Time series analysis},
	pages = {1557--1564},
	file = {ScienceDirect Snapshot:files/537/S1352231008009795.html:text/html},
}

@article{borge_development_2008,
	title = {Development of a high-resolution emission inventory for {Spain} using the {SMOKE} modelling system: {A} case study for the years 2000 and 2010},
	volume = {23},
	issn = {1364-8152},
	shorttitle = {Development of a high-resolution emission inventory for {Spain} using the {SMOKE} modelling system},
	url = {http://www.sciencedirect.com/science/article/pii/S1364815207002174},
	doi = {10.1016/j.envsoft.2007.11.002},
	abstract = {Emission preparation is a critical stage in air quality modelling. The generation of input information from regulatory emission inventories compatible with the requirements of eulerian chemical-transport models needs a computationally efficient, reliable, and flexible emissions data processing system. The Sparse Matrix Operator Kernel System (SMOKE) was developed in the United States and redesigned by the US Environmental Protection Agency to support air quality modelling activities in the framework of the USEPA Models-3 system. In this contribution the adaptation of the SMOKE system to European conditions is discussed. The system has been applied to the Iberian Peninsula and the Madrid Region (Spain) to process Spain's official emission inventories and projections for the years 2000 and 2010. This software tool has been found useful to generate emission input information for the CMAQ model as well as providing a valuable platform for emission scenario analysis. The model has been proved flexible enough to accommodate and process emissions based on the EMEP/CORINAIR methodology, although the lack of meaningful ancillary information may hinder its application outside the US. This study has established a practical methodology for the adaptation of the SMOKE system to Spain and potentially to any other European country.},
	number = {8},
	urldate = {2015-09-07},
	journal = {Environmental Modelling \& Software},
	author = {Borge, Rafael and Lumbreras, Julio and Rodríguez, Encarnacion},
	month = aug,
	year = {2008},
	keywords = {Air quality, Spain, Atmospheric emissions, Emission preparation for modelling, SMOKE},
	pages = {1026--1044},
	file = {ScienceDirect Snapshot:files/338/S1364815207002174.html:text/html},
}

@article{borge_comprehensive_2008,
	title = {A comprehensive sensitivity analysis of the {WRF} model for air quality applications over the {Iberian} {Peninsula}},
	volume = {42},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231008007954},
	doi = {10.1016/j.atmosenv.2008.08.032},
	abstract = {Meteorological inputs play a vital role on regional air quality modelling. An extensive sensitivity analysis of the Weather Research and Forecasting (WRF) model was performed, in the framework of the Integrated Assessment Modelling System for the Iberian Peninsula (SIMCA) project. Up to 23 alternative model configurations, including Planetary Boundary Layer schemes, Microphysics, Land-surface models, Radiation schemes, Sea Surface Temperature and Four-Dimensional Data Assimilation were tested in a 3 km spatial resolution domain. Model results for the most significant meteorological variables, were assessed through a series of common statistics. The physics options identified to produce better results (Yonsei University Planetary Boundary Layer, WRF Single-Moment 6-class microphysics, Noah Land-surface model, Eta Geophysical Fluid Dynamics Laboratory longwave radiation and MM5 shortwave radiation schemes) along with other relevant user settings (time-varying Sea Surface Temperature and combined grid-observational nudging) where included in a “best case” configuration. This setup was tested and found to produce more accurate estimation of temperature, wind and humidity fields at surface level than any other configuration for the two episodes simulated. Planetary Boundary Layer height predictions showed a reasonable agreement with estimations derived from routine atmospheric soundings. Although some seasonal and geographical differences were observed, the model showed an acceptable behaviour overall. Despite being useful to define the most appropriate setup of the WRF model for air quality modelling over the Iberian Peninsula, this study provides a general overview of WRF sensitivity and can constitute a reference for future mesoscale meteorological modelling exercises.},
	number = {37},
	urldate = {2015-09-07},
	journal = {Atmospheric Environment},
	author = {Borge, Rafael and Alexandrov, Vassil and José del Vas, Juan and Lumbreras, Julio and Rodríguez, Encarnacion},
	month = dec,
	year = {2008},
	keywords = {Air quality modelling, Sensitivity analysis, Iberian Peninsula, Meterological modelling, WRF setup},
	pages = {8560--8574},
	file = {ScienceDirect Snapshot:files/339/S1352231008007954.html:text/html},
}

@article{guaita_short-term_2011,
	title = {Short-term impact of particulate matter ({PM}(2.5)) on respiratory mortality in {Madrid}},
	volume = {21},
	issn = {1369-1619},
	doi = {10.1080/09603123.2010.544033},
	abstract = {OBJECTIVES: This paper sought to quantify the particulate matter (PM(2.5)) pollutant's impact on short-term daily respiratory-cause mortality in the city of Madrid.
METHODS: As our dependent variable, we took daily mortality registered in Madrid from 1 January 2003 to 31 December 2005, attributed to all diseases of the respiratory system as classified under heads J00-J99 of the ICD 10 and broken down as follows: J12-J18, pneumonia; J40-J44, chronic diseases of the respiratory system except asthma; J45-J46, asthma; and J96, respiratory failure.
RESULTS: The relative risk (RR) for daily overall respiratory mortality was RR 1.0281 (1.0043-1.0520), with a proportional attributable risk (PAR) of 2.74\%. This effect occurred in lag 1; respiratory failure, RR 1.0816 (1.0119-1.1512) and PAR 7.54\% at lag 5; and pneumonia, RR 1.0438 (1.0001-1.0875) and PAR 4.19\% at lag 6.
CONCLUSIONS: Our results reflect the association that exists between PM(2.5) concentrations and daily respiratory-cause mortality.},
	language = {eng},
	number = {4},
	journal = {International Journal of Environmental Health Research},
	author = {Guaita, Rosana and Pichiule, Miryam and Maté, Tomás and Linares, Cristina and Díaz, Julio},
	month = aug,
	year = {2011},
	pmid = {21644129},
	keywords = {Particulate Matter, Spain, Particle Size, Air Pollutants, Humans, Risk, Cities, Environmental Monitoring, Epidemiological Monitoring, Respiration Disorders},
	pages = {260--274},
	file = {respiratoriasPM2,5.pdf:files/340/respiratoriasPM2,5.pdf:application/pdf},
}

@article{gomez-moreno_influence_2011,
	title = {Influence of seasonal factors on the atmospheric particle number concentration and size distribution in {Madrid}},
	volume = {45},
	issn = {1352-2310},
	url = {http://www.sciencedirect.com/science/article/pii/S1352231011001853},
	doi = {10.1016/j.atmosenv.2011.02.041},
	abstract = {The ambient particle number concentration and size distribution have been measured in an urban background site in Madrid, a continental Mediterranean area, over more than two years (Oct 2006–Dec 2008). The objective was to study the sources and processes affecting or contributing to fine and ultrafine particles in this area. They have been measured with a TSI–SMPS (15–600 nm) instrument and with a modified Vienna type DMA (3–80 nm) and a CPC 3025 (TSI) during 6 months. The average particle number concentration was lower than in other sites as it is an urban background site and because of its location in the Mediterranean area. The particle number concentrations have shown a clear seasonal influence: maximum values were observed every year in the period November–January, coinciding with atmospheric stagnant conditions and pollution episodes, while minimum values were measured in springtime, a period in which wind speed produced high atmospheric dilution. The Aitken and accumulation modes have shown similar seasonal behavior, with two maxima related to vehicle emissions. The nucleation mode had a third maximum observed at noon during spring and summer. The size distributions were bimodal during most of the time: the first mode was centered on 20–50 nm and was associated with fresh particles related to vehicle exhaust emission; the second mode, between 50 and 160 nm, mainly corresponded to the evolution of the first mode. The evolution of the size distributions reveals a marked annual cycle related to the season, with an increase of median diameters during summer and a decrease during winter. Different evolutions of particle size distribution corresponding to different meteorological and seasonal scenarios were identified. The influence of higher wind speeds on particle size distribution has been confirmed to cause a decrease in the particle number concentration and in the size distribution mode. Particle nucleation is not a frequent phenomenon in this measurement site, where 63 events per year have been observed. They mainly occurred during spring and summer periods, with the minimum number during winter. This suggests that insolation and temperature are important variables in nucleation. Class Ia nucleation events mainly occurred during spring and summer. High wind speeds were important during class II events, as the particles suffered low growth or lost their semivolatile compounds.},
	number = {18},
	urldate = {2015-09-01},
	journal = {Atmospheric Environment},
	author = {Gómez-Moreno, F. J. and Pujadas, M. and Plaza, J. and Rodríguez-Maroto, J. J. and Martínez-Lozano, P. and Artíñano, B.},
	month = jun,
	year = {2011},
	keywords = {wind speed, Fine particles, Nucleation mode, Particle nucleation, Particle number concentration, Particle size distribution},
	pages = {3169--3180},
	file = {ScienceDirect Full Text PDF:files/342/Gómez-Moreno et al. - 2011 - Influence of seasonal factors on the atmospheric p.pdf:application/pdf;ScienceDirect Snapshot:files/343/S1352231011001853.html:text/html},
}

@article{garrett_short-term_2011,
	title = {Short-term effect of fine particulate matter ({PM2}.5) and ozone on daily mortality in {Lisbon}, {Portugal}},
	volume = {18},
	issn = {0944-1344, 1614-7499},
	url = {http://link.springer.com/article/10.1007/s11356-011-0519-z},
	doi = {10.1007/s11356-011-0519-z},
	language = {en},
	number = {9},
	urldate = {2015-09-01},
	journal = {Environmental Science and Pollution Research},
	author = {Garrett, Pedro and Casimiro, Elsa},
	month = may,
	year = {2011},
	keywords = {time series, Air pollution, Ozone, PM2.5, Atmospheric Protection/Air Quality Control/Air Pollution, Mortality, Environment, general, Industrial Pollution Prevention, Lisbon, Portugal, Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution},
	pages = {1585--1592},
	file = {Full Text PDF:files/344/Garrett y Casimiro - 2011 - Short-term effect of fine particulate matter (PM2..pdf:application/pdf;Snapshot:files/345/s11356-011-0519-z.html:text/html},
}

@article{monzon_seasonal_1999,
	title = {Seasonal analysis of air pollution levels in {Madrid}},
	volume = {235},
	issn = {0048-9697},
	url = {http://www.sciencedirect.com/science/article/pii/S0048969799002296},
	doi = {10.1016/S0048-9697(99)00229-6},
	abstract = {Madrid city has a high density of population and suffers from chronic congestion problems. It means that some pollutants could produce atmospheric emergency situations when weather stability periods last longer. Due to the low level of industry in the region, mobile sources have an important contribution to total emissions. Madrid has a 20-year-old air pollution control network which is composed of 24 permanent stations which control all pollutants and atmospheric variables. This paper analyses inmission values from 1990 to 1997. The analysis covers main pollutant values and their variations within the week and between seasons. The study has a twofold approach: mean-daily values and semi-hourly values. The results allow to draw some conclusions about inmission values in different areas of the city and how traffic-flow contributes to them.},
	number = {1–3},
	urldate = {2015-09-01},
	journal = {Science of The Total Environment},
	author = {Monzón, Andrés and Moragues, Amparo and Acha, Carlos},
	month = sep,
	year = {1999},
	keywords = {Air pollution, Emission, Traffic flow},
	pages = {343--345},
	file = {ScienceDirect Full Text PDF:files/346/Monzón et al. - 1999 - Seasonal analysis of air pollution levels in Madri.pdf:application/pdf;ScienceDirect Snapshot:files/347/S0048969799002296.html:text/html},
}

@article{artinano_influence_2004,
	series = {Highway and {Urban} {Pollution}},
	title = {Influence of traffic on the {PM10} and {PM2}.5 urban aerosol fractions in {Madrid} ({Spain})},
	volume = {334–335},
	issn = {0048-9697},
	url = {http://www.sciencedirect.com/science/article/pii/S004896970400364X},
	doi = {10.1016/j.scitotenv.2004.04.032},
	abstract = {A preliminary assessment carried out in Madrid demonstrates the difficulty of compliance with the new European PM10 tolerances. Daily and annual limiting values would be exceeded at almost all the network stations under the terms of the directive's second stage. An experimental study, based on the chemical characterisation of the PM10 and PM2.5 fractions sampled at a representative urban site, provides the major mass contents of these two fractions. These are mainly related to two different particle sources: combustion processes including traffic emissions and mineral-origin particles. Nonmineral carbon is the major component of particulate matter in this region, mostly in the PM2.5 fraction, increasing its contribution in wintertime. The second largest component identified in the PM10 mass, is associated with crustal origin particles and is more relevant in summer, whereas the second largest contributor to PM2.5 is secondary particles.

In general, PM10 and PM2.5 concentrations show good agreement with traffic-related pollutants, such as nitrogen oxides and CO, being time-correlated in winter pollution episodes. PM1 and PM2.5 have been simultaneous and continuously measured indicating road transport as the main source of these finer fractions. Mineral contribution has been mainly identified in the coarser particles associated with dust resuspension and some long-range transport events of Saharan dust, although they are also present in the finer PM2.5 fraction.},
	urldate = {2015-09-01},
	journal = {Science of The Total Environment},
	author = {Artı́ñano, B. and Salvador, P. and Alonso, D. G. and Querol, X. and Alastuey, A.},
	year = {2004},
	keywords = {PM10, PM2.5, Madrid, Particle composition, Particle monitoring, PM1, Road transport, Urban aerosols, Urban pollution},
	pages = {111--123},
	file = {ScienceDirect Full Text PDF:files/348/Art1ñano et al. - 2004 - Influence of traffic on the PM10 and PM2.5 urban a.pdf:application/pdf;ScienceDirect Snapshot:files/349/S004896970400364X.html:text/html},
}

@techreport{ayuntamiento_de_madrid_protocolo_2015,
	title = {Protocolo de medidas a adoptar durante episodios de alta contaminación por dióxido de nitrógeno en la ciudad de {Madrid}},
	url = {http://www.madrid.es/portales/munimadrid/es/Inicio/Normativa/ANM-2015-5-Protocolo-de-medidas-a-adoptar-durante-episodios-de-alta-contaminacion-por-dioxido-de-nitrogeno?vgnextfmt=default&vgnextoid=38941b214ad6b410VgnVCM1000000b205a0aRCRD&vgnextchannel=11cf79ed268fe410VgnVCM1000000b205a0aRCRD},
	institution = {Dirección General de Sostenibilidad y Planificación de la Movilidad, Área de Gobierno de Medio Ambiente y Movilidad},
	author = {{Ayuntamiento de Madrid}},
	month = feb,
	year = {2015},
	note = {Aprobado por la Junta de Gobierno el 5 de febrero de 2015},
	file = {protocolo_contaminacion_madrid.pdf:files/352/protocolo_contaminacion_madrid.pdf:application/pdf},
}

@misc{sice_mantenimiento_2015,
	title = {Mantenimiento y explotación del {Sistema} {Integral} de {Vigilancia}, {Predicción} e {Información} de la {Calidad} del {Aire} del {Ayuntamiento} de {Madrid}},
	url = {http://www.sice.com/es/actualidad/mantenimiento-y-explotacion-del-sistema-integral-de-vigilancia-prediccion-e-informacion},
	urldate = {2015-08-26},
	author = {{SICE}},
	month = nov,
	year = {2015},
	file = {Mantenimiento y explotación del Sistema Integral de Vigilancia, Predicción e Información de la Calidad del Aire del Ayuntamiento de Madrid | SICE:files/356/mantenimiento-y-explotacion-del-sistema-integral-de-vigilancia-prediccion-e-informacion.html:text/html},
}

@inproceedings{beatriz_de_diego_sistema_2006,
	address = {Madrid},
	title = {Sistema {Integrado} de {Vigilancia}, {Predicción} e {Información} de la {Calidad} del {Aire}},
	url = {http://www.conama8.conama.org/modulodocumentos/documentos/SDs/SD43/SD43_ppt_BeatrizdeDiego.pdf},
	urldate = {2015-08-26},
	booktitle = {{CONAMA8}, {Congreso} {Nacional} de {Medio} {Ambiente}},
	publisher = {CONAMA},
	author = {{Beatriz de Diego}},
	month = nov,
	year = {2006},
	note = {http://www.conama8.conama.org/datoscd/view\_actividades.php?id=110\&idnavegacion=273},
	file = {SD43_ppt_BeatrizdeDiego.pdf:files/350/SD43_ppt_BeatrizdeDiego.pdf:application/pdf},
}

@article{departamento_de_calidad_del_aire_aire_2008,
	title = {Un aire más limpio -- {La} red de vigilancia de la contaminación atmosférica del \{{Ayuntamiento} de {Madrid}\}},
	volume = {46},
	url = {http://www.astic.es/articulos-boletic/un-aire-m%C3%A1s-limpio},
	urldate = {2015-08-26},
	journal = {Boletic, revista de la Asociación Profesional de Cuerpos Superiores de Sistemas y Tecnologías de la Información de las Administraciones Públicas},
	author = {{Departamento de calidad del aire}},
	collaborator = {{Ayuntamiento de Madrid}},
	month = jun,
	year = {2008},
	pages = {70--75},
	file = {mono_12.pdf:files/353/mono_12.pdf:application/pdf},
}

@techreport{ayuntamiento_de_madrid_plan_2012,
	address = {Madrid},
	title = {Plan de {Calidad} del {Aire} de la ciudad de {Madrid} 2011 -- 2015},
	url = {http://www.madrid.es/UnidadesDescentralizadas/Sostenibilidad/ContenidosBasicos/Ficheros/PlanCalidadAire2012.pdf},
	urldate = {2015-08-26},
	institution = {Dirección General de Sostenibilidad, Área de Gobierno de Medio Ambiente, Seguridad y Movilidad},
	author = {{Ayuntamiento de Madrid}},
	year = {2012},
	note = {Aprobado por la Junta de Gobierno del Ayuntamiento de Madrid con fecha
26 de abril de 2012},
	file = {PlanCalidadAire2012.pdf:files/354/PlanCalidadAire2012.pdf:application/pdf},
}

@misc{ayuntamiento_de_madrid_informe_2015-1,
	title = {Informe de {Evaluación} de {Objetivos} {Operativos}. {Objetivo} {Operativo}: {Continuar} reduciendo las emisiones de contaminantes producias por el tráfico},
	url = {http://www-2.munimadrid.es/SBAE_261_SF_POG_Internet/informeAccProyObjetivoOperativoCiudadano.do?KeyOperativo=33256&KeyEstrategico=33364},
	urldate = {2015-08-26},
	journal = {Sistema Gestión Operativa (Programa Operativo de Gobierno)},
	author = {{Ayuntamiento de Madrid}},
	year = {2015},
	file = {Ayuntamiento de Madrid:files/351/informeAccProyObjetivoOperativoCiudadano.html:text/html},
}

@misc{noauthor_madrid_nodate,
	title = {madrid plan calidad del aire 2012 - {Buscar} con {Google}},
	url = {https://www.google.es/search?client=ubuntu&channel=fs&q=madrid+plan+calidad+del+aire+2012&ie=utf-8&oe=utf-8&gfe_rd=cr&ei=boTdVY-6LcGA8QePt7C4CQ},
	urldate = {2015-08-26},
	file = {madrid plan calidad del aire 2012 - Buscar con Google:files/355/search.html:text/html},
}

@misc{enyse_mantenimiento_2015,
	title = {Mantenimiento y {Explotación} del {Sistema} de {Vigilancia} de la {Calidad} del {Aire}},
	url = {http://www.enyse.es/es/proyectos-relevantes/mantenimiento-y-explotacion-del-sistema-de-vigilancia-de-la-calidad-del-aire},
	urldate = {2015-08-26},
	author = {{ENYSE}},
	month = aug,
	year = {2015},
	file = {Mantenimiento y Explotación del Sistema de Vigilancia de la Calidad del Aire (Ayuntamiento de Madrid) | ENYSE:files/358/mantenimiento-y-explotacion-del-sistema-de-vigilancia-de-la-calidad-del-aire.html:text/html},
}

@techreport{twenties_project_project_2010,
	title = {Project {Objectives} \& {KPI} ({Deliverable} 2.1)},
	institution = {Twenties consortium},
	author = {{Twenties Project}},
	year = {2010},
	note = {http://www.twenties-project.eu/documents/D\_2\_1\_Objectives\_KPIs\_Final.pdf},
}

@misc{ayuntamiento_de_madrid_cuatro_2009,
	type = {Nota de prensa},
	title = {Cuatro millones de euros para vigilar la calidad del aire},
	url = {http://www.madrid.es/portales/munimadrid/es/Inicio/El-Ayuntamiento/Medios/Notas-de-prensa/Cuatro-millones-de-euros-para-vigilar-la-calidad-del-aire?vgnextfmt=default&vgnextoid=50365f2c0abb5210VgnVCM1000000b205a0aRCRD&vgnextchannel=6091317d3d2a7010VgnVCM100000dc0ca8c0RCRD},
	urldate = {2015-08-26},
	journal = {www.madrid.es},
	author = {{Ayuntamiento de Madrid}},
	month = dec,
	year = {2009},
	file = {Cuatro millones de euros para vigilar la calidad del aire - Ayuntamiento de Madrid:files/357/Cuatro-millones-de-euros-para-vigilar-la-calidad-del-aire.html:text/html},
}

@article{sevillano_habra_2012,
	title = {¿{Habrá} contaminación mañana?},
	url = {http://ccaa.elpais.com/ccaa/2012/02/27/madrid/1330381381_991666.html},
	abstract = {Madrid oculta la predicción que antes hacía pública. El invierno más seco en décadas dispara la polución},
	language = {es},
	urldate = {2015-08-25},
	journal = {EL PAÍS},
	author = {Sevillano, Elena G.},
	month = feb,
	year = {2012},
	file = {Snapshot:files/359/1330381381_991666.html:text/html},
}

@article{espana_ley_2007,
	title = {Ley 34/2007, de 15 de noviembre, de calidad del aire y protección de la atmósfera},
	volume = {275},
	url = {https://www.boe.es/buscar/doc.php?id=BOE-A-2007-19744},
	urldate = {2015-08-25},
	journal = {Boletín Oficial del Estado},
	author = {{España}},
	month = nov,
	year = {2007},
	pages = {46962 -- 46987},
	file = {BOE.es - Documento BOE-A-2007-19744:files/361/doc.html:text/html},
}

@article{espana_real_2011,
	title = {Real {Decreto} 102/2011, de 28 de enero, relativo a la mejora de la calidad del aire},
	volume = {25},
	url = {https://www.boe.es/diario_boe/txt.php?id=BOE-A-2011-1645},
	urldate = {2015-08-25},
	journal = {Boletín Oficial del Estado},
	author = {{España}},
	month = jan,
	year = {2011},
	pages = {9574 -- 9626},
	file = {BOE.es - Documento BOE-A-2011-1645:files/360/txt.html:text/html},
}

@misc{sprl_ampacimon_nodate,
	title = {Ampacimon » {Downloads}},
	url = {http://www.ampacimon.com/downloads/},
	urldate = {2014-10-23},
	author = {SPRL, Stigmatix},
	file = {Snapshot:files/366/downloads.html:text/html},
}

@techreport{raniga_stretching_nodate,
	title = {Stretching {Transmission} {Line} {Capabilities} – {A} {Transpower} {Investigation}},
	author = {Raniga, J. and Rayudu, R. K.},
}

@misc{noauthor_soporte_nodate,
	title = {Soporte a usuarios {\textbar} {Blog} de mifirma.com},
	url = {http://blog.mifirma.com/faq-2/},
	urldate = {2014-04-08},
	file = {Snapshot:files/426/faq-2.html:text/html},
}

@misc{noauthor_seismicity_nodate,
	title = {Seismicity {Pattern} {Changes} before the {M} = 4.8 {Aeolian} {Archipelago} ({Italy}) {Earthquake} of {August} 16, 2010},
	url = {http://www.hindawi.com/journals/tswj/2014/531212/ref/},
	urldate = {2014-02-19},
	file = {Seismicity Pattern Changes before the M = 4.8 Aeolian Archipelago (Italy) Earthquake of August 16, 2010:files/431/ref.html:application/xhtml+xml},
}

@article{barnard_comparative_1980,
	title = {Comparative ultrastructural study of rat hepatocytes after treatment with the hypolipidemic agents probucol, clofibrate, and fenofibrate},
	volume = {6},
	issn = {0098-4108},
	doi = {10.1080/15287398009529872},
	abstract = {Experimental findings are reported for two similarly conducted studies. One was designed to compared rat liver cell ultrastructure during and after 91 d of dosing with probucol, a hypocholesterolemic agent, and clofibrate, a hypolipidemic drug known to elicit marked alteration of rat hepatocellular morphology. The second was designed to similarly assess male rats after 28 d of treatment with the hypolipidemic agent fenofibrate. Diet mixes for these studies were prepared to attain dosage levels of approximately 500 mg/kg . d for probucol, 250 mg/kg . d for clofibrate, and 100 mg/kg . d for fenofibrate. Control rats were given untreated basal ration. Weekly adjustments in dietary concentrations were made in accordance with group mean food consumption and body weight changes. Probucol and clofibrate treatments produced statistically significant reductions in mean serum cholesterol levels of both sexes after 28 and 91 d of dosing. Only male rats were given fenofibrate, and they exhibited statistically significant cholesterol reductions after 28 d. Clofibrate and fenofibrate administration resulted in marked increases in liver weight/body weight ratios. Probucol had no statistically significant effects on liver weight/body weight ratios after 28 and 91 consecutive days of treatment. Light microscopy of liver sections stained with hematoxylin and eosin revealed an abnormal amount of cytoplasmic granularity within hepatocytes from rats given clofibrate and fenofibrate. The granules were identified by electron microscopy and cytochemistry as enlarged, proliferated peroxisomes--a known rat hepatocellular response to treatment with many hypolipidemic drugs. In addition, ultrastructural cytochemistry suggested reduced amounts of catalase in individual peroxisomes after clofibrate and fenofibrate dosing. Liver tissue from rats given probucol showed no abnormal cytoplasmic granularity and, ultrastructurally, no peroxisomal changes. Liver tissues from probucol-treated rats revealed features similar to those encountered in tissues from untreated control animals. It was concluded that the hypocholesterolemic response elicited by probucol treatment does not involve significant changes in rat liver cell morphology.},
	language = {eng},
	number = {3},
	journal = {Journal of toxicology and environmental health},
	author = {Barnard, S D and Molello, J A and Caldwell, W J and LeBeau, J E},
	month = may,
	year = {1980},
	pmid = {7420463},
	keywords = {Female, Male, Animals, Cholesterol, Clofibrate, Liver, Phenols, Probucol, Rats},
	pages = {547--557},
}

@misc{noauthor_scopus_nodate,
	title = {Scopus preview - {Scopus} - {Document} details},
	url = {http://www.scopus.com/record/display.url?eid=2-s2.0-0033402842&origin=inward&txGid=6C8CC60DBD82D2CD19222E9DB3A7408B.fM4vPBipdL1BpirDq5Cw%3a2},
	urldate = {2014-02-19},
	file = {Scopus preview - Scopus - Document details:files/433/display.html:text/html},
}

@book{pena_course_2011,
	title = {A {Course} in {Time} {Series} {Analysis}},
	isbn = {978-1-118-03122-3},
	abstract = {New statistical methods and future directions of research in time series A Course in Time Series Analysis demonstrates how to build time series models for univariate and multivariate time series data. It brings together material previously available only in the professional literature and presents a unified view of the most advanced procedures available for time series model building. The authors begin with basic concepts in univariate time series, providing an up-to-date presentation of ARIMA models, including the Kalman filter, outlier analysis, automatic methods for building ARIMA models, and signal extraction. They then move on to advanced topics, focusing on heteroscedastic models, nonlinear time series models, Bayesian time series analysis, nonparametric time series analysis, and neural networks. Multivariate time series coverage includes presentations on vector ARMA models, cointegration, and multivariate linear systems. Special features include: Contributions from eleven of the world???s leading figures in time series Shared balance between theory and application Exercise series sets Many real data examples Consistent style and clear, common notation in all contributions 60 helpful graphs and tablesRequiring no previous knowledge of the subject, A Course in Time Series Analysis is an important reference and a highly useful resource for researchers and practitioners in statistics, economics, business, engineering, and environmental analysis.An Instructor's Manual presenting detailed solutions to all the problems in the book is available upon request from the Wiley editorial department.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Peña, Daniel and Tiao, George C. and Tsay, Ruey S.},
	month = jan,
	year = {2011},
	keywords = {Mathematics / Probability \& Statistics / Time Series},
}

@article{jimenez_particulate_2011,
	title = {Particulate air pollution and short-term mortality due to specific causes among the elderly in {Madrid} ({Spain}): seasonal differences},
	volume = {21},
	issn = {0960-3123},
	shorttitle = {Particulate air pollution and short-term mortality due to specific causes among the elderly in {Madrid} ({Spain})},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09603123.2011.560251},
	doi = {10.1080/09603123.2011.560251},
	abstract = {A time-series study was conducted to ascertain the short-term effects of different-sized airborne particulate matter (PM) on daily respiratory and cardiovascular cause-specific mortality in winter and summer, among subjects aged over 75 years in Madrid. Poisson regression was used to analyse the time-series, in which the dependent variable was daily mortality due to different specific respiratory and circulatory causes, and the principal independent variables were daily mean PM10, PM2.5 and PM10–2.5 concentrations; other variables: other air pollutants (chemicals, biotic and acoustic), influenza, trend, seasonality and autocorrelation of the series. The results indicated an association between coarser PM fractions (PM10 and PM10–2.5) and respiratory-specific mortality on the one hand, and between PM2.5 and cardiovascular-specific mortality on the other. While the risk of mortality due to exposure to particulate matter was greater in summer than in winter, this difference was statistically significant solely for total organic-cause mortality.},
	number = {5},
	urldate = {2014-01-13},
	journal = {International Journal of Environmental Health Research},
	author = {Jiménez, Eva and Linares, Cristina and Martínez, David and Díaz, Julio},
	year = {2011},
	pages = {372--390},
	file = {Snapshot:files/646/09603123.2011.html:text/html},
}

@misc{noauthor_taylor_nodate,
	title = {Taylor \& {Francis} {Online} :: {Particulate} air pollution and short-term mortality due to specific causes among the elderly in {Madrid} ({Spain}): seasonal differences - {International} {Journal} of {Environmental} {Health} {Research} - {Volume} 21, {Issue} 5},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09603123.2011.560251#.UtO0o_jf1FM},
	urldate = {2014-01-13},
	file = {Taylor & Francis Online \:\: Particulate air pollution and short-term mortality due to specific causes among the elderly in Madrid (Spain)\: seasonal differences - International Journal of Environmental Health Research - Volume 21, Issue 5:files/648/09603123.2011.html:text/html},
}

@misc{noauthor_eet_nodate,
	title = {eet {G}.'s {Carpeta} de 4shared - {SEGURIDAD}},
	url = {http://www.4shared.com/folder/teFzBRdS/SEGURIDAD.html},
	abstract = {Compruebe los archivos 5  en la carpeta de 4shared SEGURIDAD - ciudades - capítulo 1.doc, ciudades - capítulo 2.doc, ciudades - capítulo 3.doc},
	urldate = {2013-10-17},
	file = {Snapshot:files/649/SEGURIDAD.html:text/html},
}

@misc{noauthor_eet_nodate-1,
	title = {eet {G}.'s {Carpeta} de 4shared - {SEGURIDAD}},
	url = {http://www.4shared.com/folder/teFzBRdS/SEGURIDAD.html},
	abstract = {Compruebe los archivos 5  en la carpeta de 4shared SEGURIDAD - ciudades - capítulo 1.doc, ciudades - capítulo 2.doc, ciudades - capítulo 3.doc},
	urldate = {2013-10-17},
	file = {Snapshot:files/650/SEGURIDAD.html:text/html},
}

@misc{noauthor_eet_nodate-2,
	title = {eet {G}.'s {Carpeta} de 4shared - {SEGURIDAD}},
	url = {http://www.4shared.com/folder/teFzBRdS/SEGURIDAD.html},
	abstract = {Compruebe los archivos 5  en la carpeta de 4shared SEGURIDAD - ciudades - capítulo 1.doc, ciudades - capítulo 2.doc, ciudades - capítulo 3.doc},
	urldate = {2013-10-17},
	file = {Snapshot:files/651/SEGURIDAD.html:text/html},
}

@misc{thomas_m._hamill_interpretation_2010,
	type = {research-article},
	title = {Interpretation of {Rank} {Histograms} for {Verifying} {Ensemble} {Forecasts}},
	copyright = {American Meteorological Society},
	url = {http://journals.ametsoc.org/doi/abs/10.1175/1520-0493%282001%29129%3C0550%3AIORHFV%3E2.0.CO%3B2},
	urldate = {2010-08-11},
	author = {Thomas M. Hamill},
	month = feb,
	year = {2010},
	note = {Abstract Rank histograms are a tool for evaluating ensemble forecasts. They are useful for determining the reliability of ensemble forecasts and for diagnosing errors in its mean and spread. Rank histograms are generated by repeatedly tallying the rank of the verification (usually an observation) relative to values from an ensemble sorted from lowest to highest. However, an uncritical use of the rank histogram can lead to misinterpretations of the qualities of that ensemble. For example, a flat rank histogram, usually taken as a sign of reliability, can still be generated from unreliable ensembles. Similarly, a U-shaped rank histogram, commonly understood as indicating a lack of variability in the ensemble, can also be a sign of conditional bias. It is also shown that flat rank histograms can be generated for some model variables if the variance of the ensemble is correctly specified, yet if covariances between model grid points are improperly specified, rank histograms for combinations of model variables...},
	file = {AMS Journals Online - Interpretation of Rank Histograms for Verifying Ensemble Forecasts:files/652/1520-0493(2001)1290550IORHFV2.0.html:text/html},
}

@article{das_skills_2008,
	title = {Skills of different mesoscale models over {Indian} region during monsoon season: {Forecast} errors},
	volume = {117},
	shorttitle = {Skills of different mesoscale models over {Indian} region during monsoon season},
	url = {http://dx.doi.org/10.1007/s12040-008-0056-4},
	doi = {10.1007/s12040-008-0056-4},
	abstract = {Abstract  Performance of four mesoscale models namely, the MM5, ETA, RSM and WRF, run at NCMRWF for short range weather forecasting
has been examined during monsoon-2006. Evaluation is carried out based upon comparisons between observations and day-1 and
day-3 forecasts of wind, temperature, specific humidity, geopotential height, rainfall, systematic errors, root mean square
errors and specific events like the monsoon depressions.

It is very difficult to address the question of which model performs best over the Indian region? An honest answer is ‘none’.
Perhaps an ensemble approach would be the best. However, if we must make a final verdict, it can be stated that in general,
(i) the WRF is able to produce best All India rainfall prediction compared to observations in the day-1 forecast and, the
MM5 is able to produce best All India rainfall forecasts in day-3, but ETA and RSM are able to depict the best distribution
of rainfall maxima along the west coast of India, (ii) the MM5 is able to produce least RMSE of wind and geopotential fields
at most of the time, and (iii) the RSM is able to produce least errors in the day-1 forecasts of the tracks, while the ETA
model produces least errors in the day-3 forecasts.},
	number = {5},
	urldate = {2010-04-08},
	journal = {Journal of Earth System Science},
	author = {Das, Someshwar and Ashrit, Raghavendra and Iyengar, Gopal and Mohandas, Saji and Das Gupta, M. and George, John and Rajagopal, E. and Dutta, Surya},
	month = oct,
	year = {2008},
	pages = {603--620},
	file = {SpringerLink Snapshot:files/668/w3882344721l4310.html:text/html},
}

@book{hulstrom_solar_1989,
	title = {Solar resources},
	isbn = {0-262-08184-9 978-0-262-08184-9},
	publisher = {MIT Press},
	author = {Hulstrom, Roland},
	month = nov,
	year = {1989},
}

@article{sabburg_improved_2004,
	title = {Improved sky imaging for studies of enhanced {UV} irradiance},
	volume = {4},
	url = {http://hal-insu.archives-ouvertes.fr/hal-00301449/},
	abstract = {A recent World Meteorological Organisation report discussed the importance of continued study of the effect of clouds on the solar UV radiation reaching the earth's surface. The report mentions that the use of all-sky imagery offers the potential to understand and quantify cloud effects more accurately. There are an increasing number of studies investigating the enhancement of surface UV irradiance, and UV actinic flux, using automated CCD and sky imagers. This paper describes new algorithms applicable to a commercially available all-sky imager (TSI-440), for research investigating cloud enhanced spectral UV irradiance. Specifically, these include three new algorithms relating to cloud amount at different spatial positions and the visible brightness of clouds surrounding the sun. A possible relationship between UV enhancement and the occurrence of near-sun cloud brightness is reported. It is found that a range of wavelength dependent intensities, from 306 to 400 nm, can occur in one day for UV enhancements. Evidence of a decreasing variation of intensity with longer wavelengths is also presented.},
	number = {5},
	urldate = {2009-11-08},
	journal = {Atmospheric Chemistry and Physics Discussions},
	author = {Sabburg, J. M and Long, C. N},
	month = oct,
	year = {2004},
	keywords = {cloud detection},
	pages = {6213--6238},
	file = {HAL - INSU \:\: [hal-00301449, version 1] Improved sky imaging for studies of enhanced UV irradiance:files/665/hal-00301449.html:text/html;Sabburg2004improved.pdf:files/673/Sabburg2004improved.pdf:application/pdf},
}

@inproceedings{casa_nova_solar_2005,
	address = {Vila Real (Portugal)},
	title = {Solar {Irradiation} {Forecast} {Model} {Using} {Time} {Series} {Analysis} and {Sky} {Images}},
	url = {http://www.efita.net/apps/accesbase/bindocload.asp?d=5941&t=0&identobj=5HFF23pk&uid=57305290&sid=57&idk=1},
	publisher = {European Federation for Information Technology in Agriculture, Food and the Environment},
	author = {Casa Nova, J. and Boaventura Cunha, J. and Oliveira, P.B. de Moura},
	month = jul,
	year = {2005},
	file = {PA352_-_Jorge_Nova.pdf:files/678/PA352_-_Jorge_Nova.pdf:application/pdf},
}

@misc{noauthor_acta_nodate,
	title = {{ACTA} {INFORMATIQUE}},
	url = {http://www.efita.net/apps/accesbase/dbsommaire.asp?d=5910&t=0&identobj=otUqqiLO&uid=57305290&sid=57&idk=1},
	urldate = {2010-02-04},
	file = {ACTA INFORMATIQUE:files/679/dbsommaire.html:text/html},
}

@article{wirth_satellite-based_2010,
	title = {Satellite-based snow identification and its impact on monitoring photovoltaic systems},
	volume = {84},
	issn = {0038-092X},
	url = {http://www.sciencedirect.com/science/article/B6V50-4Y0RWCS-1/2/b7f1e9577b6c1e3c6c94e0fc8860bacc},
	doi = {10.1016/j.solener.2009.10.023},
	abstract = {Earth observation allows the separation of snow cover and cloudiness using multispectral measurements. Several satellite-based snow monitoring services are available, ranging from regional to world-wide scales. Using these data enables photovoltaic (PV) plant management to differentiate between failures due to snow coverage on a PV system and other error sources. Additionally, yield estimates for solar siting are improved. This paper presents a validation study from January to April 2006 comparing satellite-based datasets with ground measurements from German and Swiss meteorological stations. A false alarm rate, an error due to irradiance underestimation, the availability of daily data, and the classification accuracy are introduced as quality metrics. Compared to Switzerland, generally a higher accuracy is found in all datasets for Southern Germany. The most significant difference among the datasets is found in the error pattern shifting from too much snow (which results in an error due to underestimation of irradiance) to too little snow detection, causing a false alarm in PV monitoring.
Overall, the data records of the Land Surface Analysis Satellite Application Facility (LSA SAF), the German Aerospace Center (DLR) and the Interactive Multisensor Snow and Ice Mapping System (IMS) are found to be most suitable for solar energy purposes. The IMS dataset has a low false alarm rate (4\%) and a good data availability (100\%) making it a good choice for power plant monitoring, but the error due to underestimation relevant in site auditing is large with 59\%. If a cumulative snow cover algorithm is applied to achieve information every day as needed both for power plant monitoring and site auditing, both the DLR and the LSA SAF datasets are comparable with classification accuracies of 70\%, false alarm rates of 37\% and 34\%, respectively, and errors due to irradiance underestimation in 26\% and 27\% of all coincidences.},
	number = {2},
	urldate = {2010-02-04},
	journal = {Solar Energy},
	author = {Wirth, Georg and Schroedter-Homscheidt, Marion and Zehner, Mike and Becker, Gerd},
	month = feb,
	year = {2010},
	keywords = {photovoltaic system, Monitoring, Automatic failure detection, Satellite-derived snow monitoring, Snow cover},
	pages = {215--226},
	file = {ScienceDirect Snapshot:files/680/science.html:text/html;Wirth2010satellite.pdf:files/676/Wirth2010satellite.pdf:application/pdf},
}

@misc{noauthor_korea_nodate,
	title = {Korea {Science}},
	url = {http://www.koreascience.or.kr/article/articleresultdetail.jsp?no=34521203&searchtype=JJB&listlen=6&listno=4},
	urldate = {2010-01-24},
	file = {Korea Science:files/684/articleresultdetail.html:text/html},
}

@article{bouzguenda_value_1993,
	title = {Value analysis of intermittent generation sources from the system operations perspective},
	volume = {8},
	issn = {0885-8969},
	doi = {10.1109/60.257063},
	abstract = {The objective of this study is to determine the economic and
operational impact on energy cost of incorporating large photovoltaic
(PV) and wind energy conversion systems (WECS) into the electric utility
generation mix. In most cases, PV and WECS power outputs are subtracted
from the utility load with the expectation that conventional generation
would meet the residual load. This approach is valid for small
penetration levels and/or for PV and WECS facilities connected near load
centers, However, several constraints such as thermal generation
characteristics, fuel supply and delivery, spinning reserve
requirements, and hydro availability are not adequately represented in
this process. To determine the optimal value of large-scale PV and WECS
applications, a new methodology that would take into account the
aforementioned constraints as well as a more global penetration is
developed. Results indicate that while high hydro availability increases
PV penetration levels, high ramping rates can also significantly
increase penetration levels},
	number = {3},
	journal = {Energy Conversion, IEEE Transactions on},
	author = {Bouzguenda, M. and Rahman, S.},
	year = {1993},
	keywords = {photovoltaic power systems, wind power plants, load (electric), wind energy, electricity supply industry, economics, electric utility, fuel delivery, fuel supply, generation mix, load, penetration levels, PV power systems, ramping rates, spinning reserve, thermal generation, value analysis, value engineering},
	pages = {484--490},
}

@article{barbounis_long-term_2006,
	title = {Long-term wind speed and power forecasting using local recurrent neural network models},
	volume = {21},
	issn = {0885-8969},
	doi = {10.1109/TEC.2005.847954},
	abstract = {This paper deals with the problem of long-term wind speed and power forecasting based on meteorological information. Hourly forecasts up to 72-h ahead are produced for a wind park on the Greek island of Crete. As inputs our models use the numerical forecasts of wind speed and direction provided by atmospheric modeling system SKIRON for four nearby positions up to 30 km away from the wind turbine cluster. Three types of local recurrent neural networks are employed as forecasting models, namely, the infinite impulse response multilayer perceptron (IIR-MLP), the local activation feedback multilayer network (LAF-MLN), and the diagonal recurrent neural network (RNN). These networks contain internal feedback paths, with the neuron connections implemented by means of IIR synaptic filters. Two novel and optimal on-line learning schemes are suggested for the update of the recurrent network's weights based on the recursive prediction error algorithm. The methods assure continuous stability of the network during the learning phase and exhibit improved performance compared to the conventional dynamic back propagation. Extensive experimentation is carried out where the three recurrent networks are additionally compared to two static models, a finite-impulse response NN (FIR-NN) and a conventional static-MLP network. Simulation results demonstrate that the recurrent models, trained by the suggested methods, outperform the static ones while they exhibit significant improvement over the persistent method.},
	number = {1},
	journal = {Energy Conversion, IEEE Transactions on},
	author = {Barbounis, T.G. and Theocharis, J.B. and Alexiadis, M.C. and Dokopoulos, P.S.},
	year = {2006},
	keywords = {power engineering computing, load forecasting, wind turbines, atmospheric modeling system, diagonal recurrent neural network, FIR filters, IIR filters, infinite impulse response multilayer perceptron, local activation feedback multilayer network, local recurrent neural network models, Local recurrent neural networks, long-term wind power forecasting, long-term wind speed numerical forecasting, meteorological information, multilayer perceptrons, nonlinear recursive least square learning, optimal online learning schemes, power forecasting, real time learning, recurrent neural nets, recursive prediction error algorithm, turbine cluster, wind park},
	pages = {273--284},
}

@article{alexiadis_wind_1999,
	title = {Wind speed and power forecasting based on spatial correlation models},
	volume = {14},
	issn = {0885-8969},
	doi = {10.1109/60.790962},
	abstract = {Wind energy conversion systems (WECS) cannot be dispatched like
conventional generators. This can pose problems for power system
schedulers and dispatchers, especially if the schedule of wind power
availability is not known in advance. However, if the wind speed can be
reliably forecasted up to several hours ahead, the generating schedule
can efficiently accommodate the wind generation. This paper illustrates
a technique for forecasting wind speed and power output up to several
hours ahead, based on cross correlation at neighboring sites. The
authors develop an artificial neural network (ANN) that significantly
improves forecasting accuracy comparing to the persistence forecasting
model. The method is tested at different sites over a year},
	number = {3},
	journal = {Energy Conversion, IEEE Transactions on},
	author = {Alexiadis, M.C. and Dokopoulos, P.S. and Sahsamanoglou, H.S.},
	year = {1999},
	keywords = {power generation planning, neural nets, artificial neural network, wind power, wind, wind power plants, power system analysis computing, correlation methods, cross correlation, forecasting accuracy, forecasting theory, power generation scheduling, power system dispatchers, power system schedulers, spatial correlation models, wind power availability schedule, wind power forecasting, wind power generation, wind speed forecasting},
	pages = {836--842},
}

@article{kariniotakis_wind_1996,
	title = {Wind power forecasting using advanced neural networks models},
	volume = {11},
	issn = {0885-8969},
	doi = {10.1109/60.556376},
	abstract = {In this paper, an advanced model, based on recurrent high order
neural networks, is developed for the prediction of the power output
profile of a wind park. This model outperforms simple methods like
persistence, as well as classical methods in the literature. The
architecture of a forecasting model is optimised automatically by a new
algorithm, that substitutes the usually applied trial-and-error method.
Finally, the online implementation of the developed model into an
advanced control system for the optimal operation and management of a
real autonomous wind-diesel power system, is presented},
	number = {4},
	journal = {Energy Conversion, IEEE Transactions on},
	author = {Kariniotakis, G.N. and Stavrakakis, G.S. and Nogaret, E.F.},
	year = {1996},
	keywords = {power engineering computing, wind power, wind power plants, recurrent neural nets, wind park, advanced control system, advanced neural networks models, autonomous wind-diesel power system, diesel-electric generators, management, optimal operation, power output profile prediction, power system control, recurrent high order neural networks, short term wind power forecasting, trial-and-error method},
	pages = {762--767},
}

@article{shu_fan_forecasting_2009,
	title = {Forecasting the {Wind} {Generation} {Using} a {Two}-{Stage} {Network} {Based} on {Meteorological} {Information}},
	volume = {24},
	issn = {0885-8969},
	doi = {10.1109/TEC.2008.2001457},
	abstract = {This paper proposes a practical and effective model for the generation forecasting of a wind farm with an emphasis on its scheduling and trading in a wholesale electricity market. A novel forecasting model is developed based on indepth investigations of meteorological information. This model adopts a two-stage hybrid network with Bayesian clustering by dynamics and support vector regression. The proposed structure is robust with different input data types and can deal with the nonstationarity of wind speed and generation series well. Once the network is trained, we can straightforward predict the 48-h ahead wind power generation. To demonstrate the effectiveness, the model is applied and tested on a 74-MW wind farm located in the southwest Oklahoma of the United States.},
	number = {2},
	journal = {Energy Conversion, IEEE Transactions on},
	author = {Shu Fan and Liao, J.R. and Yokoyama, R. and Luonan Chen and Wei-Jen Lee},
	year = {2009},
	keywords = {support vector regression, meteorology, wind power, power markets, wind power plants, regression analysis, wind farm, Bayes methods, meteorological information, forecasting theory, power generation scheduling, Bayesian clustering, hybrid network, Machine learning, nonstationarity, pattern clustering, power 74 MW, two stage network, wholesale electricity market, wind generation forecasting, wind generation forecfasting},
	pages = {474--482},
}

@article{taylor_wind_2009,
	title = {Wind {Power} {Density} {Forecasting} {Using} {Ensemble} {Predictions} and {Time} {Series} {Models}},
	volume = {24},
	issn = {0885-8969},
	doi = {10.1109/TEC.2009.2025431},
	abstract = {Wind power is an increasingly used form of renewable energy. The uncertainty in wind generation is very large due to the inherent variability in wind speed, and this needs to be understood by operators of power systems and wind farms. To assist with the management of this risk, this paper investigates methods for predicting the probability density function of generated wind power from one to ten days ahead at five U.K. wind farm locations. These density forecasts provide a description of the expected future value and the associated uncertainty. We construct density forecasts from weather ensemble predictions, which are a relatively new type of weather forecast generated from atmospheric models. We also consider density forecasting from statistical time series models. The best results for wind power density prediction and point forecasting were produced by an approach that involves calibration and smoothing of the ensemble-based wind power density.},
	number = {3},
	journal = {Energy Conversion, IEEE Transactions on},
	author = {Taylor, J.W. and McSharry, P.E. and Buizza, R.},
	year = {2009},
	keywords = {time series, wind power, wind speed, Density forecasting, wind generation, ensemble predictions, generalized autoregressive conditional heteroskedasticity (GARCH) models, point forecasting, probability density function, renewable energy, time series models, weather ensemble predictions, wind power density forecasting},
	pages = {775--782},
}

@book{barreto_introductory_2005,
	title = {Introductory {Econometrics}: {Using} {Monte} {Carlo} {Simulation} with {Microsoft} {Excel}},
	isbn = {0-521-84319-7},
	shorttitle = {Introductory {Econometrics}},
	publisher = {Cambridge University Press},
	author = {Barreto, Humberto and Howland, Frank},
	month = dec,
	year = {2005},
}

@incollection{noauthor_solar_nodate,
	title = {Solar {Radiation} {Forecasting} {Using} {Ad}-{Hoc} {Time} {Series} {Preprocessing} and {Neural} {Networks}},
}

@misc{noauthor_sciencedirect_nodate,
	title = {{ScienceDirect} - {Renewable} and {Sustainable} {Energy} {Reviews} : {Artificial} neural networks in renewable energy systems applications: a review},
	url = {http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6VMY-43CJV79-3&_user=7299394&_rdoc=1&_fmt=&_orig=search&_sort=d&_docanchor=&view=c&_searchStrId=1110646477&_rerunOrigin=google&_acct=C000047062&_version=1&_urlVersion=0&_userid=7299394&md5=6ce41b5495b136bb6912bd4277fbc701},
	urldate = {2009-11-26},
	file = {ScienceDirect - Renewable and Sustainable Energy Reviews \: Artificial neural networks in renewable energy systems applications\: a review:files/656/science.html:text/html},
}

@book{liu_rational_nodate,
	title = {Rational procedure for predicting the long-term average performance of flat-plate solar-energy collectors with design data for the {US}, its outlying possessions and {Canada}},
	abstract = {None},
	author = {Liu, B. Y.H and Jordan, R. C},
	keywords = {14 SOLAR ENERGY; FLAT PLATE COLLECTORS; PERFORMANCE; CANADA; DESIGN; SOLAR ENERGY CONVERSION; USA; CONVERSION; ENERGY CONVERSION; NORTH AMERICA; SOLAR COLLECTORS},
}

@article{tymvios_comparative_2005,
	title = {Comparative study of {\textbackslash}{textbackslashAAngstr}{\textbackslash}{textbackslashÃ}¶mâ€™s and artificial neural networksâ€™ methodologies in estimating global solar radiation},
	volume = {78},
	number = {6},
	journal = {Solar Energy},
	author = {Tymvios, F. S. and Jacovides, C. P. and Michaelides, S. C. and Scouteli, C.},
	year = {2005},
	keywords = {networks, neural},
	pages = {752â€“762},
}

@misc{noauthor_rise_nodate,
	title = {The {Rise} {Of} {Spring} {Allergies}: {Fact} {Or} {Fiction}? {\textbar} {The} {Rise} {Of} {Spring} {Allergies}: {Fact} {Or} {Fiction}? {\textbar} {ACAAI} {Public} {Website}},
	url = {http://acaai.org/news/rise-spring-allergies-fact-or-fiction},
	urldate = {2017-12-05},
	file = {The Rise Of Spring Allergies\: Fact Or Fiction? | The Rise Of Spring Allergies\: Fact Or Fiction? | ACAAI Public Website:files/950/rise-spring-allergies-fact-or-fiction.html:text/html},
}

@techreport{r._engelen_ecmwf_cams_2016,
	title = {{CAMS} {Service} {Product} {Portfolio}},
	url = {https://atmosphere.copernicus.eu/sites/default/files/repository/SPP%20November%202016_2.pdf},
	number = {CAMS12\_2016\_D12.0.2.7\_2016b\_service\_product\_portfolio},
	institution = {ECMWF},
	author = {{R. Engelen (ECMWF)}},
	month = nov,
	year = {2016},
}

@article{barthel_aggregation_1975,
	title = {Aggregation of blood platelets by adrenaline and its uptake},
	volume = {24},
	issn = {0006-2952},
	language = {eng},
	number = {20},
	journal = {Biochemical Pharmacology},
	author = {Barthel, W. and Markwardt, F.},
	month = oct,
	year = {1975},
	pmid = {20},
	keywords = {Animals, Blood Platelets, Dihydroergotamine, Drug Interactions, Epinephrine, In Vitro Techniques, Lysergic Acid Diethylamide, Platelet Aggregation, Rabbits},
	pages = {1903--1904},
}

@article{fox_migration_1977,
	title = {Migration inhibition produced by sodium periodate oxidation of the macrophage membrane, and reversal by sodium borohydride},
	volume = {6},
	issn = {0300-9475},
	abstract = {Guinea pig peritoneal exudate cells were harvested 3 to 4 days after the intraperitoneal injection of Marcol oil. The washed cells were exposed to various concentrations of sodium periodate in phosphate-buffered saline (PBS) at pH 7.4 for 10 min at +4 degrees C. The cells were then used in the in vitro migration assay, and migration was consistently inhibited at concentrations from 10(-3) to 10(-5) M. The viability of the macrophages was not affected by this treatment. Sodium borohydride (10(-3) to 10(-5) M) in PBS for 10 min at pH 7.4 reversed the periodate effect. Experiments with purified macrophages showed that sodium periodate has a direct effect on macrophage function rather than an indirect effect via the potentiation of migration inhibition factor. In support of this, the in vitro spreading of macrophages on glass substrate for 1 h has been shown to be inhibited. This spreading inhibition can also be reversed by treatment with sodium borohydride. These results provide a new approach to understanding the biological significance and role of macrophage migration inhibition.},
	language = {eng},
	number = {11},
	journal = {Scandinavian Journal of Immunology},
	author = {Fox, R. A. and Fernandez, L. A. and Rajaraman, R.},
	year = {1977},
	pmid = {202016},
	keywords = {Animals, Borohydrides, Cell Membrane, Cell Migration Inhibition, Cell Movement, Cell Survival, Guinea Pigs, Macrophages, Oxidation-Reduction, Periodic Acid},
	pages = {1151--1157},
}

@article{bose_delineation_1975,
	title = {Delineation of the intimate details of the backbone conformation of pyridine nucleotide coenzymes in aqueous solution},
	volume = {66},
	issn = {1090-2104},
	language = {eng},
	number = {4},
	journal = {Biochemical and Biophysical Research Communications},
	author = {Bose, K. S. and Sarma, R. H.},
	month = oct,
	year = {1975},
	pmid = {2},
	keywords = {Temperature, Fourier Analysis, Magnetic Resonance Spectroscopy, Models, Molecular, Molecular Conformation, NAD, NADP, Structure-Activity Relationship},
	pages = {1173--1179},
}

@misc{noauthor_downscaling_nodate,
	title = {Downscaling - {Wikipedia}},
	url = {https://en.wikipedia.org/wiki/Downscaling},
	urldate = {2017-12-14},
	file = {Downscaling - Wikipedia:files/960/Downscaling.html:text/html},
}

@misc{noauthor_statistical_nodate,
	title = {Statistical downscaling and local weather forecast {\textbar} {Santander} {Meteorology} {Group}},
	url = {http://www.meteo.unican.es/en/research/statistical_downscaling},
	urldate = {2017-12-14},
}

@incollection{bebis_pollen_2016,
	address = {Cham},
	title = {Pollen {Grain} {Recognition} {Using} {Deep} {Learning}},
	volume = {10072},
	isbn = {978-3-319-50834-4 978-3-319-50835-1},
	url = {http://link.springer.com/10.1007/978-3-319-50835-1_30},
	urldate = {2018-01-01},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer International Publishing},
	author = {Daood, Amar and Ribeiro, Eraldo and Bush, Mark},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Porikli, Fatih and Skaff, Sandra and Entezari, Alireza and Min, Jianyuan and Iwai, Daisuke and Sadagic, Amela and Scheidegger, Carlos and Isenberg, Tobias},
	year = {2016},
	doi = {10.1007/978-3-319-50835-1_30},
	pages = {321--330},
}

@book{bebis_advances_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Advances in {Visual} {Computing}},
	volume = {10072},
	isbn = {978-3-319-50834-4 978-3-319-50835-1},
	url = {http://link.springer.com/10.1007/978-3-319-50835-1},
	urldate = {2018-01-01},
	publisher = {Springer International Publishing},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Porikli, Fatih and Skaff, Sandra and Entezari, Alireza and Min, Jianyuan and Iwai, Daisuke and Sadagic, Amela and Scheidegger, Carlos and Isenberg, Tobias},
	year = {2016},
	doi = {10.1007/978-3-319-50835-1},
}

@book{aghdam_guide_2017,
	title = {Guide to convolutional neural networks: a practical application to traffic-sign detection and classification},
	isbn = {978-3-319-57550-6},
	shorttitle = {Guide to convolutional neural networks},
	url = {http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=1520793},
	abstract = {This must-read text/reference introduces the fundamental concepts of convolutional neural networks (ConvNets), offering practical guidance on using libraries to implement ConvNets in applications of traffic sign detection and classification. The work presents techniques for optimizing the computational efficiency of ConvNets, as well as visualization techniques to better understand the underlying processes. The proposed models are also thoroughly evaluated from different perspectives, using exploratory and quantitative analysis. Topics and features: Explains the fundamental concepts behind training linear classifiers and feature learning Discusses the wide range of loss functions for training binary and multi-class classifiers Illustrates how to derive ConvNets from fully connected neural networks, and reviews different techniques for evaluating neural networks Presents a practical library for implementing ConvNets, explaining how to use a Python interface for the library to create and assess neural networks Describes two real-world examples of the detection and classification of traffic signs using deep learning methods Examines a range of varied techniques for visualizing neural networks, using a Python interface Provides self-study exercises at the end of each chapter, in addition to a helpful glossary, with relevant Python scripts supplied at an associated website This self-contained guide will benefit those who seek to both understand the theory behind deep learning, and to gain hands-on experience in implementing ConvNets in practice. As no prior background knowledge in the field is required to follow the material, the book is ideal for all students of computer vision and machine learning, and will also be of great interest to practitioners working on autonomous cars and advanced driver assistance systems.},
	language = {English},
	urldate = {2017-12-22},
	author = {Aghdam, Hamed Habibi and Heravi, Elnaz Jahani},
	year = {2017},
	note = {OCLC: 987790957},
}

@inproceedings{corne_accurate_2014,
	title = {Accurate localized short term weather prediction for renewables planning},
	doi = {10.1109/CIASG.2014.7011547},
	abstract = {Short-term prediction of meteorological variables is important for many applications. For example, many `smart grid' planning and control scenarios rely on accurate short term prediction of renewable energy generation, which in turn requires accurate forecasts of wind-speed, cloud-cover, and other such variables. Accurate short-term weather forecasting therefore enables smooth integration of renewables into future intelligent power systems. Weather forecasting at a specific location is currently achieved by numerical weather prediction (NWP), or by statistical models built from local time series data, or by a hybrid of these two methods broadly known as `downscaling'. We introduce a new data-intensive approach to localized short-term weather prediction that relies on harvesting multiple freely available observations and forecasts pertaining to the wider geographic region. Our hypothesis is that NWP-based forecast resources, despite the benefit of a dynamical physics-based model, tend to be only sparsely informed by observation-based inputs at a local level, while statistical downscaling models, though locally well-informed, invariably miss the opportunity to include rich additional data sources concerning the wider local region. By harvesting the data stream of multiple forecasts and observations from the wider local region we expect to achieve better accuracy than available otherwise. We describe the approach and demonstrate results for three locations, focusing on the 1hr-24hrs ahead forecasting of variables crucial for renewables forecasting. This work is part of the ORIGIN EU FP7 project (www.origin-concept.eu) and the weather forecasting approach, used in ORIGIN as input for both demand and renewables prediction, began live operation (initially for three European locations) in October 2014.},
	booktitle = {2014 {IEEE} {Symposium} on {Computational} {Intelligence} {Applications} in {Smart} {Grid} ({CIASG})},
	author = {Corne, D. and Dissanayake, M. and Peacock, A. and Galloway, S. and Owens, E.},
	month = dec,
	year = {2014},
	keywords = {Forecasting, time series, wind power, weather forecasting, Predictive models, regression analysis, statistical models, power system control, accurate localized short term weather prediction, big data, Buildings, cloud-cover forecast, Clouds, data stream, data-intensive approach, downscaling, dynamical physics-based model, feature selection, Frequency selective surfaces, future intelligent power systems, local time series data, nowcasting, numerical weather prediction, NWP-based forecast resources, observation-based inputs, ORIGIN EU FP7 project, power system planning, regression, renewables forecasting, renewables planning, short term renewable energy generation prediction, short-term meteorological variable prediction, short-term weather forecasting, smart grid control scenario, smart grid planning scenario, smart power grids, statistical downscaling models, statistical weather prediction, Weather forecasting, wind-speed forecast},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:files/969/Corne et al. - 2014 - Accurate localized short term weather prediction f.html:text/html},
}

@incollection{lagerstrom_pollen_2015,
	series = {Advances in {Experimental} {Medicine} and {Biology}},
	title = {Pollen {Image} {Classification} {Using} the {Classifynder} {System}: {Algorithm} {Comparison} and a {Case} {Study} on {New} {Zealand} {Honey}},
	isbn = {978-3-319-10983-1 978-3-319-10984-8},
	shorttitle = {Pollen {Image} {Classification} {Using} the {Classifynder} {System}},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-10984-8_12},
	abstract = {We describe an investigation into how Massey University’s Pollen Classifynder can accelerate the understanding of pollen and its role in nature. The Classifynder is an imaging microscopy system that can locate, image and classify slide based pollen samples. Given the laboriousness of purely manual image acquisition and identification it is vital to exploit assistive technologies like the Classifynder to enable acquisition and analysis of pollen samples. It is also vital that we understand the strengths and limitations of automated systems so that they can be used (and improved) to compliment the strengths and weaknesses of human analysts to the greatest extent possible. This article reviews some of our experiences with the Classifynder system and our exploration of alternative classifier models to enhance both accuracy and interpretability. Our experiments in the pollen analysis problem domain have been based on samples from the Australian National University’s pollen reference collection (2,890 grains, 15 species) and images bundled with the Classifynder system (400 grains, 4 species). These samples have been represented using the Classifynder image feature set. We additionally work through a real world case study where we assess the ability of the system to determine the pollen make-up of samples of New Zealand honey. In addition to the Classifynder’s native neural network classifier, we have evaluated linear discriminant, support vector machine, decision tree and random forest classifiers on these data with encouraging results. Our hope is that our findings will help enhance the performance of future releases of the Classifynder and other systems for accelerating the acquisition and analysis of pollen samples.},
	language = {en},
	urldate = {2018-01-15},
	booktitle = {Signal and {Image} {Analysis} for {Biomedical} and {Life} {Sciences}},
	publisher = {Springer, Cham},
	author = {Lagerstrom, Ryan and Holt, Katherine and Arzhaeva, Yulia and Bischof, Leanne and Haberle, Simon and Hopf, Felicitas and Lovell, David},
	year = {2015},
	doi = {10.1007/978-3-319-10984-8_12},
	pages = {207--226},
	file = {Snapshot:files/972/10.html:text/html},
}

@misc{noauthor_automated_nodate,
	title = {automated pollen recognition system - {Shu} {Kong} ({Aimery}) - {UC} {Irvine} - {Computer} {Vision}},
	url = {http://www.ics.uci.edu/~skong2/pollen_BIC.html},
	urldate = {2018-01-15},
	file = {automated pollen recognition system - Shu Kong (Aimery) - UC Irvine - Computer Vision:files/976/pollen_BIC.html:text/html},
}

@article{liu_using_2017,
	title = {Using machine learning to estimate atmospheric {Ambrosia} pollen concentrations in {Tulsa}, {OK}},
	volume = {11},
	issn = {1178-6302},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5392111/},
	doi = {10.1177/1178630217699399},
	abstract = {This article describes an example of using machine learning to estimate the abundance of airborne Ambrosia pollen for Tulsa, OK. Twenty-seven years of historical pollen observations were used. These pollen observations were combined with machine learning and a very complete meteorological and land surface context of 85 variables to estimate the daily Ambrosia abundance. The machine learning algorithms employed were Least Absolute Shrinkage and Selection Operator (LASSO), neural networks, and random forests. The best performance was obtained using random forests. The physical insights provided by the random forest are also discussed.},
	urldate = {2018-01-15},
	journal = {Environmental Health Insights},
	author = {Liu, Xun and Wu, Daji and Zewdie, Gebreab K and Wijerante, Lakitha and Timms, Christopher I and Riley, Alexander and Levetin, Estelle and Lary, David J},
	month = mar,
	year = {2017},
	pmid = {28469446},
	pmcid = {PMC5392111},
	file = {PubMed Central Full Text PDF:files/977/Liu et al. - 2017 - Using machine learning to estimate atmospheric Amb.pdf:application/pdf},
}

@article{rodriguez-damian_automatic_2006,
	title = {Automatic detection and classification of grains of pollen based on shape and texture},
	volume = {36},
	issn = {1094-6977},
	doi = {10.1109/TSMCC.2005.855426},
	abstract = {Palynological data are used in a wide range of applications. Some studies describe the benefits of the development of a computer system to pollinic analysis. The system should involve the detection of the pollen grains on a slice, and their classification. This paper presents a system that realizes both tasks. The latter is based on the combination of shape and texture analysis. In relation to shape parameters, different ways to understand the contours are presented. The resulting system is evaluated for the discrimination of species of the Urticaceae family which are quite similar. The performance achieved is 89\% of correct pollen grain classification},
	number = {4},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Rodriguez-Damian, M. and Cernadas, E. and Formella, A. and Fernandez-Delgado, M. and Sa-Otero, Pilar De},
	month = jul,
	year = {2006},
	keywords = {Geology, Air safety, Application software, botany, computer vision, Food technology, image classification, Image reconstruction, image segmentation, image texture, Numerical analysis, Object recognition, Optical microscopy, palynological data, Pollen classification, pollen grain, pollen grain classification, pollen grain detection, pollinic analysis, Scanning electron microscopy, segmentation, Shape, shape analysis, texture analysis, Urticaceae family},
	pages = {531--542},
	file = {IEEE Xplore Abstract Record:files/979/1643845.html:text/html},
}

@inproceedings{hodgson_progress_2005,
	title = {Progress towards a system for the automatic recognition of pollen using light microscope images},
	doi = {10.1109/ISPA.2005.195387},
	abstract = {This paper is a progress report on a continuing project aimed at the eventual realization of low-cost, automatic systems for the recognition and counting of both ancient and live pollen. A previous paper has reported on the classification of optical microscope images of pollen grains using Gabor transforms and shape described by moment invariants. This work has since been extended by the introduction of a range of additional texture measures including grey-level cooccurrence matrices (GLCM), laws features and a wavelet decomposition. The additional texture measures have improved the pollen recognition rate of the system. This paper primarily reports on the development and evaluation of image capture and segmentation schemes. Further system developments in progress are briefly reported.},
	booktitle = {{ISPA} 2005. {Proceedings} of the 4th {International} {Symposium} on {Image} and {Signal} {Processing} and {Analysis}, 2005.},
	author = {Hodgson, R. M. and Holdaway, C. A. and Zhang, Yongping and Fountain, D. W. and Flenley, J. R.},
	month = sep,
	year = {2005},
	keywords = {matrix algebra, botany, image classification, Optical microscopy, Scanning electron microscopy, Atmosphere, Costs, Gabor transforms, Geologic measurements, grey-level cooccurrence matrices, Image analysis, image recognition, Image recognition, Image segmentation, light microscope images, optical microscope images, optical microscopes, Optical signal processing, pollen automatic recognition, pollen grains, Technology planning, wavelet decomposition, wavelet transforms},
	pages = {76--81},
	file = {IEEE Xplore Abstract Record:files/981/1521266.html:text/html},
}

@article{pozo-banos_image_2012,
	title = {Image {Processing} for {Pollen} {Classification}},
	url = {http://www.intechopen.com/books/biodiversity-enrichment-in-a-diverse-world/image-processing-for-pollen-classification},
	doi = {10.5772/48456},
	abstract = {Authors: Marcos del Pozo-Baños, Jaime R. Ticay-Rivas, Jousé Cabrera-Falcón, Jorge Arroyo, Carlos M. Travieso-González, Luis Sánchez-Chavez, Santiago T. Pérez, Jesús B. Alonso and Melvín Ramírez-Bogantes},
	language = {en},
	urldate = {2018-01-15},
	author = {Pozo-Baños, Marcos del and Ticay-Rivas, Jaime R. and Cabrera-Falcón, Jousé and Arroyo, Jorge and Travieso-González, Carlos M. and Sánchez-Chavez, Luis and Pérez, Santiago T. and Alonso, Jesús B. and Ramírez-Bogantes, Melvín},
	year = {2012},
	file = {Full Text PDF:files/983/Pozo-Baños et al. - 2012 - Image Processing for Pollen Classification.pdf:application/pdf;Snapshot:files/984/image-processing-for-pollen-classification.html:text/html},
}

@inproceedings{daood_pollen_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Pollen {Grain} {Recognition} {Using} {Deep} {Learning}},
	isbn = {978-3-319-50834-4 978-3-319-50835-1},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-50835-1_30},
	doi = {10.1007/978-3-319-50835-1_30},
	abstract = {Pollen identification helps forensic scientists solve elusive crimes, provides data for climate-change modelers, and even hints at potential sites for petroleum exploration. Despite its wide range of applications, most pollen identification is still done by time-consuming visual inspection by well-trained experts. Although partial automation is currently available, automatic pollen identification remains an open problem. Current pollen-classification methods use pre-designed features of texture and contours, which may not be sufficiently distinctive. Instead of using pre-designed features, our pollen-recognition method learns both features and classifier from training data under the deep-learning framework. To further enhance our network’s classification ability, we use transfer learning to leverage knowledge from networks that have been pre-trained on large datasets of images. Our method achieved ≈≈{\textbackslash}approx 94\% classification rate on a dataset of 30 pollen types. These rates are among the highest obtained in this problem.},
	language = {en},
	urldate = {2018-01-15},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer, Cham},
	author = {Daood, Amar and Ribeiro, Eraldo and Bush, Mark},
	month = dec,
	year = {2016},
	pages = {321--330},
	file = {Snapshot:files/986/978-3-319-50835-1_30.html:text/html},
}

@article{fernandez-delgado_improved_2003,
	title = {Improved {Classification} of {Pollen} {Texture} {Images} {Using} {SVM} and {MLP}},
	volume = {2},
	abstract = {Humans are interested in the determination of the geo-graphical origin of honeybee pollen due to its nutritional value and therapeutical benefits. This task is currently be-ing developed in a manual way using images from optical microscopy. We have proposed [1, 2] an automatic system for pollen identification, based on its texture classification using a minimum distance classifier. In the present paper, we explore the use of more sophisticated classifiers to im-prove the classification stage. Specifically, we apply sev-eral well-known classifiers, KNN, Support Vector Machine and Multi-Layer Perceptron, in order to increase the classi-fication rate on this problem.},
	journal = {3rd IASTED International Conference on Visualization, Imaging and Image Processing (VIIP2003)},
	author = {Fernández-Delgado, M and Carrión, Pilar and Cernadas, E and Galvez, J.F},
	month = jan,
	year = {2003},
	file = {Full Text PDF:files/988/Fernández-Delgado et al. - 2003 - Improved Classification of Pollen Texture Images U.pdf:application/pdf},
}

@article{li_towards_2004,
	title = {Towards automation of palynology 2: the use of texture measures and neural network analysis for automated identification of optical images of pollen grains},
	volume = {19},
	issn = {1099-1417},
	shorttitle = {Towards automation of palynology 2},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/jqs.874/abstract},
	doi = {10.1002/jqs.874},
	abstract = {The automation of palynology (the identification and counting of pollen grains and spores) will be a small step for image recognition, but a giant stride for palynology. Here we show the first successful automated identification, with 100\% accuracy, of a realistic number of taxa. The technique used involves a neural network classifier applied to surface texture data from light microscope images. A further significance of the technique is that it could be adapted for the identification of a wide range of biological objects, both microscopic and macroscopic. Copyright © 2004 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {8},
	urldate = {2018-01-15},
	journal = {Journal of Quaternary Science},
	author = {Li, P. and Treloar, W. J. and Flenley, J. R. and Empson, L.},
	month = dec,
	year = {2004},
	keywords = {neural networks, pollen, automation, classification, texture measures},
	pages = {755--762},
	file = {Snapshot:files/992/abstract.html:text/html},
}

@article{treloar_towards_2004,
	title = {Towards automation of palynology 1: analysis of pollen shape and ornamentation using simple geometric measures, derived from scanning electron microscope images},
	volume = {19},
	issn = {1099-1417},
	shorttitle = {Towards automation of palynology 1},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/jqs.871/abstract},
	doi = {10.1002/jqs.871},
	abstract = {This is the first of a series of papers on the theme of automated pollen analysis. The automation of pollen analysis could result in numerous advantages for the reconstruction of past environments, with larger data sets made practical, objectivity and fine resolution sampling. There are also applications in apiculture and medicine. Previous work on the classification of pollen using texture measures has been successful with small numbers of pollen taxa. However, as the number of pollen taxa to be identified increases, more features may be required to achieve a successful classification. This paper describes the use of simple geometric measures to augment the texture measures. The feasibility of this new approach is tested using scanning electron microscope (SEM) images of 12 taxa of fresh pollen taken from reference material collected on Henderson Island, Polynesia. Pollen images were captured directly from a SEM connected to a PC. A threshold grey-level was set and binary images were then generated. Pollen edges were then located and the boundaries were traced using a chain coding system. A number of simple geometric variables were calculated directly from the chain code of the pollen and a variable selection procedure was used to choose the optimal subset to be used for classification. The efficiency of these variables was tested using a leave-one-out classification procedure. The system successfully split the original 12 taxa sample into five sub-samples containing no more than six pollen taxa each. The further subdivision of echinate pollen types was then attempted with a subset of four pollen taxa. A set of difference codes was constructed for a range of displacements along the chain code. From these difference codes probability variables were calculated. A variable selection procedure was again used to choose the optimal subset of probabilities that may be used for classification. The efficiency of these variables was again tested using a leave-one-out classification procedure. The proportion of correctly classified pollen ranged from 81\% to 100\% depending on the subset of variables used. The best set of variables had an overall classification rate averaging at about 95\%. This is comparable with the classification rates from the earlier texture analysis work for other types of pollen. Copyright © 2004 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {8},
	urldate = {2018-01-15},
	journal = {Journal of Quaternary Science},
	author = {Treloar, W. J. and Taylor, G. E. and Flenley, J. R.},
	month = dec,
	year = {2004},
	keywords = {pollen, automation, classification, ornamentation, shape},
	pages = {745--754},
	file = {Snapshot:files/993/abstract.html:text/html},
}

@inproceedings{lozano-vega_analysis_2014,
	series = {{IFIP} {Advances} in {Information} and {Communication} {Technology}},
	title = {Analysis of {Relevant} {Features} for {Pollen} {Classification}},
	isbn = {978-3-662-44653-9 978-3-662-44654-6},
	url = {https://link.springer.com/chapter/10.1007/978-3-662-44654-6_39},
	doi = {10.1007/978-3-662-44654-6_39},
	abstract = {The correct classification of airborne pollen is relevant for medical treatment of allergies, and the regular manual process is costly and time consuming. Aiming at automatic processing, we propose a set of relevant image-based features for the recognition of top allergenic pollen taxa. The foundation of our proposal is the testing and evaluation of features that can properly describe pollen in terms of shape, texture, size and apertures. In this regard, a new flexible aperture detector is incorporated to the tests. The selected set is demonstrated to overcome the intra-class variance and inter-class similarity in a SVM classification scheme with a performance comparable to the state of the art procedures.},
	language = {en},
	urldate = {2018-01-15},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Lozano-Vega, Gildardo and Benezeth, Yannick and Marzani, Franck and Boochs, Frank},
	month = sep,
	year = {2014},
	pages = {395--404},
	file = {Snapshot:files/997/10.html:text/html},
}

@article{chung_general_2015,
	title = {A {General} {Framework} for {Multi}-focal {Image} {Classification} and {Authentication}: {Application} to {Microscope} {Pollen} {Images}},
	shorttitle = {A {General} {Framework} for {Multi}-focal {Image} {Classification} and {Authentication}},
	url = {http://arxiv.org/abs/1503.05786},
	abstract = {In this article, we propose a general framework for multi-focal image classification and authentication, the methodology being demonstrated on microscope pollen images. The framework is meant to be generic and based on a brute force-like approach aimed to be efficient not only on any kind, and any number, of pollen images (regardless of the pollen type), but also on any kind of multi-focal images. All stages of the framework's pipeline are designed to be used in an automatic fashion. First, the optimal focus is selected using the absolute gradient method. Then, pollen grains are extracted using a coarse-to-fine approach involving both clustering and morphological techniques (coarse stage), and a snake-based segmentation (fine stage). Finally, features are extracted and selected using a generalized approach, and their classification is tested with four classifiers: Weighted Neighbor Distance, Neural Network, Decision Tree and Random Forest. The latter method, which has shown the best and more robust classification accuracy results (above 97{\textbackslash}\% for any number of pollen types), is finally used for the authentication stage.},
	urldate = {2018-01-15},
	journal = {arXiv:1503.05786 [cs]},
	author = {Chung, François and Rodríguez, Tomás},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.05786},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1503.05786 PDF:files/1000/Chung and Rodríguez - 2015 - A General Framework for Multi-focal Image Classifi.pdf:application/pdf;arXiv.org Snapshot:files/1001/1503.html:text/html},
}

@article{vlahogianni_short-term_2014,
	series = {Special {Issue} on {Short}-term {Traffic} {Flow} {Forecasting}},
	title = {Short-term traffic forecasting: {Where} we are and where we’re going},
	volume = {43},
	issn = {0968-090X},
	shorttitle = {Short-term traffic forecasting},
	url = {http://www.sciencedirect.com/science/article/pii/S0968090X14000096},
	doi = {10.1016/j.trc.2014.01.005},
	abstract = {Since the early 1980s, short-term traffic forecasting has been an integral part of most Intelligent Transportation Systems (ITS) research and applications; most effort has gone into developing methodologies that can be used to model traffic characteristics and produce anticipated traffic conditions. Existing literature is voluminous, and has largely used single point data from motorways and has employed univariate mathematical models to predict traffic volumes or travel times. Recent developments in technology and the widespread use of powerful computers and mathematical models allow researchers an unprecedented opportunity to expand horizons and direct work in 10 challenging, yet relatively under researched, directions. It is these existing challenges that we review in this paper and offer suggestions for future work.},
	urldate = {2018-01-29},
	journal = {Transportation Research Part C: Emerging Technologies},
	author = {Vlahogianni, Eleni I. and Karlaftis, Matthew G. and Golias, John C.},
	month = jun,
	year = {2014},
	keywords = {Time series analysis, Computational intelligence, Intelligent Transportation Systems, Prediction models, Responsive algorithms, Short-term traffic},
	pages = {3--19},
	file = {ScienceDirect Snapshot:files/1004/S0968090X14000096.html:text/html},
}

@article{chen_analysis_2017,
	title = {Analysis and {Forecast} of {Traffic} {Accident} {Big} {Data}},
	volume = {12},
	copyright = {© The Authors, published by EDP Sciences, 2017},
	issn = {2271-2097},
	url = {https://www.itm-conferences.org/articles/itmconf/abs/2017/04/itmconf_ita2017_04029/itmconf_ita2017_04029.html},
	doi = {10.1051/itmconf/20171204029},
	abstract = {Nowadays, as traffic accidents keep happening, traffic safety has become a major focus of contemporary social issues. Many factors account for traffic accidents, such as accident location, time period, driver’s feelings, weather and other uncertain complex factors. As a result, the occurrence of traffic accidents is nonlinear, so it is necessary to explore the correlation between the data from many different aspects so as to avoid risks. By analyzing traffic data and graphics, R language shows how the data is related. After data preprocess, data selection by using R language Remap package remapB and remapH function, we get the locations of the accidents and the accident thermal chart, where you can find high- frequency accident locations. Besides, we employ decision tree, linear regression, random forest algorithm to model the data. According to the actual results, we can verify the correctness of the model and get the most accurate model and it can help us to predict this model with similar data in the future. The ultimate goal of data analysis is to choose the most accurate model after validating the model, analyzing the characteristics of the data and the relationship between the model and the data.},
	language = {en},
	urldate = {2018-02-01},
	journal = {ITM Web of Conferences},
	author = {Chen, Chen},
	year = {2017},
	pages = {04029},
	file = {Full Text PDF:files/1007/Chen - 2017 - Analysis and Forecast of Traffic Accident Big Data.pdf:application/pdf;Snapshot:files/1008/itmconf_ita2017_04029.html:text/html},
}

@article{garcia-ferrer_forecasting_2006,
	title = {Forecasting traffic accidents using disaggregated data},
	volume = {22},
	issn = {0169-2070},
	url = {http://www.sciencedirect.com/science/article/pii/S0169207005001202},
	doi = {10.1016/j.ijforecast.2005.11.001},
	abstract = {Traffic accidents, measured monthly, present different characteristics when the aggregate is compared to its individual components. When disaggregated data are used, the effects of policy variables, calendar events, and different seasonal behaviors should be clearly understood and their coefficients properly estimated. In this paper, we compare the empirical performance of various models in assessing the effects of policy variables, legal changes, and traffic security campaigns. In addition, aggregated versus disaggregated forecasts of the main accident variables are compared in order to examine the robustness of the forecasting improvement from using disaggregated data. In particular, we test the robustness of this improvement against the specification of the model, information set, type of measure of forecasting accuracy, and forecast year. Overall, we conclude that forecast combinations based on disaggregated models display better performance.},
	number = {2},
	urldate = {2019-01-10},
	journal = {International Journal of Forecasting},
	author = {García-Ferrer, A. and de Juan, A. and Poncela, P.},
	month = apr,
	year = {2006},
	keywords = {Forecast combination, Time series, Accuracy criteria, Disaggregation, Traffic accidents},
	pages = {203--222},
	file = {ScienceDirect Snapshot:files/1013/S0169207005001202.html:text/html},
}

@misc{noauthor_automobile_nodate,
	title = {Automobile {Collision} {Prediction} in {Louisville}, {KY}},
	url = {https://gduer.github.io/Collision-Prediction-in-Louisville-KY/#appendices},
	urldate = {2019-01-10},
	file = {Automobile Collision Prediction in Louisville, KY:files/1014/Collision-Prediction-in-Louisville-KY.html:text/html},
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {Learning} {Analytics} in {Higher} {Education} {Development}: {A} {Roadmap}},
	shorttitle = {({PDF}) {Learning} {Analytics} in {Higher} {Education} {Development}},
	url = {https://www.researchgate.net/publication/318463686_Learning_Analytics_in_Higher_Education_Development_A_Roadmap},
	abstract = {PDF {\textbar} The increase in education data and advance in technology are bringing about enhanced teaching and learning methodology. The emerging field of Learning Analytics (LA) continues to seek ways to improve the different methods of gathering, analysing, managing and presenting...},
	language = {en},
	urldate = {2019-01-24},
	journal = {ResearchGate},
	file = {Snapshot:files/1119/318463686_Learning_Analytics_in_Higher_Education_Development_A_Roadmap.html:text/html},
}

@article{adejo_learning_2017,
	title = {Learning {Analytics} in {Higher} {Education} {Development}: {A} {Roadmap}},
	volume = {8},
	issn = {2222-1735},
	shorttitle = {Learning {Analytics} in {Higher} {Education} {Development}},
	url = {https://eric.ed.gov/?id=EJ1143829},
	abstract = {The increase in education data and advance in technology are bringing about enhanced teaching and learning methodology. The emerging field of Learning Analytics (LA) continues to seek ways to improve the different methods of gathering, analysing, managing and presenting learners' data with the sole aim of using it to improve the student learning experience and the study environment. In this paper, we try to explore the concept and salient features of LA potential in higher education and suggest strategies on how this emerging field can make use of data mining techniques alongside learners' data to produce useful and informed decision making. Using the Technology-Organisation-Human frameworks, the paper investigates the roadmap for successful implementation of LA in higher Educational Institutions.},
	language = {en},
	number = {15},
	urldate = {2019-01-24},
	journal = {Journal of Education and Practice},
	author = {Adejo, Olugbenga and Connolly, Thomas},
	year = {2017},
	keywords = {Change Strategies, Data Collection, Data Processing, Educational Administration, Educational Practices, Evidence Based Practice, Higher Education, Information Utilization, Management Information Systems, Performance Factors, Program Implementation, Student Records, Technology Planning},
	pages = {156--163},
	file = {Full Text PDF:files/1121/Adejo and Connolly - 2017 - Learning Analytics in Higher Education Development.pdf:application/pdf;Snapshot:files/1122/eric.ed.gov.html:text/html},
}

@article{aldowah_educational_2019,
	title = {Educational {Data} {Mining} and {Learning} {Analytics} for 21stcentury higher education: {A} {Review} and {Synthesis}},
	issn = {0736-5853},
	shorttitle = {Educational {Data} {Mining} and {Learning} {Analytics} for 21stcentury higher education},
	url = {http://www.sciencedirect.com/science/article/pii/S0736585318304234},
	doi = {10.1016/j.tele.2019.01.007},
	abstract = {The potential influence of data mining analytics on the students’ learning processes and outcomes has been realized in higher education. Hence, a comprehensive review of educational data mining (EDM) and learning analytics (LA) in higher education was conducted. This review covered the most relevant studies related to four main dimensions: computer-supported learning analytics (CSLA), computer-supported predictive analytics (CSPA), computer-supported behavioral analytics (CSBA), and computer-supported visualization analytics (CSVA) from 2000 till 2017. The relevant EDM and LA techniques were identified and compared across these dimensions. Based on the results of 402 studies, it was found that specific EDM and LA techniques could offer the best means of solving certain learning problems. Applying EDM and LA in higher education can be useful in developing a student-focused strategy and providing the required tools that institutions will be able to use for the purposes of continuous improvement.},
	urldate = {2019-01-24},
	journal = {Telematics and Informatics},
	author = {Aldowah, Hanan and Al-Samarraie, Hosam and Fauzy, Wan Mohamad},
	month = jan,
	year = {2019},
	keywords = {Data analytics, Educational data mining, Higher education, Learning analytics},
	file = {ScienceDirect Snapshot:files/1124/S0736585318304234.html:text/html},
}

@misc{noauthor_caliper_nodate,
	title = {Caliper {Analytics} {\textbar} {IMS} {Global} {Learning} {Consortium}},
	url = {https://www.imsglobal.org/activity/caliper},
	urldate = {2019-01-24},
	file = {Caliper Analytics | IMS Global Learning Consortium:files/1126/caliper.html:text/html},
}

@techreport{marie_bienkowski_enhancing_2012,
	title = {Enhancing {Teaching} and {Learning} {Through} {Educational} {Data} {Mining} and {Learning} {Analytics}: {An} {Issue} {Brief}},
	institution = {U.S. Department of Education, Office of Educational Technology},
	author = {{Marie Bienkowski} and {Mingyu Feng} and {Barbara Means}},
	month = oct,
	year = {2012},
}

@article{gasevic_learning_2016,
	title = {Learning analytics should not promote one size fits all: {The} effects of instructional conditions in predicting academic success},
	volume = {28},
	issn = {1096-7516},
	shorttitle = {Learning analytics should not promote one size fits all},
	url = {http://www.sciencedirect.com/science/article/pii/S1096751615300038},
	doi = {10.1016/j.iheduc.2015.10.002},
	abstract = {This study examined the extent to which instructional conditions influence the prediction of academic success in nine undergraduate courses offered in a blended learning model (n=4134). The study illustrates the differences in predictive power and significant predictors between course-specific models and generalized predictive models. The results suggest that it is imperative for learning analytics research to account for the diverse ways technology is adopted and applied in course-specific contexts. The differences in technology use, especially those related to whether and how learners use the learning management system, require consideration before the log-data can be merged to create a generalized model for predicting academic success. A lack of attention to instructional conditions can lead to an over or under estimation of the effects of LMS features on students' academic success. These findings have broader implications for institutions seeking generalized and portable models for identifying students at risk of academic failure.},
	urldate = {2019-01-24},
	journal = {The Internet and Higher Education},
	author = {Gašević, Dragan and Dawson, Shane and Rogers, Tim and Gasevic, Danijela},
	month = jan,
	year = {2016},
	keywords = {Learning analytics, Instructional conditions, Learning success, Self-regulated learning, Student retention},
	pages = {68--84},
	file = {Full Text:files/1131/Gašević et al. - 2016 - Learning analytics should not promote one size fit.pdf:application/pdf;ScienceDirect Snapshot:files/1130/S1096751615300038.html:text/html},
}

@article{lacave_learning_2018,
	title = {Learning {Analytics} to identify dropout factors of {Computer} {Science} studies through {Bayesian} networks},
	volume = {37},
	issn = {0144-929X},
	url = {https://doi.org/10.1080/0144929X.2018.1485053},
	doi = {10.1080/0144929X.2018.1485053},
	abstract = {Student dropout in Engineering Education is an important problem which has been studied from different perspectives, as well as using different techniques. This manuscript describes the methodology used in order to address this question in the context of learning analytics. Bayesian networks (BNs) have been used as they provide adequate methods for the representation, interpretation and contextualisation of data. The proposed approach is illustrated through a case study about Computer Science (CS) dropout at the University of Castilla-La Mancha (Spain), which is close to 40\%. To that end, several BNs were obtained from a database which contained 383 records representing both academic and social data of the students enrolled in the CS degree during four courses. Then, these probabilistic models were interpreted and evaluated. The results obtained revealed that the best model that fits the data is provided by the K2 algorithm although the great heterogeneity of the data studied did not permit the adjustment of the dropout profile of the student too accurately. Nonetheless, the methodology described here can be taken as a reference for future works.},
	number = {10-11},
	urldate = {2019-01-24},
	journal = {Behaviour \& Information Technology},
	author = {Lacave, Carmen and Molina, Ana I. and Cruz-Lemus, José A.},
	month = nov,
	year = {2018},
	keywords = {Bayesian networks, Computer Science studies dropout, Learning Analytics, student profile},
	pages = {993--1007},
	file = {Snapshot:files/1133/0144929X.2018.html:text/html},
}

@book{lang_handbook_2017,
	title = {Handbook of {Learning} {Analytics} 2017},
	copyright = {the Society for Learning Analytics Research (Creative Commons Atribución 2.5)},
	abstract = {The Handbook of Learning Analytics is designed to meet the needs of a new and growing field. It aims to balance rigor, quality, open access and breadth of appeal and was devised to be an introduction to the current state of research. The Handbook is a snapshot of the field in 2017 and features a range of prominent authors from the learning analytics and educational data mining research communities. The chapters have been peer reviewed by committed members of these fields and are being published with the endorsement of both the Society for Learning Analytics Research and the International Society for Educational Data Mining. We hope you will find the Handbook of Learning Analytics a useful and informative resource.},
	language = {Inglés},
	publisher = {Society for Learning Analytics Research},
	author = {Lang, Charles},
	month = jun,
	year = {2017},
}

@article{ullmann_visualisation_2019,
	title = {A {Visualisation} {Dashboard} for {Contested} {Collective} {Intelligence} {Learning} {Analytics} to {Improve} {Sensemaking} of {Group} {Discussion}},
	volume = {22},
	copyright = {Copyright (c) 2018 RIED. Revista Iberoamericana de Educación a Distancia},
	issn = {1390-3306},
	url = {http://revistas.uned.es/index.php/ried/article/view/22294},
	doi = {10.5944/ried.22.1.22294},
	abstract = {La habilidad para participar y contribuir a los debates es importante para el aprendizaje informal y formal. Especialmente cuando se abordan temas altamente complejos, puede ser difícil apoyar a los alumnos que participan en una discusión grupal efectiva y mantenerse al tanto de toda la información generada colectivamente durante la discusión. La tecnología puede ayudar con el compromiso y razonamiento en debates tan grandes, por ejemplo, puede monitorear cuán saludable es un debate y proporcionar indicadores sobre la distribución de la participación. Un marco especial que pretende aprovechar la inteligencia de grupos de pequeños a muy grandes con el apoyo de herramientas de discurso y argumentación estructuradas es la Inteligencia Colectiva Controvertida (ICC). Las herramientas de CCI proporcionan una fuente rica de datos semánticos que, si se procesan de manera adecuada, pueden generar un sofisticado análisis del discurso en línea. Este estudio presenta un panel de visualización con varios análisis visuales que muestran aspectos importantes de los debates en línea que han sido facilitados por las herramientas de discusión de CCI. El tablero de instrumentos fue diseñado para mejorar la creación de sentidos y la participación en los debates en línea y se ha evaluado con dos estudios, un experimento de laboratorio y un estudio de campo, en el contexto de dos institutos de educación superior. Este artículo informa sobre los resultados de una evaluación de usabilidad del panel de visualización. Los hallazgos descriptivos sugieren que los participantes con poca experiencia en el uso de visualizaciones analíticas pudieron desempeñarse bien en determinadas tareas. Esto constituye un resultado prometedor para la aplicación de tales tecnologías de visualización, ya que las interfaces analíticas de aprendizaje centradas en el discurso pueden ayudar a apoyar el compromiso de los alumnos y su razonamiento en debates en línea complejos.},
	language = {en},
	number = {1},
	urldate = {2019-01-24},
	journal = {RIED. Revista Iberoamericana de Educación a Distancia},
	author = {Ullmann, Thomas Daniel and Liddo, Anna De and Bachler, Michelle},
	month = jan,
	year = {2019},
	keywords = {Learning analytics, análisis de aprendizaje, Análisis de aprendizaje, argumentación, argumentation, collective intelligence, dashboard., deliberación en línea, discusión en línea, information visualisations, inteligencia colectiva, learning analytics, online deliberation, online discussion, razonamiento, sensemaking, tablero de instrumentos., visualización de información},
	pages = {41--80},
	file = {Full Text PDF:files/1138/Ullmann et al. - 2019 - A Visualisation Dashboard for Contested Collective.pdf:application/pdf;Snapshot:files/1139/22294.html:text/html},
}

@inproceedings{alharbi_using_2016,
	title = {Using data mining techniques to predict students at risk of poor performance},
	doi = {10.1109/SAI.2016.7556030},
	abstract = {The achievement of good honours in Undergraduate degrees is important in the context of Higher Education (HE), both for students and for the institutions that host them. In this paper, we look at whether data mining can be used to highlight performance problems early on and propose remedial actions. Furthermore, some of the methods may also form the basis for recommender systems that may guide students towards their module choices to increase their chances of a good outcome. We use data collected through the admission process and through the students' degrees. In this paper, we predict good honours outcomes based on data at admission and on the first year module results. To validate the proposed results, we evaluate data relating to students with different characteristics from different schools. The analysis is achieved by using historical data from the Data Warehouse of a specific University. The methods used, however, are fairly general and can be used in any HE institution. Our results highlight groups of students at considerable risk of obtaining poor outcomes. For example, using admissions and first year module performance data we can isolate groups for one of the studied schools in which only 24\% of students achieve good honour degrees. Over 67\% of all low achievers in the school can be identified within this group.},
	booktitle = {2016 {SAI} {Computing} {Conference} ({SAI})},
	author = {Alharbi, Z. and Cornford, J. and Dolder, L. and Iglesia, B. De La},
	month = jul,
	year = {2016},
	keywords = {Data Mining, Predictive models, Classification, Decision trees, data mining, Data mining, data mining techniques, Data models, data warehouse, Education, educational administrative data processing, first year module, further education, HE, higher education, historical data, Performance Prediction, poor performance, predict students, Prediction algorithms, Recommender System, recommender systems, Recommender systems, risk analysis, undergraduate degrees},
	pages = {523--531},
	file = {IEEE Xplore Abstract Record:files/1141/7556030.html:text/html;Submitted Version:files/1142/Alharbi et al. - 2016 - Using data mining techniques to predict students a.pdf:application/pdf},
}

@phdthesis{zahyah_alharbi_analytics_2018,
	title = {Analytics and {Information} {Management} in {Higher} {Education}},
	school = {University of East Anglia},
	author = {{Zahyah Alharbi}},
	month = dec,
	year = {2018},
	file = {Zahyah Alharbi - 2018 - Analytics and Information Management in Higher Edu.pdf:files/1144/Zahyah Alharbi - 2018 - Analytics and Information Management in Higher Edu.pdf:application/pdf},
}

@article{mohamad_educational_2013,
	series = {The 9th {International} {Conference} on {Cognitive} {Science}},
	title = {Educational {Data} {Mining}: {A} {Review}},
	volume = {97},
	issn = {1877-0428},
	shorttitle = {Educational {Data} {Mining}},
	url = {http://www.sciencedirect.com/science/article/pii/S1877042813036859},
	doi = {10.1016/j.sbspro.2013.10.240},
	abstract = {Data Mining is very useful in the field of education especially when examining students’ learning behavior in online learning environment. This is due to the potential of data mining in analyzing and uncovering the hidden information of the data itself which is hard and very time consuming if to be done manually. The purpose of this review is to look into how the data mining was tackled by previous scholars and the latest trends on data mining in educational research. Several limitations of existing research are discussed and some directions for future research are suggested.},
	urldate = {2019-01-25},
	journal = {Procedia - Social and Behavioral Sciences},
	author = {Mohamad, Siti Khadijah and Tasir, Zaidatun},
	month = nov,
	year = {2013},
	keywords = {Educational data mining, Data mining, Algorithm, Elearning, Online interaction},
	pages = {320--324},
	file = {ScienceDirect Full Text PDF:files/1147/Mohamad and Tasir - 2013 - Educational Data Mining A Review.pdf:application/pdf;ScienceDirect Snapshot:files/1146/S1877042813036859.html:text/html},
}

@article{pena-ayala_educational_2014,
	title = {Educational data mining: {A} survey and a data mining-based analysis of recent works},
	volume = {41},
	issn = {0957-4174},
	shorttitle = {Educational data mining},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417413006635},
	doi = {10.1016/j.eswa.2013.08.042},
	abstract = {This review pursues a twofold goal, the first is to preserve and enhance the chronicles of recent educational data mining (EDM) advances development; the second is to organize, analyze, and discuss the content of the review based on the outcomes produced by a data mining (DM) approach. Thus, as result of the selection and analysis of 240 EDM works, an EDM work profile was compiled to describe 222 EDM approaches and 18 tools. A profile of the EDM works was organized as a raw data base, which was transformed into an ad-hoc data base suitable to be mined. As result of the execution of statistical and clustering processes, a set of educational functionalities was found, a realistic pattern of EDM approaches was discovered, and two patterns of value-instances to depict EDM approaches based on descriptive and predictive models were identified. One key finding is: most of the EDM approaches are ground on a basic set composed by three kinds of educational systems, disciplines, tasks, methods, and algorithms each. The review concludes with a snapshot of the surveyed EDM works, and provides an analysis of the EDM strengths, weakness, opportunities, and threats, whose factors represent, in a sense, future work to be fulfilled.},
	number = {4, Part 1},
	urldate = {2019-01-25},
	journal = {Expert Systems with Applications},
	author = {Peña-Ayala, Alejandro},
	month = mar,
	year = {2014},
	keywords = {Educational data mining, Data mining, Data mining profile, Educational data mining approach pattern, Pattern for descriptive and predictive educational data mining approaches},
	pages = {1432--1462},
	file = {ScienceDirect Snapshot:files/1149/S0957417413006635.html:text/html},
}

@article{romero_educational_2010,
	title = {Educational {Data} {Mining}: {A} {Review} of the {State} of the {Art}},
	volume = {40},
	issn = {1094-6977},
	shorttitle = {Educational {Data} {Mining}},
	doi = {10.1109/TSMCC.2010.2053532},
	abstract = {Educational data mining (EDM) is an emerging interdisciplinary research area that deals with the development of methods to explore data originating in an educational context. EDM uses computational approaches to analyze educational data in order to study educational questions. This paper surveys the most relevant studies carried out in this field to date. First, it introduces EDM and describes the different groups of user, types of educational environments, and the data they provide. It then goes on to list the most typical/common tasks in the educational environment that have been resolved through data-mining techniques, and finally, some of the most promising future lines of research are discussed.},
	number = {6},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Romero, C. and Ventura, S.},
	month = nov,
	year = {2010},
	keywords = {data mining, Data mining, educational administrative data processing, Data analysis, Data mining (DM), Databases, Delta modulation, educational context, educational data, educational data mining, educational data mining (EDM), educational environment, educational systems, Electronic learning, Gold, Instruments, Internet, knowledge discovery, Least squares approximation, Psychometric testing},
	pages = {601--618},
	file = {IEEE Xplore Abstract Record:files/1151/5524021.html:text/html},
}

@article{romero_data_2013,
	title = {Data {Mining} in {Education}},
	volume = {3},
	issn = {1942-4787},
	url = {http://dx.doi.org/10.1002/widm.1075},
	doi = {10.1002/widm.1075},
	abstract = {Applying data mining DM in education is an emerging interdisciplinary research field also known as educational data mining EDM. It is concerned with developing methods for exploring the unique types of data that come from educational environments. Its goal is to better understand how students learn and identify the settings in which they learn to improve educational outcomes and to gain insights into and explain educational phenomena. Educational information systems can store a huge amount of potential data from multiple sources coming in different formats and at different granularity levels. Each particular educational problem has a specific objective with special characteristics that require a different treatment of the mining problem. The issues mean that traditional DM techniques cannot be applied directly to these types of data and problems. As a consequence, the knowledge discovery process has to be adapted and some specific DM techniques are needed. This paper introduces and reviews key milestones and the current state of affairs in the field of EDM, together with specific applications, tools, and future insights. © 2012 Wiley Periodicals, Inc.},
	number = {1},
	urldate = {2019-01-25},
	journal = {Wiley Int. Rev. Data Min. and Knowl. Disc.},
	author = {Romero, Cristobal and Ventura, Sebastian},
	month = jan,
	year = {2013},
	pages = {12--27},
}

@article{baker_data_2010,
	title = {Data mining for education},
	volume = {7},
	number = {3},
	journal = {International encyclopedia of education},
	author = {Baker, RSJD},
	year = {2010},
	pages = {112--118},
	file = {Full Text:files/1154/Baker - 2010 - Data mining for education.pdf:application/pdf},
}

@article{roberts_give_2017,
	title = {Give {Me} a {Customizable} {Dashboard}: {Personalized} {Learning} {Analytics} {Dashboards} in {Higher} {Education}},
	volume = {22},
	issn = {2211-1670},
	shorttitle = {Give {Me} a {Customizable} {Dashboard}},
	url = {https://doi.org/10.1007/s10758-017-9316-1},
	doi = {10.1007/s10758-017-9316-1},
	abstract = {With the increased capability of learning analytics in higher education, more institutions are developing or implementing student dashboards. Despite the emergence of dashboards as an easy way to present data to students, students have had limited involvement in the dashboard development process. As part of a larger program of research examining student and academic perceptions of learning analytics, we report here on work in progress exploring student perceptions of dashboards and student preferences for dashboard features. First, we present findings on higher education students’ attitudes towards learning analytic dashboards resulting from four focus groups (N = 41). Thematic analysis of the focus group transcripts identified five key themes relating to dashboards: ‘provide everyone with the same learning opportunities’, ‘to compare or not to compare’, ‘dashboard privacy’, ‘automate alerts’ and ‘make it meaningful—give me a customizable dashboard’. Next we present findings from a content analysis of students’ drawings of dashboards demonstrating that students are interested in features that support learning opportunities, provide comparisons to peers and are meaningful to the student. Finally, we present preliminary findings from a survey of higher education students, reinforcing students’ desire to choose whether to have a dashboard and to be able to customize their dashboards. These findings highlight the potential for providing students with some level of control over learning analytics as a means to increasing self-regulated learning and academic achievement. Future research directions aimed at better understanding students emotional and behavioral responses to learning analytics feedback on dashboards and alerts are outlined.},
	language = {en},
	number = {3},
	urldate = {2019-03-19},
	journal = {Technology, Knowledge and Learning},
	author = {Roberts, Lynne D. and Howell, Joel A. and Seaman, Kristen},
	month = oct,
	year = {2017},
	keywords = {Big data, Higher education, Learning analytics, Student attitudes, Student dashboards},
	pages = {317--333},
}

@incollection{roberts_ethical_2017,
	address = {Cham},
	title = {Ethical {Considerations} in {Adopting} a {University}- and {System}-{Wide} {Approach} to {Data} and {Learning} {Analytics}},
	isbn = {978-3-319-06520-5},
	url = {https://doi.org/10.1007/978-3-319-06520-5_7},
	abstract = {The rapid adoption of learning analytics in the higher education sector has not been matched by ethical considerations surrounding their use, with ethical issues now slated as one of the major concerns facing learning analytics. Further, adoption of learning analytics within universities has typically involved small-scale projects rather than university- or system-wide approaches, and missing from the research literature is consideration of learning analytics from a ‘big systems’ point of view. We begin to address these gaps through providing an introduction to ethical considerations in adopting a university- and system-wide approach to learning analytics. Drawing on the existing literature on ethical considerations associated with learning analytics, we identify key questions that require consideration during the process of introducing learning analytics within a university. We then map these questions onto layers of systems and roles within universities, detailing how these ethical considerations may affect learning analytics decisions at differing levels of the university.},
	language = {en},
	urldate = {2019-03-19},
	booktitle = {Big {Data} and {Learning} {Analytics} in {Higher} {Education}: {Current} {Theory} and {Practice}},
	publisher = {Springer International Publishing},
	author = {Roberts, Lynne D. and Chang, Vanessa and Gibson, David},
	editor = {Kei Daniel, Ben},
	year = {2017},
	doi = {10.1007/978-3-319-06520-5_7},
	keywords = {Big data, Learning analytics, Consent, Data governance, Ethical considerations, Privacy, Student agency},
	pages = {89--108},
}

@book{blake_marriage_1790,
	title = {The {Marriage} of {Heaven} and {Hell}},
	shorttitle = {The {Marriage} of {Heaven} and {Hell}},
	language = {en},
	author = {Blake, William},
	year = {1790},
	note = {Google-Books-ID: YUa8AQAAQBAJ},
	keywords = {Art / Individual Artists / Artists' Books},
}

@book{kitchin_data_2014,
	title = {The {Data} {Revolution}: {Big} {Data}, {Open} {Data}, {Data} {Infrastructures} \& {Their} {Consequences}},
	isbn = {978-1-4462-8748-4 978-1-4739-0947-2},
	shorttitle = {The {Data} {Revolution}},
	url = {http://methods.sagepub.com/book/the-data-revolution},
	urldate = {2019-03-19},
	publisher = {SAGE Publications Ltd},
	author = {Kitchin, Rob},
	year = {2014},
	doi = {10.4135/9781473909472},
}

@book{selwyn_distrusting_2013,
	title = {Distrusting educational technology: {Critical} questions for changing times},
	shorttitle = {Distrusting educational technology},
	publisher = {Routledge},
	author = {Selwyn, Neil},
	year = {2013},
	file = {Snapshot:files/1201/9781134607693.html:text/html},
}

@incollection{prinsloo_big_2017,
	address = {Cham},
	title = {Big {Data}, {Higher} {Education} and {Learning} {Analytics}: {Beyond} {Justice}, {Towards} an {Ethics} of {Care}},
	isbn = {978-3-319-06520-5},
	shorttitle = {Big {Data}, {Higher} {Education} and {Learning} {Analytics}},
	url = {https://doi.org/10.1007/978-3-319-06520-5_8},
	abstract = {There is no doubt that Big Data in higher education offers huge potential. However, there is a critical need to interrogate the underlying epistemologies and paradigms which inform our understanding of the potential of learning analytics to increase student engagement, retention and success. The harvesting, analyses and application of student data are not neutral acts, and all flow from and perpetuate social, political, economic and cultural agendas. Therefore, it is crucial to explicitly recognise and engage with the complications, contradictions and conflicts inherent in Big Data and learning analytics. The context of increasing funding constraints, the impact of neoliberal and market-driven curricula and admission requirements and the proliferation of accountability and reporting regimes encourage higher education institutions to embrace the harvesting, analysis and use of student data without necessarily considering issues of justice and ethics. Considering higher education as a moral and political practice, this chapter proposes to formulate a framework for information justice based on an ethics of justice and care. The inherent tensions between an ethics of justice and an ethics of care allow for and necessitate a critical engagement with the hype surrounding Big Data in higher education.},
	language = {en},
	urldate = {2019-03-19},
	booktitle = {Big {Data} and {Learning} {Analytics} in {Higher} {Education}: {Current} {Theory} and {Practice}},
	publisher = {Springer International Publishing},
	author = {Prinsloo, Paul and Slade, Sharon},
	editor = {Kei Daniel, Ben},
	year = {2017},
	doi = {10.1007/978-3-319-06520-5_8},
	keywords = {Big Data, Higher education, Learning analytics, Ethics of care, Ethics of justice, Information justice},
	pages = {109--124},
}

@inproceedings{drachsler_privacy_2016,
	address = {New York, NY, USA},
	series = {{LAK} '16},
	title = {Privacy and {Analytics}: {It}'s a {DELICATE} {Issue} a {Checklist} for {Trusted} {Learning} {Analytics}},
	isbn = {978-1-4503-4190-5},
	shorttitle = {Privacy and {Analytics}},
	url = {http://doi.acm.org/10.1145/2883851.2883893},
	doi = {10.1145/2883851.2883893},
	abstract = {The widespread adoption of Learning Analytics (LA) and Educational Data Mining (EDM) has somewhat stagnated recently, and in some prominent cases even been reversed following concerns by governments, stakeholders and civil rights groups about privacy and ethics applied to the handling of personal data. In this ongoing discussion, fears and realities are often indistinguishably mixed up, leading to an atmosphere of uncertainty among potential beneficiaries of Learning Analytics, as well as hesitations among institutional managers who aim to innovate their institution's learning support by implementing data and analytics with a view on improving student success. In this paper, we try to get to the heart of the matter, by analysing the most common views and the propositions made by the LA community to solve them. We conclude the paper with an eight-point checklist named DELICATE that can be applied by researchers, policy makers and institutional managers to facilitate a trusted implementation of Learning Analytics.},
	urldate = {2019-03-19},
	booktitle = {Proceedings of the {Sixth} {International} {Conference} on {Learning} {Analytics} \& {Knowledge}},
	publisher = {ACM},
	author = {Drachsler, Hendrik and Greller, Wolfgang},
	year = {2016},
	note = {event-place: Edinburgh, United Kingdom},
	keywords = {ethics, learning analytics, educational data mining, data management, implementation, legal aspects, privacy, trust},
	pages = {89--98},
}

@misc{paul_bailey_and_niall_sclatter_code_nodate,
	title = {Code of practice for learning analytics},
	url = {https://www.jisc.ac.uk/guides/code-of-practice-for-learning-analytics},
	abstract = {Setting out the responsibilities of educational institutions to ensure that learning analytics is carried out responsibly, appropriately and effectively.},
	language = {en-GB},
	urldate = {2019-03-19},
	journal = {Jisc},
	author = {{Paul Bailey and Niall Sclatter}},
	file = {Snapshot:files/1205/code-of-practice-for-learning-analytics.html:text/html},
}

@misc{noauthor_ethical_nodate,
	title = {Ethical use of {Student} {Data} for {Learning} {Analytics} -- {Student} {Policies} and {Regulations} -- {Open} {University}},
	url = {https://help.open.ac.uk/documents/policies/ethical-use-of-student-data},
	urldate = {2019-03-19},
	file = {Student Policies and Regulations - Open University:files/1209/ethical-use-of-student-data.html:text/html},
}

@misc{robert_farrow_ethical_nodate,
	title = {Ethical principles of learning analytics – mini critique},
	url = {https://philosopher1978.wordpress.com/2015/12/03/ethical-principles-of-learning-analytics-mini-critique/},
	urldate = {2019-03-19},
	author = {{Robert Farrow}},
	file = {Ethical principles of learning analytics – mini critique – Dr. Robert Farrow:files/1211/ethical-principles-of-learning-analytics-mini-critique.html:text/html},
}

@techreport{sharon_slade_and_alan_tait_global_2019,
	title = {Global guidelines: {Ethics} in {Learning} {Analytics}},
	url = {https://icde.memberclicks.net/assets/Global%20guidelines%20for%20Ethics%20in%20Learning%20Analytics%20Web%20ready%20March%202019.pdf},
	institution = {International Council for Open and Distance Education (ICDE)},
	author = {{Sharon Slade and Alan Tait}},
	month = mar,
	year = {2019},
	note = {https://icde.memberclicks.net/assets/Global\%20guidelines\%20for\%20Ethics\%20in\%20Learning\%20Analytics\%20Web\%20ready\%20March\%202019.pdf},
}

@misc{singer_inbloom_2014,
	title = {{InBloom} {Student} {Data} {Repository} to {Close}},
	url = {https://bits.blogs.nytimes.com/2014/04/21/inbloom-student-data-repository-to-close/},
	abstract = {The student data warehousing venture that became a lightning rod for some parents’ data privacy and security concerns, announced it would close.},
	language = {en-US},
	urldate = {2019-03-19},
	journal = {Bits Blog},
	author = {Singer, Natasha},
	month = apr,
	year = {2014},
	keywords = {Privacy, Children and Tech, Education Technology},
}

@misc{noauthor_inbloom_nodate,
	title = {{inBloom}},
	url = {https://github.com/inbloom},
	abstract = {inBloom has 23 repositories available. Follow their code on GitHub.},
	language = {en},
	urldate = {2019-03-19},
	journal = {GitHub},
	file = {Snapshot:files/1215/inbloom.html:text/html},
}

@misc{noauthor_parent_nodate,
	title = {Parent {Coalition} for {Student} {Privacy}},
	url = {https://www.studentprivacymatters.org/},
	language = {en-US},
	urldate = {2019-03-19},
	journal = {Parent Coalition for Student Privacy},
	file = {Snapshot:files/1217/www.studentprivacymatters.org.html:text/html},
}

@article{steiner_lea_2016,
	title = {{LEA} in {Private}: {A} {Privacy} and {Data} {Protection} {Framework} for a {Learning} {Analytics} {Toolbox}},
	volume = {3},
	copyright = {Copyright (c) 2016 Journal of Learning Analytics},
	issn = {1929-7750},
	shorttitle = {{LEA} in {Private}},
	url = {https://learning-analytics.info/journals/index.php/JLA/article/view/4588},
	doi = {10.18608/jla.2016.31.5},
	abstract = {To find a balance between learning analytics research and individual privacy learning analytics initiatives need to appropriately address ethical, privacy and data protection issues and comply with relevant legal regulations. A range of general guidelines, model codes, and principles for handling ethical issues and for appropriate data and privacy protection exist, which may serve the consideration of these topics in a learning analytics context. The importance and significance of data security and protection are also reflected in national and international laws and directives, where data protection is usually considered as a fundamental right. Existing guidelines, approaches and relevant regulations served as a basis for elaborating a comprehensive privacy and data protection framework for the LEA’s BOX project. It comprises a set of eight principles to derive implications for ensuring an ethical treatment of personal data in a learning analytics platform and its services. The privacy and data protection policy set out in the framework is suitable to be used as best practice for other learning analytics projects.},
	language = {en},
	number = {1},
	urldate = {2019-03-19},
	journal = {Journal of Learning Analytics},
	author = {Steiner, Christina M. and Kickmeier-Rust, Michael D. and Albert, Dietrich},
	month = apr,
	year = {2016},
	keywords = {ethics, Learning analytics, privacy, data protection},
	pages = {66--90--66--90},
	file = {Full Text PDF:files/1219/Steiner et al. - 2016 - LEA in Private A Privacy and Data Protection Fram.pdf:application/pdf;Snapshot:files/1220/4588.html:text/html},
}

@misc{noauthor_responsible_2014,
	type = {Text},
	title = {Responsible research \& innovation},
	url = {https://ec.europa.eu/programmes/horizon2020/en/h2020-section/responsible-research-innovation},
	abstract = {Responsible research \& innovation},
	language = {en},
	urldate = {2019-03-19},
	journal = {Horizon 2020 - European Commission},
	month = apr,
	year = {2014},
	file = {Snapshot:files/1222/responsible-research-innovation.html:text/html},
}

@article{linan_educational_2015,
	title = {Educational {Data} {Mining} and {Learning} {Analytics}: differences, similarities, and time evolution},
	volume = {12},
	copyright = {Copyright (c) 2015 FUOC},
	issn = {1698-580X},
	shorttitle = {Educational {Data} {Mining} and {Learning} {Analytics}},
	url = {../v12n3-calvet-juan.html},
	abstract = {Technological progress in recent decades has enabled people to learn in different ways. Universities now have more educational models to choose from, i.e., b-learning and e-learning. Despite the increasing opportunities for students and instructors, online learning also brings challenges due to the absence of direct human contact. Online environments allow the generation of large amounts of data related to learning/teaching processes, which offers the possibility of extracting valuable information that may be employed to improve students’ performance. In this paper, we aim to review the similarities and differences between Educational Data Mining and Learning Analytics, two relatively new and increasingly popular fields of research concerned with the collection, analysis, and interpretation of educational data. Their origins, goals, differences, similarities, time evolution, and challenges are addressed, as are their relationship with Big Data and MOOCs.},
	language = {en},
	number = {3},
	urldate = {2019-03-26},
	journal = {RUSC. Universities and Knowledge Society Journal},
	author = {Liñán, Laura Calvet and Pérez, Ángel Alejandro Juan},
	month = jul,
	year = {2015},
	pages = {98--112},
	file = {Full Text PDF:files/1227/Liñán and Pérez - 2015 - Educational Data Mining and Learning Analytics di.pdf:application/pdf;Snapshot:files/1233/2746.html:text/html},
}

@techreport{bienkowski_enhancing_2012,
	title = {Enhancing teaching and learning through educational data mining and learning analytics: {An} issue brief},
	shorttitle = {Enhancing teaching and learning through educational data mining and learning analytics},
	url = {https://tech.ed.gov/wp-content/uploads/2014/03/edm-la-brief.pdf},
	institution = {US Department of Education, Office of Educational Technology},
	author = {Bienkowski, Marie and Feng, Mingyu and Means, Barbara},
	year = {2012},
	pages = {1--57},
	file = {Snapshot:files/1235/scholar.html:text/html},
}

@article{floridi_what_2016,
	title = {What is {Data} {Ethics}?},
	volume = {374},
	number = {2083},
	journal = {Philosophical Transactions of the Royal Society A},
	author = {Floridi, Luciano and Taddeo, Mariarosaria},
	year = {2016},
}

@inproceedings{baker_contextual_2010,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Contextual {Slip} and {Prediction} of {Student} {Performance} after {Use} of an {Intelligent} {Tutor}},
	isbn = {978-3-642-13470-8},
	abstract = {Intelligent tutoring systems that utilize Bayesian Knowledge Tracing have achieved the ability to accurately predict student performance not only within the intelligent tutoring system, but on paper post-tests outside of the system. Recent work has suggested that contextual estimation of student guessing and slipping leads to better prediction within the tutoring software (Baker, Corbett, \& Aleven, 2008a, 2008b). However, it is not yet clear whether this new variant on knowledge tracing is effective at predicting the latent student knowledge that leads to successful post-test performance. In this paper, we compare the Contextual-Guess-and-Slip variant on Bayesian Knowledge Tracing to classical four-parameter Bayesian Knowledge Tracing and the Individual Difference Weights variant of Bayesian Knowledge Tracing (Corbett \& Anderson, 1995), investigating how well each model variant predicts post-test performance. We also test other ways to utilize contextual estimation of slipping within the tutor in post-test prediction, and discuss hypotheses for why slipping during tutor use is a significant predictor of post-test performance, even after Bayesian Knowledge Tracing estimates are controlled for.},
	language = {en},
	booktitle = {User {Modeling}, {Adaptation}, and {Personalization}},
	publisher = {Springer Berlin Heidelberg},
	author = {Baker, Ryan S. J. d. and Corbett, Albert T. and Gowda, Sujith M. and Wagner, Angela Z. and MacLaren, Benjamin A. and Kauffman, Linda R. and Mitchell, Aaron P. and Giguere, Stephen},
	editor = {De Bra, Paul and Kobsa, Alfred and Chin, David},
	year = {2010},
	keywords = {Bayesian Knowledge Tracing, Contextual Slip, Educational Data Mining, Intelligent Tutoring Systems, Student Modeling},
	pages = {52--63},
}

@article{fischer_user_2001,
	title = {User {Modeling} in {Human}–{Computer} {Interaction}},
	volume = {11},
	issn = {1573-1391},
	url = {https://doi.org/10.1023/A:1011145532042},
	doi = {10.1023/A:1011145532042},
	abstract = {A fundamental objective of human–computer interaction research is to make systems more usable, more useful, and to provide users with experiences fitting their specific background knowledge and objectives. The challenge in an information-rich world is not only to make information available to people at any time, at any place, and in any form, but specifically to say the “right” thing at the “right” time in the “right” way. Designers of collaborative human–computer systems face the formidable task of writing software for millions of users (at design time) while making it work as if it were designed for each individual user (only known at use time). User modeling research has attempted to address these issues. In this article, I will first review the objectives, progress, and unfulfilled hopes that have occurred over the last ten years, and illustrate them with some interesting computational environments and their underlying conceptual frameworks. A special emphasis is given to high-functionality applications and the impact of user modeling to make them more usable, useful, and learnable. Finally, an assessment of the current state of the art followed by some future challenges is given.},
	language = {en},
	number = {1},
	urldate = {2019-04-04},
	journal = {User Modeling and User-Adapted Interaction},
	author = {Fischer, Gerhard},
	month = mar,
	year = {2001},
	keywords = {active help systems, adaptive and adaptable systems, collaborative human-computer systems, critiquing systems, design environments, high functionality applications, human computer interaction, user modeling},
	pages = {65--86},
	file = {Springer Full Text PDF:files/1257/Fischer - 2001 - User Modeling in Human–Computer Interaction.pdf:application/pdf},
}

@book{demillo_revolution_2015,
	title = {Revolution in {Higher} {Education}: {How} a {Small} {Band} of {Innovators} {Will} {Make} {College} {Accessible} and {Affordable}},
	isbn = {978-0-262-02964-3},
	shorttitle = {Revolution in {Higher} {Education}},
	abstract = {A report from the front lines of higher education and technology that chronicles efforts to transform teaching, learning, and opportunity.Colleges and universities have become increasingly costly, and, except for a handful of highly selective, elite institutions, unresponsive to twenty-first-century needs. But for the past few years, technology-fueled innovation has begun to transform higher education, introducing new ways to disseminate knowledge and better ways to learn—all at lower cost. In this impassioned account, Richard DeMillo tells the behind-the-scenes story of these pioneering efforts and offers a roadmap for transforming higher education. Building on his earlier book, Abelard to Apple, DeMillo argues that the current system of higher education is clearly unsustainable. Colleges and universities are in financial crisis. Tuition rises inexorably. Graduates of reputable schools often fail to learn basic skills, and many cannot find suitable jobs. Meanwhile, student-loan default rates have soared while the elite Ivy and near-Ivy schools seem remote and irrelevant. Where are the revolutionaries who can save higher education? DeMillo's heroes are a small band of innovators who are bringing the revolution in technology to colleges and universities. DeMillo chronicles, among other things, the invention of MOOCs (Massive Open Online Courses) by professors at Stanford and MIT; Salman Khan's Khan Academy; the use of technology by struggling historically black colleges and universities to make learning more accessible; and the latest research on learning and the brain. He describes the revolution's goals and the entrenched hierarchical system it aims to overthrow; and he reframes the nature of the contract between society and its universities. The new institutions of a transformed higher education promise to demonstrate not only that education has value but also that it has values—virtues for the common good.},
	language = {en},
	publisher = {MIT Press},
	author = {DeMillo, Richard A. and Young, Andrew J.},
	month = aug,
	year = {2015},
	note = {Google-Books-ID: HO93CgAAQBAJ},
	keywords = {Education / Administration / Higher, Education / Higher},
}

@inproceedings{baker_more_2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {More {Accurate} {Student} {Modeling} through {Contextual} {Estimation} of {Slip} and {Guess} {Probabilities} in {Bayesian} {Knowledge} {Tracing}},
	isbn = {978-3-540-69132-7},
	abstract = {Modeling students’ knowledge is a fundamental part of intelligent tutoring systems. One of the most popular methods for estimating students’ knowledge is Corbett and Anderson’s [6] Bayesian Knowledge Tracing model. The model uses four parameters per skill, fit using student performance data, to relate performance to learning. Beck [1] showed that existing methods for determining these parameters are prone to the Identifiability Problem: the same performance data can be fit equally well by different parameters, with different implications on system behavior. Beck offered a solution based on Dirichlet Priors [1], but, we show this solution is vulnerable to a different problem, Model Degeneracy, where parameter values violate the model’s conceptual meaning (such as a student being more likely to get a correct answer if he/she does not know a skill than if he/she does).We offer a new method for instantiating Bayesian Knowledge Tracing, using machine learning to make contextual estimations of the probability that a student has guessed or slipped. This method is no more prone to problems with Identifiability than Beck’s solution, has less Model Degeneracy than competing approaches, and fits student performance data better than prior methods. Thus, it allows for more accurate and reliable student modeling in ITSs that use knowledge tracing.},
	language = {en},
	booktitle = {Intelligent {Tutoring} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Baker, Ryan S. J. d. and Corbett, Albert T. and Aleven, Vincent},
	editor = {Woolf, Beverley P. and Aïmeur, Esma and Nkambou, Roger and Lajoie, Susanne},
	year = {2008},
	keywords = {Student Modeling, Intelligent Tutoring System, Slip Model, Slip Parameter, Student Knowledge},
	pages = {406--415},
}

@inproceedings{blikstein_using_2011,
	address = {New York, NY, USA},
	series = {{LAK} '11},
	title = {Using {Learning} {Analytics} to {Assess} {Students}' {Behavior} in {Open}-ended {Programming} {Tasks}},
	isbn = {978-1-4503-0944-8},
	url = {http://doi.acm.org/10.1145/2090116.2090132},
	doi = {10.1145/2090116.2090132},
	abstract = {There is great interest in assessing student learning in unscripted, open-ended environments, but students' work can evolve in ways that are too subtle or too complex to be detected by the human eye. In this paper, I describe an automated technique to assess, analyze and visualize students learning computer programming. I logged hundreds of snapshots of students' code during a programming assignment, and I employ different quantitative techniques to extract students' behaviors and categorize them in terms of programming experience. First I review the literature on educational data mining, learning analytics, computer vision applied to assessment, and emotion detection, discuss the relevance of the work, and describe one case study with a group undergraduate engineering students},
	urldate = {2019-04-04},
	booktitle = {Proceedings of the 1st {International} {Conference} on {Learning} {Analytics} and {Knowledge}},
	publisher = {ACM},
	author = {Blikstein, Paulo},
	year = {2011},
	note = {event-place: Banff, Alberta, Canada},
	keywords = {learning analytics, educational data mining, automated assessment, constructionism, logging},
	pages = {110--116},
}

@article{macfadyen_mining_2010,
	title = {Mining {LMS} data to develop an “early warning system” for educators: {A} proof of concept},
	volume = {54},
	issn = {0360-1315},
	shorttitle = {Mining {LMS} data to develop an “early warning system” for educators},
	url = {http://www.sciencedirect.com/science/article/pii/S0360131509002486},
	doi = {10.1016/j.compedu.2009.09.008},
	abstract = {Earlier studies have suggested that higher education institutions could harness the predictive power of Learning Management System (LMS) data to develop reporting tools that identify at-risk students and allow for more timely pedagogical interventions. This paper confirms and extends this proposition by providing data from an international research project investigating which student online activities accurately predict academic achievement. Analysis of LMS tracking data from a Blackboard Vista-supported course identified 15 variables demonstrating a significant simple correlation with student final grade. Regression modelling generated a best-fit predictive model for this course which incorporates key variables such as total number of discussion messages posted, total number of mail messages sent, and total number of assessments completed and which explains more than 30\% of the variation in student final grade. Logistic modelling demonstrated the predictive power of this model, which correctly identified 81\% of students who achieved a failing grade. Moreover, network analysis of course discussion forums afforded insight into the development of the student learning community by identifying disconnected students, patterns of student-to-student communication, and instructor positioning within the network. This study affirms that pedagogically meaningful information can be extracted from LMS-generated student tracking data, and discusses how these findings are informing the development of a customizable dashboard-like reporting tool for educators that will extract and visualize real-time data on student engagement and likelihood of success.},
	number = {2},
	urldate = {2019-04-04},
	journal = {Computers \& Education},
	author = {Macfadyen, Leah P. and Dawson, Shane},
	month = feb,
	year = {2010},
	keywords = {Collaborative learning, Evaluation methodologies, Learning communities, Post-secondary education, Teaching/learning strategies},
	pages = {588--599},
	file = {ScienceDirect Snapshot:files/1263/S0360131509002486.html:text/html},
}

@article{heathcote_harnessing_2010,
	title = {Harnessing {ICT} potential: {The} adoption and analysis of {ICT} systems for enhancing the student learning experience},
	volume = {24},
	issn = {0951-354X},
	shorttitle = {Harnessing {ICT} potential},
	url = {https://www.emeraldinsight.com/doi/abs/10.1108/09513541011020936},
	doi = {10.1108/09513541011020936},
	number = {2},
	urldate = {2019-04-04},
	journal = {International Journal of Educational Management},
	author = {Heathcote, Liz and Dawson, Shane and Poole, Gary},
	month = feb,
	year = {2010},
	keywords = {Higher education, Communication technologies, E‐learning},
	pages = {116--128},
	file = {Snapshot:files/1265/09513541011020936.html:text/html},
}

@techreport{noauthor_guipractica_2018,
	title = {Guía práctica para las evaluaciones de impacto en la protección de los datos sujetas al {RGPD}},
	url = {https://www.aepd.es/media/guias/guia-evaluaciones-de-impacto-rgpd.pdf},
	institution = {Agencia Española de Protección de Datos},
	month = may,
	year = {2018},
}

@article{maalej_toward_2016,
	title = {Toward {Data}-{Driven} {Requirements} {Engineering}},
	volume = {33},
	issn = {0740-7459},
	doi = {10.1109/MS.2015.153},
	abstract = {Nowadays, users can easily submit feedback about software products in app stores, social media, or user groups. Moreover, software vendors are collecting massive amounts of implicit feedback in the form of usage data, error logs, and sensor data. These trends suggest a shift toward data-driven user-centered identification, prioritization, and management of software requirements. Developers should be able to adopt the requirements of masses of users when deciding what to develop and when to release. They could systematically use explicit and implicit user data in an aggregated form to support requirements decisions. The goal is data-driven requirements engineering by the masses and for the masses.},
	number = {1},
	journal = {IEEE Software},
	author = {Maalej, W. and Nayebi, M. and Johann, T. and Ruhe, G.},
	month = jan,
	year = {2016},
	keywords = {app reviews, data-driven requirements engineering, data-driven user-centered software requirement identification, data-driven user-centered software requirement management, data-driven user-centered software requirement prioritization, decision support, error logs, explicit user data, Feature extraction, formal specification, implicit user data, Market research, Media, requirements engineering, Requirements engineering, sensor data, software analytics, software development, software engineering, Software engineering, software management, software products, software vendors, Stakeholders, usage data, user feedback},
	pages = {48--54},
	file = {IEEE Xplore Abstract Record:files/1268/7325177.html:text/html},
}

@misc{tsai_y.-s._sheila:_2018,
	title = {{SHEILA}: {Supporting} {Higher} {Education} to {Intergrade} {Learning} {Analytics}},
	url = {https://sheilaproject.eu/2018/11/30/sheila-final-research-report/},
	author = {{Tsai, Y.-S.} and {Gašević, D.} and {Whitelock-Wainwright, A.} and {Muñoz-Merino, P. J.} and {Moreno-Marcos, P. M.} and {Fernández, A. R.} and {Kloos, C. D.} and {Scheffel, M.} and {Jivet, I.} and {Drachsler, H.} and {Tammets, K.} and {Calleja, A. R.} and {Kollom, K.}},
	year = {2018},
}

@misc{noauthor_reglamento_2016,
	title = {Reglamento ({UE}) 2016/679 del {Parlamento} {Europeo} y del {Consejo}, de 27 de abril de 2016, relativo a la protección de las personas físicas en lo que respecta al tratamiento de datos personales y a la libre circulación de estos datos y por el que se deroga la {Directiva} 95/46/{CE} ({Reglamento} general de protección de datos) ({Texto} pertinente a efectos del {EEE})},
	url = {http://data.europa.eu/eli/reg/2016/679/oj/spa},
	language = {es},
	urldate = {2019-05-22},
	month = may,
	year = {2016},
}

@article{biosca_p._lo_2018,
	title = {Lo mejor y lo peor de la nueva normativa de privacidad, según los expertos},
	url = {https://www.abc.es/tecnologia/redes/abci-rgpd-mejor-y-peor-nueva-normativa-privacidad-segun-expertos-201805242219_noticia.html},
	abstract = {El nuevo reglamento europeo de protección de datos se aplica formalmente desde este viernes para darle mayor control de su información al usuario, aunque también ha recibido algunas críticas. ABC habla con cuatro profesionales del sector que valoran los aspectos positivos y negativos de las nuevas reglas del juego},
	language = {es},
	urldate = {2019-05-22},
	journal = {ABC},
	author = {{Biosca, P.}},
	month = may,
	year = {2018},
	file = {Snapshot:files/1327/abci-rgpd-mejor-y-peor-nueva-normativa-privacidad-segun-expertos-201805242219_noticia.html:text/html},
}

@article{jackie_wang_this_2017,
	edition = {\#87},
	title = {“{This} {Is} a {Story} {About} {Nerds} and {Cops}”: {PredPol} and {Algorithmic} {Policing}},
	shorttitle = {{PredPol} and {Algorithmic} {Policing}},
	url = {https://www.e-flux.com/journal/87/169043/this-is-a-story-about-nerds-and-cops-predpol-and-algorithmic-policing/},
	language = {en},
	urldate = {2019-07-22},
	journal = {e-flux},
	author = {{Jackie Wang}},
	month = dec,
	year = {2017},
	file = {Snapshot:files/1368/this-is-a-story-about-nerds-and-cops-predpol-and-algorithmic-policing.html:text/html},
}

@book{noauthor_ethics_nodate,
	title = {Ethics and {Data} {Science}},
	url = {https://learning.oreilly.com/library/view/ethics-and-data/9781492043898/},
	abstract = {As the impact of data science continues to grow on society there is an increased need to discuss how data is appropriately used and how to address misuse. Ye...},
	language = {en},
	urldate = {2019-07-22},
	file = {Snapshot:files/1370/9781492043898.html:text/html},
}

@book{loukides_ethics_2018,
	title = {Ethics and {Data} {Science}},
	abstract = {As the impact of data science continues to grow on society there is an increased need to discuss how data is appropriately used and how to address misuse. Yet, ethical principles for working with data have been available for decades. The real issue today is how to put those principles into action. With this report, authors Mike Loukides, Hilary Mason, and DJ Patil examine practical ways for making ethical data standards part of your work every day. To help you consider all of possible ramifications of your work on data projects, this report includes: A sample checklist that you can adapt for your own procedures Five framing guidelines (the Five C's) for building data products: consent, clarity, consistency, control, and consequences Suggestions for building ethics into your data-driven culture Now is the time to invest in a deliberate practice of data ethics, for better products, better teams, and better outcomes. Get a copy of this report and learn what it takes to do good data science today.},
	language = {en},
	publisher = {O'Reilly Media},
	author = {Loukides, Michael Kosta and Mason, Hilary and Patil, Dhanurjay},
	year = {2018},
}

@misc{noauthor_sistema_nodate,
	title = {Sistema {VioGén} - {Ministerio} del {Interior}},
	url = {http://www.interior.gob.es/web/servicios-al-ciudadano/violencia-contra-la-mujer/sistema-viogen},
	abstract = {Ministerio del Interior, Espa{\textbackslash}ntildea},
	language = {es-ES},
	urldate = {2019-07-22},
	journal = {Servicios al Ciudadano},
	file = {Snapshot:files/1374/sistema-viogen.html:text/html},
}

@book{hastie_elements_2001,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}},
	isbn = {978-0-387-21606-5},
	shorttitle = {The {Elements} of {Statistical} {Learning}},
	url = {https://www.springer.com/gp/book/9780387216065},
	abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
	language = {en},
	urldate = {2019-07-22},
	publisher = {Springer-Verlag},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2001},
	file = {Snapshot:files/1376/9780387216065.html:text/html},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	title = {Deep {Learning}},
	isbn = {978-0-262-03561-3},
	abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”―Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	language = {English},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	month = nov,
	year = {2016},
}

@misc{noauthor_home_nodate,
	title = {Home - {Keras} {Documentation}},
	url = {https://keras.io/},
	urldate = {2019-07-22},
	file = {Home - Keras Documentation:files/1380/keras.io.html:text/html},
}

@misc{noauthor_train_nodate,
	title = {Train your first neural network: basic classification {\textbar} {TensorFlow} {Core}},
	shorttitle = {Train your first neural network},
	url = {https://www.tensorflow.org/tutorials/keras/basic_classification},
	language = {en},
	urldate = {2019-07-25},
	journal = {TensorFlow},
	file = {Snapshot:files/1383/basic_classification.html:text/html},
}

@misc{noauthor_tutorial:_nodate,
	title = {Tutorial: {Basic} {Classification}},
	shorttitle = {Tutorial},
	url = {https://keras.rstudio.com/articles/tutorial_basic_classification.html},
	language = {en},
	urldate = {2019-07-25},
}

@article{sevillano_improving_2018,
	title = {Improving classification of pollen grain images of the {POLEN23E} dataset through three different applications of deep learning convolutional neural networks},
	volume = {13},
	copyright = {All rights reserved},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201807},
	doi = {10.1371/journal.pone.0201807},
	abstract = {In palynology, the visual classification of pollen grains from different species is a hard task which is usually tackled by human operators using microscopes. Its complete automatization would save a high quantity of resources and provide valuable improvements especially for allergy-related information systems, but also for other application fields as paleoclimate reconstruction, quality control of honey based products, collection of evidences in criminal investigations or fabric dating and tracking. This paper presents three state-of-the-art deep learning classification methods applied to the recently published POLEN23E image dataset. The three methods make use of convolutional neural networks: the first one is strictly based on the idea of transfer learning, the second one is based on feature extraction and the third one represents a hybrid approach, combining transfer learning and feature extraction. The results from the three methods are indeed very good, reaching over 97\% correct classification rates in images not previously seen by the models, where other authors reported around 70.},
	language = {en},
	number = {9},
	urldate = {2019-12-06},
	journal = {PLOS ONE},
	author = {Sevillano, Víctor and Aznarte, José L.},
	month = sep,
	year = {2018},
	keywords = {Allergies, Honey, Imaging techniques, Learning, Machine learning, Machine learning algorithms, Neural networks, Pollen},
	pages = {e0201807},
	file = {Full Text:files/1902/José L. Aznarte and Victor Sevillano and Suzannah Rutherford and Jose L. Aznarte - 2018 - Improving classification of pollen grain images of.pdf:application/pdf;Full Text PDF:files/1743/Sevillano and Aznarte - 2018 - Improving classification of pollen grain images of.pdf:application/pdf;Snapshot:files/1755/article.html:text/html},
}

@article{damato_allergenic_2007,
	title = {Allergenic pollen and pollen allergy in {Europe}},
	volume = {62},
	issn = {1398-9995},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1398-9995.2007.01393.x},
	doi = {10.1111/j.1398-9995.2007.01393.x},
	abstract = {The allergenic content of the atmosphere varies according to climate, geography and vegetation. Data on the presence and prevalence of allergenic airborne pollens, obtained from both aerobiological studies and allergological investigations, make it possible to design pollen calendars with the approximate flowering period of the plants in the sampling area. In this way, even though pollen production and dispersal from year to year depend on the patterns of preseason weather and on the conditions prevailing at the time of anthesis, it is usually possible to forecast the chances of encountering high atmospheric allergenic pollen concentrations in different areas. Aerobiological and allergological studies show that the pollen map of Europe is changing also as a result of cultural factors (for example, importation of plants such as birch and cypress for urban parklands), greater international travel (e.g. colonization by ragweed in France, northern Italy, Austria, Hungary etc.) and climate change. In this regard, the higher frequency of weather extremes, like thunderstorms, and increasing episodes of long range transport of allergenic pollen represent new challenges for researchers. Furthermore, in the last few years, experimental data on pollen and subpollen-particles structure, the pathogenetic role of pollen and the interaction between pollen and air pollutants, gave new insights into the mechanisms of respiratory allergic diseases.},
	language = {en},
	number = {9},
	urldate = {2019-12-06},
	journal = {Allergy},
	author = {D’Amato, G. and Cecchi, L. and Bonini, S. and Nunes, C. and Annesi‐Maesano, I. and Behrendt, H. and Liccardi, G. and Popov, T. and Cauwenberge, P. Van},
	year = {2007},
	keywords = {pollinosis, airway hypersensitivity, allergenic pollens, allergic asthma, allergic rhinitis, bronchial asthma, outdoor air-pollution, respiratory allergy, seasonal allergy},
	pages = {976--990},
	file = {Full Text PDF:files/1766/D’Amato et al. - 2007 - Allergenic pollen and pollen allergy in Europe.pdf:application/pdf;Snapshot:files/1761/j.1398-9995.2007.01393.html:text/html},
}

@article{bastl_evaluation_2017,
	title = {Evaluation of {Pollen} {Apps} {Forecasts}: {The} {Need} for {Quality} {Control} in an {eHealth} {Service}},
	volume = {19},
	issn = {1439-4456},
	shorttitle = {Evaluation of {Pollen} {Apps} {Forecasts}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5440733/},
	doi = {10.2196/jmir.7426},
	abstract = {Background
Pollen forecasts are highly valuable for allergen avoidance and thus raising the quality of life of persons concerned by pollen allergies. They are considered as valuable free services for the public. Careful scientific evaluation of pollen forecasts in terms of accurateness and reliability has not been available till date.

Objective
The aim of this study was to analyze 9 mobile apps, which deliver pollen information and pollen forecasts, with a focus on their accurateness regarding the prediction of the pollen load in the grass pollen season 2016 to assess their usefulness for pollen allergy sufferers.

Methods
The following number of apps was evaluated for each location: 3 apps for Vienna (Austria), 4 apps for Berlin (Germany), and 1 app each for Basel (Switzerland) and London (United Kingdom). All mobile apps were freely available. Today’s grass pollen forecast was compared throughout the defined grass pollen season at each respective location with measured grass pollen concentrations. Hit rates were calculated for the exact performance and for a tolerance in a range of ±2 and ±4 pollen per cubic meter.

Results
In general, for most apps, hit rates score around 50\% (6 apps). It was found that 1 app showed better results, whereas 3 apps performed less well. Hit rates increased when calculated with tolerances for most apps. In contrast, the forecast for the “readiness to flower” for grasses was performed at a sufficiently accurate level, although only two apps provided such a forecast. The last of those forecasts coincided with the first moderate grass pollen load on the predicted day or 3 days after and performed even from about a month before well within the range of 3 days. Advertisement was present in 3 of the 9 analyzed apps, whereas an imprint mentioning institutions with experience in pollen forecasting was present in only three other apps.

Conclusions
The quality of pollen forecasts is in need of improvement, and quality control for pollen forecasts is recommended to avoid potential harm to pollen allergy sufferers due to inadequate forecasts. The inclusion of information on reliability of provided forecasts and a similar handling regarding probabilistic weather forecasts should be considered.},
	number = {5},
	urldate = {2019-12-06},
	journal = {Journal of Medical Internet Research},
	author = {Bastl, Katharina and Berger, Uwe and Kmenta, Maximilian},
	month = may,
	year = {2017},
	pmid = {28483740},
	pmcid = {PMC5440733},
}

@article{ohe_harmonized_2004,
	title = {Harmonized methods of melissopalynology},
	volume = {35},
	copyright = {INRA, EDP Sciences, DIB, AGIB},
	issn = {0044-8435, 1297-9678},
	url = {http://dx.doi.org/10.1051/apido:2004050},
	doi = {10.1051/apido:2004050},
	abstract = {Apidologie, A Quality Journal in Bee Science},
	language = {en},
	number = {Suppl. 1},
	urldate = {2019-12-06},
	journal = {Apidologie},
	author = {Ohe, Werner Von Der and Oddo, Livia Persano and Piana, Maria Lucia and Morlot, Monique and Martin, Peter},
	year = {2004},
	pages = {S18--S25},
	file = {Full Text PDF:files/1781/Ohe et al. - 2004 - Harmonized methods of melissopalynology.pdf:application/pdf;Snapshot:files/1782/MHS07.html:text/html},
}

@inproceedings{daood_sequential_2018,
	title = {Sequential {Recognition} of {Pollen} {Grain} {Z}-{Stacks} by {Combining} {CNN} and {RNN}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:  1. Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  2. The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  3. The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys\&rsquo; fees incurred therein.  4. Author(s) retain all proprietary rights other than copyright (such as patent rights).  5. Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  6. Author(s) may reproduce, or have reproduced, their article/paper for the author\&rsquo;s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author\&rsquo;s employer, and then only on the author\&rsquo;s or the employer\&rsquo;s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author\&rsquo;s or the employer\&rsquo;s creation (including tables of contents with links to other papers) without AAAI\&rsquo;s written permission.  7. Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  8. In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  9. In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/FLAIRS/FLAIRS18/paper/view/17642},
	abstract = {Pollen recognition has a wide range of industrial and scientific applications. It guides the energy industry to potential oil and gas deposits, it is proxy data for climate-change scien- tists, and it increases agricultural production. However, pollen recognition is time consuming because it is usually done by visual inspection. Current automated solutions rely on pre-designed measurements of texture and contours, which require tuning for optimal features of a dataset. Also, most methods classify pollen using single-focus images, which require pollen grains to be captured at specific focal planes. We take a difference approach. Instead of using single-focus images, we use stacks of multifocal images (i.e., z-stack) to account for both visual characteristics and 3-D information. We automatically learn from the data the best visual characteristics for classifying pollen using deep-learning methods. Here, we train convolutional and recurrent neural networks (CNN and RNN) to learn the optimal features and recognize a pollen grain as a sequence of multifocal images acquired by an optical microscope. Additionally, we transfer the knowledge pre-trained network to ours to improve its classification and convergence speed. We evaluated our method using 392 stack sequences of 10 types of pollen grains with 10 images for each sequence. Our method achieved a remarkable classi- fication rate of 100\%.},
	language = {en},
	urldate = {2019-12-10},
	booktitle = {The {Thirty}-{First} {International} {Flairs} {Conference}},
	author = {Daood, Amar and Ribeiro, Eraldo and Bush, Mark},
	month = may,
	year = {2018},
	file = {Full Text PDF:files/1784/Daood et al. - 2018 - Sequential Recognition of Pollen Grain Z-Stacks by.pdf:application/pdf;Snapshot:files/1785/17642.html:text/html},
}

@article{lagerstrom_comparison_2013,
	title = {A comparison of classification algorithms within the {Classifynder} pollen imaging system},
	volume = {1559},
	issn = {0094-243X},
	url = {https://aip.scitation.org/doi/abs/10.1063/1.4825017},
	doi = {10.1063/1.4825017},
	number = {1},
	urldate = {2019-12-10},
	journal = {AIP Conference Proceedings},
	author = {Lagerstrom, Ryan and Arzhaeva, Yulia and Bischof, Leanne and Haberle, Simon and Hopf, Felicitas and Lovell, David},
	month = oct,
	year = {2013},
	pages = {250--259},
	file = {Full Text:files/1788/Lagerstrom et al. - 2013 - A comparison of classification algorithms within t.pdf:application/pdf;Snapshot:files/1787/1.html:text/html},
}

@inproceedings{khanzhina_pollen_2018,
	title = {Pollen grain recognition using convolutional neural network.},
	booktitle = {{ESANN}},
	author = {Khanzhina, Natalia and Putin, Evgeny and Filchenkov, Andrey and Zamyatina, Elena},
	year = {2018},
	file = {Full Text:files/1790/Khanzhina et al. - 2018 - Pollen grain recognition using convolutional neura.pdf:application/pdf},
}

@inproceedings{menad_deep_2019,
	title = {Deep {Convolutional} {Neural} {Network} for {Pollen} {Grains} {Classification}.},
	booktitle = {{JERI}},
	author = {Menad, Hanane and Ben-Naoum, Farah and Amine, Abdelmalek},
	year = {2019},
	file = {Full Text:files/1792/Menad et al. - 2019 - Deep Convolutional Neural Network for Pollen Grain.pdf:application/pdf},
}

@article{goncalves_feature_2016,
	title = {Feature {Extraction} and {Machine} {Learning} for the {Classification} of {Brazilian} {Savannah} {Pollen} {Grains}},
	volume = {11},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0157044},
	doi = {10.1371/journal.pone.0157044},
	abstract = {The classification of pollen species and types is an important task in many areas like forensic palynology, archaeological palynology and melissopalynology. This paper presents the first annotated image dataset for the Brazilian Savannah pollen types that can be used to train and test computer vision based automatic pollen classifiers. A first baseline human and computer performance for this dataset has been established using 805 pollen images of 23 pollen types. In order to access the computer performance, a combination of three feature extractors and four machine learning techniques has been implemented, fine tuned and tested. The results of these tests are also presented in this paper.},
	language = {en},
	number = {6},
	urldate = {2019-12-10},
	journal = {PLOS ONE},
	author = {Gonçalves, Ariadne Barbosa and Souza, Junior Silva and Silva, Gercina Gonçalves da and Cereda, Marney Pascoli and Pott, Arnildo and Naka, Marco Hiroshi and Pistori, Hemerson},
	month = jun,
	year = {2016},
	keywords = {Pollen, Honey, Imaging techniques, Computer imaging, Computer vision, Computers, Extraction techniques, Human performance},
	pages = {e0157044},
	file = {Full Text PDF:files/1794/Gonçalves et al. - 2016 - Feature Extraction and Machine Learning for the Cl.pdf:application/pdf;Snapshot:files/1795/article.html:text/html},
}

@article{bogota-a_rapid_2011,
	title = {Rapid climate change from north {Andean} {Lake} {Fúquene} pollen records driven by obliquity: implications for a basin-wide biostratigraphic zonation for the last 284 ka},
	volume = {30},
	issn = {0277-3791},
	shorttitle = {Rapid climate change from north {Andean} {Lake} {Fúquene} pollen records driven by obliquity},
	url = {http://www.sciencedirect.com/science/article/pii/S0277379111002393},
	doi = {10.1016/j.quascirev.2011.08.003},
	abstract = {This paper compares a new super-high resolution pollen record from a central location in Lake Fúquene (4°N) with 3 pollen records from marginal sites from the same lake basin, located at 2540 m elevation in the Eastern Cordillera of Colombia. We harmonized the pollen sum of all records, and provided previously published records of climate change with an improved age model using a new approach for long continental pollen records. We dissociated from subjective curve matching and applied a more objective procedure including radiocarbon ages, cyclostratigraphy, and orbital tuning using the new 284 ka long Fúquene Basin Composite record (Fq-BC) as the backbone (Groot et al., 2011). We showed that a common ∼9 m cycle in the arboreal pollen percentage (AP\%) records reflects obliquity forcing and drives vegetational and climatic change. The AP\% records were tuned to the 41 kyr component filtered from standard benthic δ18O LR04 record. Changes in sediment supply to the lake are reflected in concert by the four records making frequency analysis in the depth domain an adequate method to compare records from the same basin. We calibrated the original 14C ages and used where necessary biostratigraphic correlation, i.e. for records shorter than one obliquity cycle. Pollen records from the periphery of the lake showed changes in the abundance of Alnus and Weinmannia forests more clearly while centrally located record Fq-9C shows a more integrated signal of regional vegetation change. The revised age models show that core Fq-2 reflects the last 44 ka and composite record Fq-7C the last 85.5 ka. Marginally located core Fq-3 has an age of 133 ka at 32 m core depth and the lowermost 11 m of sediments appear of older but unknown age. The longest record Fq-BC shows ∼60 yr resolution over the period of 284-27 ka. All pollen records are in support of a common regional vegetation development leading to a robust reconstruction of long series of submillennial climate oscillations reflecting Dansgaard–Oeschger (DO) cycles. Reconstructed climate variability in the tropical Andes since marine isotope stage (MIS) 8 compares well with NGRIP (δ18O based), Epica Dome C (δD based) and the Mediterranean sea surface temperature record MD01-2443/44 (UK′37 based) underpinning the global significance of the climate record from this tropical Andean lake. A basin-wide biostratigraphy is presented and we concluded although with varying robustness that each core is representative of regional vegetational and climatic change.},
	language = {en},
	number = {23},
	urldate = {2019-12-18},
	journal = {Quaternary Science Reviews},
	author = {Bogotá-A, R. G. and Groot, M. H. M. and Hooghiemstra, H. and Lourens, L. J. and Van der Linden, M. and Berrio, J. C.},
	month = nov,
	year = {2011},
	keywords = {Biostratigraphic zonation, Dansgaard–Oeschger cycles, Frequency analysis, Fúquene basin, Lake level change, Late Pleistocene, Pollen records, Rapid climate change},
	pages = {3321--3337},
	file = {ScienceDirect Snapshot:files/1805/S0277379111002393.html:text/html},
}

@article{odgaard_fossil_1999,
	title = {Fossil pollen as a record of past biodiversity},
	volume = {26},
	issn = {1365-2699},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1046/j.1365-2699.1999.00280.x},
	doi = {10.1046/j.1365-2699.1999.00280.x},
	abstract = {Quaternary pollen records may contribute uniquely to the understanding of present plant diversity. Pollen assemblages can reflect diversity at community and landscape scales but the time resolution of most studies does not match that of modern ecological studies. Because of the complicating effects of differential pollen productivity and dispersal, pollen records do not directly reflect equitability aspects of vegetation diversity. Vegetation diversity indices other than S (the total number of taxa) are therefore not appropriate for pollen assemblages. As a measure of the species richness palynological richness is biased by the lack of taxonomic precision, by a possible interference on pollen dispersal from vegetation structure and by pollen representation. The nonlinear relationship between species richness and pollen-taxa richness may be used in attempts to estimate past floristic richness from fossil pollen assemblages. Using a hypothetical example the strong effect of cover shifts in the vegetation affecting taxa with different representation (Rrel) values on observed palynological richness is demonstrated. It is suggested that estimates of relative pollen productivity should be used to guide the pollen sum on which pollen-type richness is estimated by rarefaction techniques and this approach is illustrated using a paired site study of late Holocene diversity dynamics. The need for a modern training set relating pollen-type richness to species richness, pollen productivity and vegetation structure is emphasized.},
	language = {en},
	number = {1},
	urldate = {2019-12-18},
	journal = {Journal of Biogeography},
	author = {Odgaard, Bent Vad},
	year = {1999},
	keywords = {Floristic richness, palaeoecology, palynological richness, Quaternary, rarefaction},
	pages = {7--17},
	file = {Snapshot:files/1807/j.1365-2699.1999.00280.html:text/html},
}

@article{flantua_climate_2016,
	title = {Climate variability and human impact in {South} {America} during the last 2000 years: synthesis and perspectives from pollen records},
	volume = {12},
	issn = {1814-9332},
	shorttitle = {Climate variability and human impact in {South} {America} during the last 2000 years},
	url = {http://oro.open.ac.uk/45927/},
	abstract = {An improved understanding of present-day climate variability and change relies on high-quality data sets from the past 2 millennia. Global efforts to model regional climate modes are in the process of being validated against, and integrated with, records of past vegetation change. For South America, however, the full potential of vegetation records for evaluating and improving climate models has hitherto not been sufficiently acknowledged due to an absence of information on the spatial and temporal coverage of study sites. This paper therefore serves as a guide to high-quality pollen records that capture environmental variability during the last 2 millennia. We identify 60 vegetation (pollen) records from across South America which satisfy geochronological requirements set out for climate modelling, and we discuss their sensitivity to the spatial signature of climate modes throughout the continent. Diverse patterns of vegetation response to climate change are observed, with more similar patterns of change in the lowlands and varying intensity and direction of responses in the highlands. Pollen records display local-scale responses to climate modes; thus, it is necessary to understand how vegetation–climate interactions might diverge under variable settings. We provide a qualitative translation from pollen metrics to climate variables. Additionally, pollen is an excellent indicator of human impact through time. We discuss evidence for human land use in pollen records and provide an overview considered useful for archaeological hypothesis testing and important in distinguishing natural from anthropogenically driven vegetation change. We stress the need for the palynological community to be more familiar with climate variability patterns to correctly attribute the potential causes of observed vegetation dynamics. This manuscript forms part of the wider LOng-Term multi-proxy climate REconstructions and Dynamics in South America – 2k initiative that provides the ideal framework for the integration of the various palaeoclimatic subdisciplines and palaeo-science, thereby jump-starting and fostering multidisciplinary research into environmental change on centennial and millennial timescales.},
	urldate = {2019-12-18},
	journal = {Climate of the Past},
	author = {Flantua, S. G. A. and Hooghiemstra, H. and Vuille, M. and Behling, H. and Carson, J. F. and Gosling, W. D. and Hoyos, I. and Ledru, M. P. and Montoya, E. and Mayle, F. and Maldonado, A. and Rull, V. and Tonello, M. S. and Whitney, B. S. and González-Arango, C.},
	year = {2016},
	pages = {483--523},
	file = {Full Text PDF:files/1811/Flantua et al. - 2016 - Climate variability and human impact in South Amer.pdf:application/pdf;Snapshot:files/1812/45927.html:text/html},
}

@article{mildenhall_forensic_2006,
	title = {Forensic palynology: why do it and how it works},
	volume = {163},
	issn = {0379-0738},
	shorttitle = {Forensic palynology},
	doi = {10.1016/j.forsciint.2006.07.012},
	abstract = {Forensic palynology has been a law enforcement tool for over 50 years. Forensic palynology is the application of pollen and spores in solving legal issues, either civil or criminal. Pollen and spores can be obtained from an extremely wide range of items, including bodies. Pollen and spores provide clues as to the source of the items and the characteristics of the environments from which the material on them is sourced. Their usefulness lies in a combination of their abundance, dispersal mechanisms, resistance to mechanical and chemical destruction, microscopic size, and morphology. Their often complex morphology allows identification to an individual parent plant taxon that can be related to a specific ecological habitat or a specific scene. Pollen and spore assemblages characterise different environments and scenes and can easily be picked up and transported away from scenes of interest without providing any visual clue to a suspect as to what has occurred. With so many publications and high-profile cases involving forensic palynology and environmental analysis now receiving publicity, the future of this branch of forensic science is assured. Furthermore, with the development of multi-disciplinary approaches to environmental analyses of crime scenes, far more detailed information is now available to law enforcement agencies, enabling them to determine with greater accuracy what may have happened during the commission of criminal activities.},
	language = {eng},
	number = {3},
	journal = {Forensic Science International},
	author = {Mildenhall, D. C. and Wiltshire, P. E. J. and Bryant, V. M.},
	month = nov,
	year = {2006},
	pmid = {16920303},
	keywords = {Humans, Pollen, Botany, Ecosystem, Forensic Medicine, Specimen Handling, Spores},
	pages = {163--172},
}

@article{holt_principles_2014,
	title = {Principles and methods for automated palynology},
	volume = {203},
	copyright = {© 2014 The Authors. New Phytologist © 2014 New Phytologist Trust},
	issn = {1469-8137},
	url = {https://nph.onlinelibrary.wiley.com/doi/abs/10.1111/nph.12848},
	doi = {10.1111/nph.12848},
	abstract = {Pollen grains are microscopic so their identification and quantification has, for decades, depended upon human observers using light microscopes: a labour-intensive approach. Modern improvements in computing and imaging hardware and software now bring automation of pollen analyses within reach. In this paper, we provide the first review in over 15 yr of progress towards automation of the part of palynology concerned with counting and classifying pollen, bringing together literature published from a wide spectrum of sources. We consider which attempts offer the most potential for an automated palynology system for universal application across all fields of research concerned with pollen classification and counting. We discuss what is required to make the datasets of these automated systems as acceptable as those produced by human palynologists, and present suggestions for how automation will generate novel approaches to counting and classifying pollen that have hitherto been unthinkable.},
	language = {en},
	number = {3},
	urldate = {2019-12-18},
	journal = {New Phytologist},
	author = {Holt, K. A. and Bennett, K. D.},
	year = {2014},
	keywords = {automation, palynology, pollen counting, pollen identification, protocols},
	pages = {735--742},
	file = {Full Text PDF:files/1815/Holt and Bennett - 2014 - Principles and methods for automated palynology.pdf:application/pdf;Snapshot:files/1814/nph.html:text/html},
}

@article{mitsumoto_classification_2009,
	title = {Classification of pollen species using autofluorescence image analysis},
	volume = {107},
	issn = {1389-1723},
	url = {http://www.sciencedirect.com/science/article/pii/S1389172308000510},
	doi = {10.1016/j.jbiosc.2008.10.001},
	abstract = {A new method to classify pollen species was developed by monitoring autofluorescence images of pollen grains. The pollens of nine species were selected, and their autofluorescence images were captured by a microscope equipped with a digital camera. The pollen size and the ratio of the blue to red pollen autofluorescence spectra (the B/R ratio) were calculated by image processing. The B/R ratios and pollen size varied among the species. Furthermore, the scatter-plot of pollen size versus the B/R ratio showed that pollen could be classified to the species level using both parameters. The pollen size and B/R ratio were confirmed by means of particle flow image analysis and the fluorescence spectra, respectively. These results suggest that a flow system capable of measuring both scattered light and the autofluorescence of particles could classify and count pollen grains in real time.},
	language = {en},
	number = {1},
	urldate = {2019-12-18},
	journal = {Journal of Bioscience and Bioengineering},
	author = {Mitsumoto, Kotaro and Yabusaki, Katsumi and Aoyagi, Hideki},
	month = jan,
	year = {2009},
	keywords = {Pollen, Pollen classification, Autofluorescence, Hay fever, Microscopic image analysis, Pollen counting},
	pages = {90--94},
	file = {ScienceDirect Snapshot:files/1817/S1389172308000510.html:text/html},
}

@article{dellanna_pollen_2009,
	title = {Pollen discrimination and classification by {Fourier} transform infrared ({FT}-{IR}) microspectroscopy and machine learning},
	volume = {394},
	issn = {1618-2650},
	url = {https://doi.org/10.1007/s00216-009-2794-9},
	doi = {10.1007/s00216-009-2794-9},
	abstract = {The discrimination and classification of allergy-relevant pollen was studied for the first time by mid-infrared Fourier transform infrared (FT-IR) microspectroscopy together with unsupervised and supervised multivariate statistical methods. Pollen samples of 11 different taxa were collected, whose outdoor air concentration during the flowering time is typically measured by aerobiological monitoring networks. Unsupervised hierarchical cluster analysis provided valuable information about the reproducibility of FT-IR spectra of the same taxon acquired either from one pollen grain in a 25 × 25 μm2 area or from a group of grains inside a 100 × 100 μm2 area. As regards the supervised learning method, best results were achieved using a K nearest neighbors classifier and the leave-one-out cross-validation procedure on the dataset composed of single pollen grain spectra (overall accuracy 84\%). FT-IR microspectroscopy is therefore a reliable method for discrimination and classification of allergenic pollen. The limits of its practical application to the monitoring performed in the aerobiological stations were also discussed.},
	language = {en},
	number = {5},
	urldate = {2019-12-18},
	journal = {Analytical and Bioanalytical Chemistry},
	author = {Dell’Anna, R. and Lazzeri, P. and Frisanco, M. and Monti, F. and Malvezzi Campeggi, F. and Gottardini, E. and Bersani, M.},
	month = jul,
	year = {2009},
	pages = {1443--1452},
}

@article{ivleva_characterization_2005,
	title = {Characterization and discrimination of pollen by {Raman} microscopy},
	volume = {381},
	issn = {1618-2650},
	url = {https://doi.org/10.1007/s00216-004-2942-1},
	doi = {10.1007/s00216-004-2942-1},
	abstract = {The chemical characterization and discrimination of allergy-relevant pollen (common ragweed (Ambrosia artemisiifolia), white birch (Betula pendula), English oak (Quercus robur), and European linden (Tilia cordata)) has been studied by Raman microscopy. Spectra were obtained at different excitation wavelengths (514, 633, and 780 nm) and various methods were examined to minimize the strong fluorescence background. The use of a He–Ne laser (633 nm) for excitation yields high-quality single pollen Raman spectra, which contain multiple bands due to pollen components such as carotenoids, proteins, nucleic acids, carbohydrates, and lipids. Multivariate classification, i.e. principal component analysis (PCA) and hierarchical cluster analysis, demonstrated the validity of the approach for discrimination between different pollen species.},
	language = {en},
	number = {1},
	urldate = {2019-12-18},
	journal = {Analytical and Bioanalytical Chemistry},
	author = {Ivleva, N. P. and Niessner, R. and Panne, U.},
	month = jan,
	year = {2005},
	pages = {261--267},
}

@article{holt_progress_2011,
	title = {Progress towards an automated trainable pollen location and classifier system for use in the palynology laboratory},
	volume = {167},
	issn = {0034-6667},
	url = {http://www.sciencedirect.com/science/article/pii/S0034666711001205},
	doi = {10.1016/j.revpalbo.2011.08.006},
	abstract = {Palynological analysis, as applied in vegetation reconstruction, climate change studies, allergy research, melissopalynology and forensic science, is a slow, laborious process. Here, we present an ongoing project aimed at the realisation of a low-cost, automatic, trainable system for the location, recognition and counting of pollen on standard glass microscope slides. This system is designed to dramatically reduce the time that the palynologist must spend at the microscope, thus considerably increasing productivity in the pollen lab. The system employs robotics, image processing and neural network technology to locate, photograph and classify pollen on a conventionally prepared pollen slide. After locating pollen grains on a microscope slide, it captures images of them. The individual images of the pollen are then analysed using a set of mathematically defined features. These feature sets are then classified by the system by comparison with feature sets previously obtained from the analysis of images of known pollen types. The classified images are then presented to the palynologist for checking. This ability for post-classification checking is a key part of the automated palynology process, as it is likely that under the current technology, it will be very difficult to produce an automated pollen counting and classifier system that is 100\% correct 100\% of the time. However, it is important to remember that pollen counts performed by human palynologists are seldom 100\% correct 100\% of the time as well. The system has been tested on slides containing fresh pollen of six different species. The slides were counted repeatedly by both the system and by human palynologists. The results of these tests show that the machine can produce counts with very similar proportions to human palynologists (typically within 1–4\%). Although the means of the machine counts were usually slightly lower than those of the human counts, the variance was also lower, demonstrating that the machine counts pollen more consistently than human palynologists. The system described herein should be viewed as a potentially very valuable tool in the palynological laboratory. Its ability to discriminate between the bulk of pollen and debris on a slide and capture and store images of each pollen grain is in itself a very useful feature. This capability combined with the relatively positive results from this first all-of-system capture-and-classify test clearly demonstrate the potential of the system to considerably improve the efficiency of palynological analysis. However, more tests are required before the extent of the system's potential can be fully realised. The next step, testing the system on fossil pollen samples, is now underway.},
	language = {en},
	number = {3},
	urldate = {2019-12-18},
	journal = {Review of Palaeobotany and Palynology},
	author = {Holt, K. and Allen, G. and Hodgson, R. and Marsland, S. and Flenley, J.},
	month = oct,
	year = {2011},
	keywords = {neural networks, automated palynology, pollen analysis, pollen classification},
	pages = {175--183},
	file = {ScienceDirect Snapshot:files/1821/S0034666711001205.html:text/html},
}

@article{g._erdtman_acetolysis_1960,
	title = {The acetolysis method, a revised description.},
	volume = {54},
	journal = {Svensk Bot. Tidskr.},
	author = {{G. Erdtman}},
	year = {1960},
	pages = {561--564},
	file = {The acetolysis method, a revised description. | EUNOPS website:files/1823/acetolysis-method-revised-description.html:text/html},
}

@book{moar_pollen_1994,
	address = {Lincoln, N.Z.},
	title = {Pollen grains of {New} {Zealand} dicotyledonous plants},
	isbn = {978-0-478-04500-0 978-0-487-04500-8},
	language = {English},
	publisher = {Manaaki Whenua Press},
	author = {Moar, N. T},
	year = {1994},
	note = {OCLC: 31198927},
}

@incollection{jr_flenley_problem_1968,
	address = {Canberra},
	edition = {M.B. Clowes \& J.P. Penny},
	title = {The problem of pollen recognition},
	booktitle = {Problems in {Picture} {Interpretation}},
	publisher = {C S I R O},
	author = {{JR Flenley}},
	editor = {{M.B. Clowes} and {J.P. Penny}},
	year = {1968},
	pages = {141--145},
}

@article{holt_separating_2014,
	title = {Separating morphologically similar pollen types using basic shape features from digital images: {A} preliminary study1},
	volume = {2},
	issn = {2168-0450},
	shorttitle = {Separating morphologically similar pollen types using basic shape features from digital images},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4141716/},
	doi = {10.3732/apps.1400032},
	abstract = {• Premise of the study: One of the many advantages offered by automated palynology systems is the ability to vastly increase the number of observations made on a particular sample or samples. This is of particular benefit when attempting to fully quantify the degree of variation within or between closely related pollen types., • Methods: An automated palynology system (Classifynder) has been used to further investigate the variation in pollen morphology between two New Zealand species of Myrtaceae (Leptospermum scoparium and Kunzea ericoides) that are of significance in the New Zealand honey industry. Seven geometric features extracted from automatically gathered digital images were used to characterize the range of shape and size of the two taxa, and to examine the extent of previously reported overlap in these variables., • Results: Our results indicate a degree of overlap in all cases. The narrowest overlap was in measurements of maximum Feret diameter (MFD) in grains oriented in polar view. Multivariate statistical analysis using all seven factors provided the most robust discrimination between the two types., • Discussion: Further work is required before this approach could be routinely applied to separating the two pollen types used in this study, most notably the development of comprehensive reference distributions for the types in question.},
	number = {8},
	urldate = {2019-12-18},
	journal = {Applications in Plant Sciences},
	author = {Holt, Katherine A. and Bebbington, Mark S.},
	month = aug,
	year = {2014},
	pmid = {25202650},
	pmcid = {PMC4141716},
	file = {Full Text PDF:files/995/Holt and Bebbington - 2014 - Separating Morphologically Similar Pollen Types Us.pdf:application/pdf;PubMed Central Full Text PDF:files/1828/Holt and Bebbington - 2014 - Separating morphologically similar pollen types us.pdf:application/pdf;Snapshot:files/998/apps.html:text/html},
}

@article{aznarte_financial_2012,
	title = {Financial time series forecasting with a bio-inspired fuzzy model},
	volume = {39},
	copyright = {All rights reserved},
	url = {10.1016/j.eswa.2012.02.135},
	doi = {10.1016/j.eswa.2012.02.135},
	number = {16},
	urldate = {2020-01-13},
	journal = {Expert Systems With Applications},
	author = {Aznarte, Jose Luis and Alcala-Fdez, Jesus and Arauzo-Azofra, Antonio and Manuel Benitez, Jose},
	year = {2012},
	pages = {12302--12309},
}

@article{aznarte_test_2011,
	title = {A test for the homoscedasticity of the residuals in fuzzy rule-based forecasters},
	volume = {34},
	copyright = {All rights reserved},
	url = {10.1007/s10489-011-0288-x},
	doi = {10.1007/s10489-011-0288-x},
	number = {3},
	urldate = {2020-01-13},
	journal = {Applied Intelligence},
	author = {Aznarte, Jose Luis and Molina, Daniel and Sanchez, Ana M. and Benitez, Jose M.},
	year = {2011},
	pages = {386--393},
}

@article{bergmeir_time_2012,
	title = {Time {Series} {Modeling} and {Forecasting} {Using} {Memetic} {Algorithms} for {Regime}-{Switching} {Models}},
	volume = {23},
	copyright = {All rights reserved},
	url = {10.1109/TNNLS.2012.2216898},
	doi = {10.1109/TNNLS.2012.2216898},
	number = {11},
	urldate = {2020-01-13},
	journal = {Ieee Transactions on Neural Networks and Learning Systems},
	author = {Bergmeir, Christoph and Triguero, Isaac and Molina, Daniel and Luis Aznarte, Jose and Manuel Benitez, Jose},
	year = {2012},
	pages = {1841--1847},
}

@article{arauzo-azofra_empirical_2011,
	title = {Empirical study of feature selection methods based on individual feature evaluation for classification problems},
	volume = {38},
	copyright = {All rights reserved},
	url = {10.1016/j.eswa.2010.12.160},
	doi = {10.1016/j.eswa.2010.12.160},
	number = {7},
	urldate = {2020-01-13},
	journal = {Expert Systems With Applications},
	author = {Arauzo-Azofra, Antonio and Aznarte, Jose Luis and Benitez, Jose M.},
	year = {2011},
	pages = {8170--8177},
}

@article{aznarte_equivalences_2010,
	title = {Equivalences {Between} {Neural}-{Autoregressive} {Time} {Series} {Models} and {Fuzzy} {Systems}},
	volume = {21},
	copyright = {All rights reserved},
	url = {10.1109/TNN.2010.2060209},
	doi = {10.1109/TNN.2010.2060209},
	number = {9},
	urldate = {2020-01-13},
	journal = {Ieee Transactions on Neural Networks},
	author = {Aznarte, Jose Luis and Manuel Benitez, Jose},
	year = {2010},
	pages = {1434--1444},
}

@article{luis_aznarte_fuzzy_2011,
	title = {{FUZZY} {AUTOREGRESSIVE} {RULES}: {TOWARDS} {LINGUISTIC} {TIME} {SERIES} {MODELING}},
	volume = {30},
	copyright = {All rights reserved},
	shorttitle = {{FUZZY} {AUTOREGRESSIVE} {RULES}},
	url = {10.1080/07474938.2011.553569},
	doi = {10.1080/07474938.2011.553569},
	number = {6},
	urldate = {2020-01-13},
	journal = {Econometric Reviews},
	author = {Luis Aznarte, Jose and Alcala-Fdez, Jesus and Arauzo, Antonio and Manuel Benitez, Jose},
	year = {2011},
	pages = {646--668},
}

@article{luis_aznarte_linearity_2010,
	title = {Linearity testing for fuzzy rule-based models},
	volume = {161},
	copyright = {All rights reserved},
	url = {10.1016/j.fss.2010.01.005},
	doi = {10.1016/j.fss.2010.01.005},
	number = {13},
	urldate = {2020-01-13},
	journal = {Fuzzy Sets and Systems},
	author = {Luis Aznarte, Jose and Medeiros, Marcelo C. and Benitez, Jose M.},
	year = {2010},
	pages = {1836--1851},
}

@article{aznarte_testing_2010,
	title = {{TESTING} {FOR} {REMAINING} {AUTOCORRELATION} {OF} {THE} {RESIDUALS} {IN} {THE} {FRAMEWORK} {OF} {FUZZY} {RULE}-{BASED} {TIME} {SERIES} {MODELLING}},
	volume = {18},
	copyright = {All rights reserved},
	url = {10.1142/S021848851000660X},
	doi = {10.1142/S021848851000660X},
	number = {4},
	urldate = {2020-01-13},
	journal = {International Journal of Uncertainty Fuzziness and Knowledge-Based Systems},
	author = {Aznarte, Jose Luis and Medeiros, Marcelo C. and Benitez, Jose M.},
	year = {2010},
	pages = {371--387},
}

@article{navajas-perez_satdna_2007,
	title = {{SatDNA} analyzer: a computing tool for satellite-{DNA} evolutionary analysis},
	volume = {23},
	copyright = {All rights reserved},
	shorttitle = {{SatDNA} analyzer},
	url = {10.1093/bioinformatics/btm005},
	doi = {10.1093/bioinformatics/btm005},
	number = {6},
	urldate = {2020-01-13},
	journal = {Bioinformatics},
	author = {Navajas-Perez, Rafael and Rubio-Escudero, Cristina and Aznarte, Jose Luis and Rejon, Manuel Ruiz and Garrido-Ramos, Manuel A.},
	year = {2007},
	pages = {767--768},
	file = {Full Text:files/1900/Navajas-Perez et al. - 2007 - SatDNA analyzer a computing tool for satellite-DN.pdf:application/pdf},
}

@article{aznarte_m_smooth_2007,
	title = {Smooth transition autoregressive models and fuzzy rule-based systems: {Functional} equivalence and consequences},
	volume = {158},
	copyright = {All rights reserved},
	shorttitle = {Smooth transition autoregressive models and fuzzy rule-based systems},
	url = {10.1016/j.fss.2007.03.021},
	doi = {10.1016/j.fss.2007.03.021},
	number = {24},
	urldate = {2020-01-13},
	journal = {Fuzzy Sets and Systems},
	author = {Aznarte M., Jose Luis and Manuel Benitez, Jose and Luis Castro, Juan},
	year = {2007},
	pages = {2734--2745},
}

@inproceedings{arauzo-azofra_empirical_2009,
	series = {2009 {Ninth} {International} {Conference} on {Intelligent} {Systems} {Design} and {Applications}},
	title = {Empirical {Study} of {Individual} {Feature} {Evaluators} and {Cutting} {Criteria} for {Feature} {Selection} in {Classification}},
	copyright = {All rights reserved},
	url = {http://dx.doi.org/10.1109/isda.2009.175},
	doi = {10.1109/isda.2009.175},
	urldate = {2020-01-13},
	booktitle = {2009 {Ninth} {International} {Conference} on {Intelligent} {Systems} {Design} and {Applications}},
	publisher = {IEEE},
	author = {Arauzo-Azofra, Antonio and M., José L. Aznarte and Bentez, José M.},
	year = {2009},
}

@article{florido_detecting_2015,
	title = {Detecting precursory patterns to enhance earthquake prediction in {Chile}},
	volume = {76},
	copyright = {All rights reserved},
	url = {http://dx.doi.org/10.1016/j.cageo.2014.12.002},
	doi = {10.1016/j.cageo.2014.12.002},
	urldate = {2020-01-13},
	journal = {Computers \& Geosciences},
	author = {Florido, E. and Martnez-Álvarez, F. and Morales-Esteban, A. and Reyes, J. and Aznarte-Mellado, J.L.},
	month = mar,
	year = {2015},
	pages = {112--120},
}

@article{aznarte_dynamic_2016,
	title = {Dynamic {Line} {Rating} {Using} {Numerical} {Weather} {Predictions} and {Machine} {Learning}: a {Case} {Study}},
	copyright = {All rights reserved},
	shorttitle = {Dynamic {Line} {Rating} {Using} {Numerical} {Weather} {Predictions} and {Machine} {Learning}},
	url = {http://dx.doi.org/10.1109/TPWRD.2016.2543818},
	doi = {10.1109/tpwrd.2016.2543818},
	urldate = {2020-01-13},
	journal = {IEEE Transactions on Power Delivery},
	author = {Aznarte, Jose and Siebert, Nils},
	year = {2016},
	pages = {1},
}

@article{navares_predicting_2016,
	title = {Predicting the {Poaceae} pollen season: six month-ahead forecasting and identification of relevant features},
	copyright = {All rights reserved},
	shorttitle = {Predicting the {Poaceae} pollen season},
	url = {https://doi.org/10.1007%2Fs00484-016-1242-8},
	doi = {10.1007/s00484-016-1242-8},
	urldate = {2020-01-13},
	journal = {International Journal of Biometeorology},
	author = {Navares, Ricardo and Aznarte, José Luis},
	month = sep,
	year = {2016},
}

@incollection{navajas-perez_satdna_2007-1,
	series = {Bioinformatics {Research} and {Development}, {Proceedings}},
	title = {{satDNA} analyzer 1.2 as a valuable computing tool for evolutionary analysis of satellite-{DNA} families: {Revisiting} {Y}-linked satellite-{DNA} sequences of {Rumex} ({Polygonaceae})},
	volume = {4414 LNBI},
	copyright = {All rights reserved},
	shorttitle = {{satDNA} analyzer 1.2 as a valuable computing tool for evolutionary analysis of satellite-{DNA} families},
	booktitle = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	author = {Navajas-Pérez, R. and Rejón, M.R. and Garrido-Ramos, M. and Aznarte, J.L. and Rubio-Escudero, C.},
	year = {2007},
	pages = {131--139},
}

@article{aznarte_m_neuro-fuzzy_2005,
	title = {Neuro-fuzzy prediction of airborne pollen concentrations},
	copyright = {All rights reserved},
	journal = {Proceedings - 4th Conference of the European Society for Fuzzy Logic and Technology and 11th French Days on Fuzzy Logic and Applications, EUSFLAT-LFA 2005 Joint Conference},
	author = {Aznarte M., J.L. and Lugilde, D.N. and Benítez, J.M. and De Linares Fernández, C.},
	year = {2005},
	pages = {1325--1330},
}

@incollection{aznarte_identifiability_2006,
	title = {On the identifiability of {TSK} additive fuzzy rule-based models},
	volume = {37},
	copyright = {All rights reserved},
	url = {10.1007/3-540-34777-1_11},
	urldate = {2020-01-13},
	booktitle = {Advances in {Soft} {Computing}},
	author = {Aznarte, J.L. and Benítez, J.M.},
	year = {2006},
	pages = {79--86},
	file = {Scopus - Advances in Soft Computing\: On the identifiability of TSK additive fuzzy rule-based models:files/654/display.html:text/html},
}

@inproceedings{aznarte_m_testing_2009,
	title = {Testing for serial independence of the residuals in the framework of fuzzy rule-based time series modeling},
	copyright = {All rights reserved},
	url = {10.1109/ISDA.2009.249},
	doi = {10.1109/ISDA.2009.249},
	urldate = {2020-01-13},
	booktitle = {{ISDA} 2009 - 9th {International} {Conference} on {Intelligent} {Systems} {Design} and {Applications}},
	author = {Aznarte M, J.L. and Arauzo, A. and Sánchez, J.M.B.},
	year = {2009},
	pages = {1383--1387},
}

@article{aznarte_links_2013,
	title = {The {Links} between {Statistical} and {Fuzzy} {Models} for {Time} {Series} {Analysis} and {Forecasting}},
	volume = {47},
	copyright = {All rights reserved},
	url = {10.1007/978-3-642-33439-9_1},
	doi = {10.1007/978-3-642-33439-9_1},
	urldate = {2020-01-13},
	journal = {Intelligent Systems Reference Library},
	author = {Aznarte, J.L. and Benítez, J.M.},
	year = {2013},
	pages = {1--30},
}

@incollection{aznarte_m_testing_2010,
	series = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	title = {Testing for heteroskedasticity of the residuals in fuzzy rule-based models},
	volume = {6097 LNAI},
	copyright = {All rights reserved},
	url = {10.1007/978-3-642-13025-0_26},
	urldate = {2020-01-13},
	booktitle = {Trends in {Applied} {Intelligent} {Systems}, {Pt} {Ii}, {Proceedings}},
	author = {Aznarte M., J.L. and Benítez, J.M.},
	year = {2010},
	pages = {239--246},
	file = {Submitted Version:files/1901/Aznarte M. and Benítez - 2010 - Testing for heteroskedasticity of the residuals in.pdf:application/pdf},
}

@article{navares_what_2017,
	title = {What are the most important variables for {Poaceae} airborne pollen forecasting?},
	volume = {579},
	copyright = {All rights reserved},
	url = {10.1016/j.scitotenv.2016.11.096},
	doi = {10.1016/j.scitotenv.2016.11.096},
	urldate = {2020-01-13},
	journal = {Science of the Total Environment},
	author = {Navares, R. and Aznarte, J.L.},
	year = {2017},
	pages = {1161--1169},
}

@article{aznarte_probabilistic_2017,
	title = {Probabilistic forecasting for extreme {NO} 2  pollution episodes},
	volume = {229},
	copyright = {All rights reserved},
	url = {https://doi.org/10.1016%2Fj.envpol.2017.05.079},
	doi = {10.1016/j.envpol.2017.05.079},
	urldate = {2020-01-13},
	journal = {Environmental Pollution},
	author = {Aznarte, José L.},
	month = oct,
	year = {2017},
	pages = {321--328},
}

@incollection{navares_forecasting_2017,
	address = {Cham},
	series = {Advances in {Time} {Series} {Analysis} and {Forecasting}: {Selected} {Contributions} from {ITISE} 2016},
	title = {Forecasting the {Start} and {End} of {Pollen} {Season} in {Madrid}},
	copyright = {All rights reserved},
	isbn = {978-3-319-55789-2},
	url = {https://doi.org/10.1007/978-3-319-55789-2_27},
	abstract = {In this paper we approach the problem of predicting the start and the end dates for the pollen season of grasses (family Poaceae) and plantains (family Plantago) in the city of Madrid. A classification-based approach is introduced to forecast the main pollination season, and the proposed method is applied to a range of parameters such as the threshold level, which defines the pollen season, and several forecasting horizons. Different computational intelligence approaches are tested including Random Forests, Logistic Regression and Support Vector Machines. The model allows to predict risk exposures for patients and thus anticipate the activation of preventive measures for clinical institutions.},
	urldate = {2020-01-13},
	booktitle = {Advances in {Time} {Series} {Analysis} and {Forecasting}: {Selected} {Contributions} from {ITISE} 2016},
	publisher = {Springer International Publishing},
	author = {Navares, Ricardo and Aznarte, José Luis},
	editor = {Rojas, Ignacio and Pomares, Héctor and Valenzuela, Olga and Rojas, Ignacio and Pomares, Héctor and Valenzuela, Olga},
	collaborator = {Rojas, Ignacio and Pomares, Héctor and Valenzuela, Olga},
	year = {2017},
	pages = {387--399},
}

@article{valput_forecasting_2019,
	title = {Forecasting hourly  \{{\textbackslash}textdollar\}\{{\textbackslash}textdollar\}\${\textbackslash}lbrace\$\{{\textbackslash}textbackslash\}hbox \${\textbackslash}lbrace\$\{{NO}\}\${\textbackslash}rbrace\$\{{\textbackslash}\_\}\${\textbackslash}lbrace\$2\${\textbackslash}rbrace\$\${\textbackslash}rbrace\$\{{\textbackslash}textdollar\}\{{\textbackslash}textdollar\} \{{NO}\} 2 concentrations by ensembling neural networks and mesoscale models},
	copyright = {All rights reserved},
	url = {https://doi.org/10.1007%2Fs00521-019-04442-z},
	doi = {10.1007/s00521-019-04442-z},
	urldate = {2020-01-13},
	journal = {Neural Computing and Applications},
	author = {Valput, Damir and Navares, Ricardo and Aznarte, José L.},
	month = aug,
	year = {2019},
}

@article{aznarte_m_forecasting_2007,
	title = {Forecasting airborne pollen concentration time series with neural and neuro-fuzzy models},
	volume = {32},
	copyright = {All rights reserved},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417406000972},
	doi = {10.1016/j.eswa.2006.02.011},
	abstract = {Forecasting airborne pollen concentrations is one of the most studied topics in aerobiology, due to its crucial application to allergology. The most used tools for this problem are single lineal regressions and autoregressive models (ARIMA). Notwithstanding, few works have used more sophisticated tools based in Artificial Intelligence, as are neural or neuro-fuzzy models. In this work, we applied some of these models to forecast olive pollen concentrations in the atmosphere of Granada (Spain). We first studied the overall performance of the selected models, then considering the data segmented into intervals (low, medium and high concentration), to test how they behave on each interval. Experimental results show an advantage of the neuro-fuzzy models against classical statistical methods, although there is still room for improvement.1This research has been partially supported by Ministerio de Ciencia y Tecnología under project TIC2003-04650.1},
	language = {en},
	number = {4},
	urldate = {2021-08-02},
	journal = {Expert Systems with Applications},
	author = {Aznarte M., José Luis and Benítez Sánchez, José Manuel and Lugilde, Diego Nieto and de Linares Fernández, Concepción and de la Guardia, Consuelo Díaz and Sánchez, Francisca Alba},
	month = may,
	year = {2007},
	keywords = {Aerobiology, Airborne pollen, Forecasting, Neuro-fuzzy, Time series},
	pages = {1218--1225},
	file = {ScienceDirect Snapshot:files/2938/S0957417406000972.html:text/html},
}
